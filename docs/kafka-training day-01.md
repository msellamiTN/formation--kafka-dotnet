
# Formation Apache Kafka pour DÃ©veloppeurs .NET

## Programme Intensif 3 Jours (21 heures) - Version ComplÃ¨te

**Version:** 3.0 - FÃ©vrier 2026  
**Cible:** DÃ©veloppeurs seniors .NET sans connaissance prÃ©alable Kafka  
**Environnement:** OpenShift/OKD + .NET 8 + Kafka 4.0.0 + Kafka Connect  
**Instructeur:** Expert Kafka & .NET Architecture  

---

# ğŸ“‹ TABLE DES MATIÃˆRES

## JOUR 1 : FONDATIONS & PREMIERS PAS (7h)
- [Bloc 1.1 : Introduction & Architecture (1h30)](#bloc-11-introduction--architecture)
- [Bloc 1.2 : Premier Producer C# (2h)](#bloc-12-premier-producer-c)
- [Bloc 1.3 : Premier Consumer C# (2h30)](#bloc-13-premier-consumer-c)
- [Bloc 1.4 : RÃ©capitulatif & Q&A (1h)](#bloc-14-rÃ©capitulatif--qa)

## JOUR 2 : PATTERNS DE PRODUCTION & SÃ‰RIALISATION (7h)
- [Bloc 2.1 : SÃ©rialisation AvancÃ©e (2h)](#bloc-21-sÃ©rialisation-avancÃ©e)
- [Bloc 2.2 : Producer Patterns AvancÃ©s (2h30)](#bloc-22-producer-patterns-avancÃ©s)
- [Bloc 2.3 : Consumer Patterns AvancÃ©s (2h)](#bloc-23-consumer-patterns-avancÃ©s)
- [Bloc 2.4 : Kafka Connect Introduction (0h30)](#bloc-24-kafka-connect-introduction)

## JOUR 3 : STREAMS, CONNECT & PRODUCTION (7h)
- [Bloc 3.1 : Kafka Streams avec .NET (2h)](#bloc-31-kafka-streams-avec-net)
- [Bloc 3.2 : Kafka Connect AvancÃ© (1h30)](#bloc-32-kafka-connect-avancÃ©)
- [Bloc 3.3 : DÃ©ploiement OpenShift (1h30)](#bloc-33-dÃ©ploiement-openshift)
- [Bloc 3.4 : SÃ©curitÃ© & Monitoring (1h)](#bloc-34-sÃ©curitÃ©--monitoring)
- [Bloc 3.5 : Troubleshooting Production (1h)](#bloc-35-troubleshooting-production)

## ANNEXES
- [Stack Technique ComplÃ¨te](#stack-technique-complÃ¨te)
- [PrÃ©requis Environnement](#prÃ©requis-environnement)
- [Tips & Best Practices](#tips--best-practices)
- [Troubleshooting Guide Complet](#troubleshooting-guide-complet)
- [Ressources ComplÃ©mentaires](#ressources-complÃ©mentaires)
- [Checklist Production](#checklist-production)

---

# PRÃ‰SENTATION DE LA FORMATION

## Objectifs PÃ©dagogiques

Ã€ l'issue de cette formation intensive de 3 jours, les participants seront capables de :

âœ… **Comprendre** l'architecture distribuÃ©e de Kafka (brokers, topics, partitions, rÃ©plication)  
âœ… **DÃ©velopper** des Producers et Consumers .NET production-ready avec gestion d'erreurs avancÃ©e  
âœ… **IntÃ©grer** Kafka dans des architectures microservices event-driven  
âœ… **SÃ©rialiser** des messages avec Avro et Schema Registry pour Ã©volution de schÃ©ma  
âœ… **Configurer** Kafka Connect pour intÃ©gration avec bases de donnÃ©es et systÃ¨mes externes  
âœ… **DÃ©ployer** des applications Kafka sur OpenShift/OKD avec configuration sÃ©curisÃ©e  
âœ… **Monitorer** la performance (consumer lag, throughput) via Prometheus/Grafana  
âœ… **Troubleshooter** les problÃ¨mes courants en production  
âœ… **Appliquer** les patterns de production (idempotence, retry, dead-letter queue, exactly-once)  

## Public Cible

- **DÃ©veloppeurs .NET seniors** avec 3+ ans d'expÃ©rience en dÃ©veloppement backend
- **Architectes logiciels** concevant des systÃ¨mes distribuÃ©s
- **Tech Leads** responsables de l'intÃ©gration de nouvelles technologies
- **DevOps engineers** dÃ©ployant des applications .NET sur Kubernetes/OpenShift

**PrÃ©requis techniques** :
- MaÃ®trise de C# et .NET 8 (async/await, DI, middleware, hosted services)
- ExpÃ©rience avec APIs REST et JSON
- Notions de conteneurs Docker et Kubernetes/OpenShift
- Connaissance de SQL et bases de donnÃ©es relationnelles
- **Aucune** connaissance prÃ©alable de messaging ou Kafka

## Approche PÃ©dagogique

Cette formation suit une mÃ©thodologie **hands-on first** avec des labs intensifs :

1. **ThÃ©orie minimale juste-Ã -temps** : concepts introduits au moment oÃ¹ ils sont nÃ©cessaires
2. **Labs progressifs** : chaque exercice construit sur le prÃ©cÃ©dent (15 labs au total)
3. **Use cases rÃ©els** : exemples tirÃ©s de systÃ¨mes e-commerce, banking, IoT
4. **Code production-ready** : patterns .NET idiomatiques, error handling, logging, observability
5. **Environnement rÃ©aliste** : dÃ©ploiement sur OpenShift comme en production
6. **Troubleshooting intÃ©grÃ©** : rÃ©solution de problÃ¨mes courants Ã  chaque bloc

**Ratio thÃ©orie/pratique** : 30% / 70%

---

# JOUR 1 : FONDATIONS & PREMIERS PAS

---

## BLOC 1.1 : INTRODUCTION & ARCHITECTURE (1h30)

### Objectifs du Bloc
- Comprendre **pourquoi** Kafka (vs alternatives comme RabbitMQ, Azure Service Bus)
- MaÃ®triser les concepts fondamentaux (topic, partition, offset, consumer group)
- Visualiser l'architecture distribuÃ©e et la rÃ©plication
- DÃ©ployer un cluster Kafka sur OpenShift

---

### 1.1.1 ProblÃ©matique MÃ©tier : Pourquoi Kafka ?

#### Cas d'Usage : SystÃ¨me de Commandes E-Commerce

**ScÃ©nario** : Une plateforme e-commerce traite 10 000 commandes/heure avec ces exigences :

- **OrderService** crÃ©e la commande
- **InventoryService** doit rÃ©server le stock
- **PaymentService** doit traiter le paiement
- **ShippingService** doit prÃ©parer l'expÃ©dition
- **NotificationService** doit envoyer email/SMS au client
- **AnalyticsService** doit tracker les conversions

#### âŒ Approche 1 : Appels REST Synchrones

```csharp
// OrderService.cs - Approche synchrone (problÃ©matique)
public async Task<IActionResult> CreateOrder(OrderDto order)
{
    var createdOrder = await _orderRepository.SaveAsync(order);
    
    // Appel synchrone Ã  chaque service (couplage fort)
    await _inventoryHttpClient.ReserveStock(order.Items);
    await _paymentHttpClient.ProcessPayment(order.PaymentInfo);
    await _shippingHttpClient.CreateShipment(order.ShippingAddress);
    await _notificationHttpClient.SendConfirmation(order.CustomerId);
    await _analyticsHttpClient.TrackConversion(order);
    
    return Ok(createdOrder);
}
```

**ProblÃ¨mes** :
- â±ï¸ **Latence cumulÃ©e** : 5 services Ã— 200ms = 1 seconde de rÃ©ponse
- ğŸ’¥ **Point de dÃ©faillance unique** : si ShippingService down â†’ toute la commande Ã©choue
- ğŸ”— **Couplage fort** : OrderService connaÃ®t tous les services downstream
- ğŸ“ˆ **ScalabilitÃ© limitÃ©e** : impossible de traiter services Ã  des vitesses diffÃ©rentes
- ğŸ”„ **Retry complexe** : gÃ©rer les retries pour chaque service individuellement

#### âŒ Approche 2 : File d'Attente Classique (RabbitMQ)

```csharp
// Avec RabbitMQ
await _rabbitMqPublisher.Publish("orders.created", order);
// ProblÃ¨me : Pas de rejouabilitÃ© (message consommÃ© = supprimÃ©)
// ProblÃ¨me : Nouveau service = duplication de messages
```

**Limites** :
- ğŸ“¦ **Messages Ã©phÃ©mÃ¨res** : une fois consommÃ©s, ils disparaissent
- ğŸš« **Pas de multi-consumer natif** : chaque nouveau service nÃ©cessite duplication via exchanges
- âª **Pas de rejouabilitÃ©** : impossible de retraiter l'historique
- ğŸ“Š **Throughput limitÃ©** : ~20K messages/sec vs 100K+ pour Kafka

#### âœ… Approche 3 : Event Streaming avec Kafka

```csharp
// OrderService.cs - Approche event-driven
public async Task<IActionResult> CreateOrder(OrderDto order)
{
    var createdOrder = await _orderRepository.SaveAsync(order);
    
    // Publier un Ã©vÃ©nement (fire-and-forget)
    await _kafkaProducer.ProduceAsync("orders.created", new Message<string, Order>
    {
        Key = order.OrderId,
        Value = order
    });
    
    return Accepted(createdOrder); // RÃ©ponse immÃ©diate (202)
}
```

**Avantages** :
- âš¡ **Latence faible** : rÃ©ponse en ~50ms (juste Ã©criture dans Kafka)
- ğŸ”„ **DÃ©couplage total** : OrderService ne connaÃ®t pas les consumers
- ğŸ“ˆ **ScalabilitÃ© horizontale** : chaque service scale indÃ©pendamment
- âª **RejouabilitÃ©** : nouveaux services peuvent lire l'historique complet
- ğŸ›¡ï¸ **RÃ©silience** : si PaymentService down, les messages restent dans Kafka
- ğŸ“Š **Throughput Ã©levÃ©** : 100K+ messages/sec par partition

ğŸ’¡ **TIP** : Kafka n'est pas un simple message broker, c'est une **plateforme de streaming distribuÃ©e**. Pensez-y comme un journal distribuÃ© (distributed log) plutÃ´t qu'une queue.

---

### 1.1.2 Concepts Fondamentaux

#### Topic = "Base de DonnÃ©es Append-Only"

Un **topic** est un journal ordonnÃ© d'Ã©vÃ©nements, comparable Ã  une table de base de donnÃ©es en append-only.

Topic: orders.created
+--------+--------+--------+--------+--------+--------+
| Msg 0  | Msg 1  | Msg 2  | Msg 3  | Msg 4  | Msg 5  | ...
+--------+--------+--------+--------+--------+--------+
  â†‘                                               â†‘
  DÃ©but                                         Fin (toujours en croissance)

**PropriÃ©tÃ©s** :
- **Immutable** : les messages ne peuvent pas Ãªtre modifiÃ©s ou supprimÃ©s
- **OrdonnÃ©** : ordre d'Ã©criture prÃ©servÃ© dans chaque partition
- **Durable** : messages conservÃ©s selon politique de rÃ©tention (ex: 7 jours)
- **Multi-consumer** : plusieurs services peuvent lire simultanÃ©ment sans interfÃ©rence

âš ï¸ **ATTENTION** : Un topic ne peut pas Ãªtre renommÃ© aprÃ¨s crÃ©ation. Choisissez bien vos noms dÃ¨s le dÃ©but.

ğŸ’¡ **TIP** : Convention de nommage recommandÃ©e : `<domaine>.<entitÃ©>.<action>` (ex: `ecommerce.orders.created`, `payment.transactions.processed`)

#### Partition = UnitÃ© de ParallÃ©lisme

Un topic est divisÃ© en **partitions** pour permettre la scalabilitÃ© horizontale.

Topic: orders.created (3 partitions)

Partition 0: [Msg 0] [Msg 3] [Msg 6] [Msg 9]  ...
Partition 1: [Msg 1] [Msg 4] [Msg 7] [Msg 10] ...
Partition 2: [Msg 2] [Msg 5] [Msg 8] [Msg 11] ...

**RÃ¨gle de partitionnement** :
// Si message a une clÃ©
partition = hash(key) % nombre_partitions

// Exemple
key = "customer-12345" â†’ hash â†’ partition 1
key = "customer-67890" â†’ hash â†’ partition 0

// Si pas de clÃ© â†’ round-robin (sticky partitioner depuis Kafka 2.4+)

**Pourquoi partitionner ?** :
- **ParallÃ©lisme** : chaque partition peut Ãªtre lue par un consumer diffÃ©rent
- **Ordre garanti** : messages avec mÃªme clÃ© â†’ mÃªme partition â†’ ordre prÃ©servÃ©
- **ScalabilitÃ©** : 10 partitions = 10 consumers max en parallÃ¨le

ğŸ’¡ **TIP** : Formule pour dimensionner les partitions :
Nombre de partitions = max(
  Throughput_cible (MB/s) / Throughput_par_partition (MB/s),
  Nombre_de_consumers_max_souhaitÃ©s
)

âš ï¸ **ATTENTION** : Le nombre de partitions ne peut qu'augmenter, jamais diminuer. Dimensionnez large dÃ¨s le dÃ©but (ex: 12 ou 24 partitions pour production).

#### Offset = Pointeur de Lecture

L'**offset** est la position d'un message dans une partition (entier incrÃ©mental 64 bits).

Partition 0: [Msg 0] [Msg 1] [Msg 2] [Msg 3] [Msg 4]
Offsets:        0        1        2        3        4

Consumer A a lu jusqu'Ã  offset 2 â†’ prochaine lecture = offset 3
Consumer B a lu jusqu'Ã  offset 4 â†’ prochaine lecture = offset 5 (nouveau message)

**Gestion des offsets** :
- **Auto-commit** : Kafka commit automatiquement toutes les 5 secondes (par dÃ©faut)
- **Manual-commit** : application commit explicitement aprÃ¨s traitement
- **Stockage** : offsets stockÃ©s dans topic interne `__consumer_offsets` (50 partitions)

ğŸ’¡ **TIP** : Les offsets ne sont jamais rÃ©initialisÃ©s. MÃªme si vous supprimez et recrÃ©ez un consumer group, les anciens offsets peuvent Ãªtre rÃ©cupÃ©rÃ©s pendant 7 jours (par dÃ©faut).

#### Consumer Group = Scaling Horizontal

Un **consumer group** permet Ã  plusieurs instances d'une application de consommer un topic en parallÃ¨le.

Topic: orders.created (6 partitions)

Consumer Group: inventory-service
  Consumer Instance 1 â†’ Partitions 0, 1
  Consumer Instance 2 â†’ Partitions 2, 3
  Consumer Instance 3 â†’ Partitions 4, 5

Si Instance 2 crash â†’ Rebalancing automatique:
  Instance 1 â†’ Partitions 0, 1, 2
  Instance 3 â†’ Partitions 3, 4, 5

**RÃ¨gles** :
- Chaque partition est assignÃ©e Ã  **un seul** consumer dans le group
- Si plus de consumers que de partitions â†’ certains consumers inactifs
- Nouveau consumer rejoint â†’ rebalancing (redistribution partitions)
- Consumer quitte â†’ rebalancing (ses partitions redistribuÃ©es)

ğŸ’¡ **TIP** : Pour tester en local sans rebalancing constant, utilisez des GroupIds diffÃ©rents : `inventory-service-dev-yourname`

---

### 1.1.3 Architecture Kafka DistribuÃ©e

#### Composants ClÃ©s

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KAFKA CLUSTER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Broker 1 â”‚  â”‚ Broker 2 â”‚  â”‚ Broker 3 â”‚            â”‚
â”‚  â”‚ (Leader) â”‚  â”‚(Follower)â”‚  â”‚(Follower)â”‚            â”‚
â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚            â”‚
â”‚  â”‚ Part 0   â”‚  â”‚ Part 0   â”‚  â”‚ Part 0   â”‚  Replication
â”‚  â”‚ Part 1   â”‚  â”‚ Part 1   â”‚  â”‚ Part 1   â”‚  Factor = 3
â”‚  â”‚ Part 2   â”‚  â”‚ Part 2   â”‚  â”‚ Part 2   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   ZooKeeper / KRaft (Metadata)  â”‚                  â”‚
â”‚  â”‚   - Broker coordination         â”‚                  â”‚
â”‚  â”‚   - Leader election             â”‚                  â”‚
â”‚  â”‚   - Configuration storage       â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

         â–²                          â”‚
         â”‚ Produce                  â”‚ Consume
         â”‚                          â–¼

   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Producer  â”‚            â”‚ Consumer      â”‚
   â”‚ (.NET App)â”‚            â”‚ Group         â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚ (.NET Worker) â”‚
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Broker** : Serveur Kafka stockant les partitions et gÃ©rant les requÃªtes clients  
**ZooKeeper/KRaft** : SystÃ¨me de coordination (Ã©lection de leader, mÃ©tadonnÃ©es)  
**Producer** : Client Ã©crivant des messages dans un topic  
**Consumer** : Client lisant des messages depuis un topic  

ğŸ’¡ **TIP** : Kafka 4.0.0 supporte KRaft (Kafka Raft) pour remplacer ZooKeeper. C'est l'architecture recommandÃ©e pour les nouveaux dÃ©ploiements.

#### RÃ©plication & Haute DisponibilitÃ©

Topic: payments (replication.factor = 3)

Partition 0:
  Leader:    Broker 1 (Ã©critures/lectures)
  Follower:  Broker 2 (copie synchrone)
  Follower:  Broker 3 (copie synchrone)

Si Broker 1 crash â†’ Broker 2 Ã©lu nouveau leader automatiquement (< 5 secondes)

**ISR (In-Sync Replicas)** : replicas qui sont Ã  jour avec le leader  
**min.insync.replicas** : nombre minimum de replicas pour accepter Ã©criture (garantie durabilitÃ©)

ğŸ’¡ **TIP** : Configuration production recommandÃ©e :
- `replication.factor = 3` (tolÃ©rance Ã  2 pannes de brokers)
- `min.insync.replicas = 2` (garantit Ã©criture sur au moins 2 brokers)
- `acks = all` cÃ´tÃ© producer (attend confirmation de tous les ISR)

âš ï¸ **ATTENTION** : `min.insync.replicas = 1` est dangereux en production (perte de donnÃ©es possible si leader crash avant rÃ©plication).

---

### LAB 1.1 : DÃ©ploiement Kafka sur OpenShift

#### Objectif
DÃ©ployer un cluster Kafka 3 brokers sur OpenShift via Strimzi Operator et crÃ©er votre premier topic.

#### PrÃ©requis
- AccÃ¨s Ã  un cluster OpenShift/OKD 4.x avec droits admin namespace
- CLI `oc` installÃ© et authentifiÃ© (`oc login`)
- Namespace dÃ©diÃ© : `kafka`
- Quota suffisant : 12 GB RAM, 6 CPU cores minimum

#### Ã‰tape 1 : Installation Strimzi Operator

Strimzi est l'opÃ©rateur Kubernetes natif pour gÃ©rer Kafka (CNCF Sandbox project).

# CrÃ©er le namespace
oc new-project kafka

# Installer Strimzi Operator (via OperatorHub ou YAML)
# Option 1 : Via Web Console OpenShift (RECOMMANDÃ‰)
# - Operators â†’ OperatorHub â†’ Rechercher "Strimzi" â†’ Install
# - Choisir "A specific namespace" â†’ kafka

# Option 2 : Via CLI
oc apply -f https://strimzi.io/install/latest?namespace=kafka -n kafka

VÃ©rifier l'installation :
oc get pods -n kafka

# Attendez que strimzi-cluster-operator-xxx soit Running (1-2 minutes)
# NAME                                        READY   STATUS    RESTARTS   AGE
# strimzi-cluster-operator-7d96cbff9b-xxxx    1/1     Running   0          2m

ğŸ’¡ **TIP** : Si le pod operator ne dÃ©marre pas, vÃ©rifiez les logs : `oc logs -l name=strimzi-cluster-operator`

#### Ã‰tape 2 : DÃ©ployer Cluster Kafka

CrÃ©er le fichier `kafka-cluster.yaml` :

```yaml
apiVersion: kafka.strimzi.io/v1
kind: Kafka
metadata:
  name: bhf-kafka
  namespace: kafka
  annotations:
    strimzi.io/kraft: "enabled"
    strimzi.io/node-pools: "enabled"
spec:
  kafka:
    version: 4.0.0
    replicas: 3  # K3s: 3, OpenShift CRC: 1
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
      default.replication.factor: 1
      min.insync.replicas: 1
      inter.broker.protocol.version: "4.0"
      log.message.format.version: "4.0"
      # Compression par dÃ©faut
      compression.type: lz4
      # RÃ©tention par dÃ©faut (7 jours)
      log.retention.hours: 168
      # Taille max segment (1 GB)
      log.segment.bytes: 1073741824
    storage:
      type: ephemeral
    resources:
      requests:
        memory: 1Gi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1
  entityOperator:
    topicOperator:
      resources:
        requests:
          memory: 256Mi
          cpu: 200m
        limits:
          memory: 512Mi
          cpu: 500m
    userOperator:
      resources:
        requests:
          memory: 256Mi
          cpu: 200m
        limits:
          memory: 512Mi
          cpu: 500m
```

DÃ©ployer :

```bash
oc apply -f kafka-cluster.yaml -n kafka

# Suivre le dÃ©ploiement (prend 3-5 minutes)
oc get kafka bhf-kafka -n kafka -w

# Attendez status: Ready
# NAME        DESIRED KAFKA REPLICAS   READY   WARNINGS
# bhf-kafka   3                        True
```

VÃ©rifier les pods :

```bash
oc get pods -n kafka
```

**RÃ©sultat attendu (K3s, 3 replicas)** :

```text
bhf-kafka-broker-0                           1/1     Running   0          5m
bhf-kafka-broker-1                           1/1     Running   0          5m
bhf-kafka-broker-2                           1/1     Running   0          5m
bhf-kafka-controller-3                       1/1     Running   0          6m
bhf-kafka-controller-4                       1/1     Running   0          6m
bhf-kafka-controller-5                       1/1     Running   0          6m
bhf-kafka-entity-operator-xxx               2/2     Running   0          4m
```

**RÃ©sultat attendu (OpenShift CRC, 1 replica)** :

```text
bhf-kafka-broker-0                           1/1     Running   0          5m
bhf-kafka-controller-0                       1/1     Running   0          6m
bhf-kafka-entity-operator-xxx               2/2     Running   0          4m
```

ğŸ’¡ **TIP** : Si un pod reste en Pending, vÃ©rifiez le PVC : `oc get pvc`. Assurez-vous qu'un StorageClass par dÃ©faut existe.

âš ï¸ **TROUBLESHOOTING** : Pod CrashLoopBackOff ?

```bash
# VÃ©rifier les logs
oc logs bhf-kafka-broker-0

# Erreur courante : Insufficient memory
# Solution : RÃ©duire resources.requests.memory Ã  1Gi pour les tests
```

#### Ã‰tape 3 : CrÃ©er un Topic

CrÃ©er le fichier `first-topic.yaml` :

```yaml
apiVersion: kafka.strimzi.io/v1
kind: KafkaTopic
metadata:
  name: orders.created
  namespace: kafka
  labels:
    strimzi.io/cluster: bhf-kafka
spec:
  partitions: 6
  replicas: 3
  config:
    retention.ms: 604800000  # 7 jours
    segment.bytes: 1073741824  # 1 GB
    compression.type: lz4
    min.insync.replicas: 2
    # Cleanup policy (delete or compact)
    cleanup.policy: delete
    # Max message size (1 MB par dÃ©faut)
    max.message.bytes: 1048576
```

Appliquer :

```bash
oc apply -f first-topic.yaml -n kafka

# VÃ©rifier la crÃ©ation
oc get kafkatopic orders.created -n kafka

# DÃ©tails du topic
oc describe kafkatopic orders.created
```

ğŸ’¡ **TIP** : Vous pouvez aussi crÃ©er des topics via CLI Kafka :

```bash
oc exec -it bhf-kafka-broker-0 -- \
  bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create \
  --topic test-topic \
  --partitions 3 \
  --replication-factor 3
```

#### Ã‰tape 4 : Test avec Console Producer/Consumer

Lancer un producer :

```bash
oc run kafka-producer -ti \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --rm=true \
  --restart=Never \
  -- bin/kafka-console-producer.sh \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic orders.created

# Taper quelques messages :
# {"orderId": "ORD-001", "customerId": "CUST-123", "amount": 99.99}
# {"orderId": "ORD-002", "customerId": "CUST-456", "amount": 149.50}
# {"orderId": "ORD-003", "customerId": "CUST-789", "amount": 249.99}
# (Ctrl+C pour quitter)
```

Lancer un consumer (dans un autre terminal) :

```bash
oc run kafka-consumer -ti \
  --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --rm=true \
  --restart=Never \
  -- bin/kafka-console-consumer.sh \
  --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic orders.created \
  --from-beginning

# Vous devriez voir les 3 messages prÃ©cÃ©dents
```

ğŸ’¡ **TIP** : Ajouter `--property print.key=true` pour voir les clÃ©s des messages.

#### Ã‰tape 5 : VÃ©rifier le Cluster

```bash
# Lister tous les topics
oc exec -it bhf-kafka-broker-0 -- \
  bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --list

# DÃ©crire un topic
oc exec -it bhf-kafka-broker-0 -- \
  bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --describe \
  --topic orders.created

# Output attendu :
# Topic: orders.created  PartitionCount: 6  ReplicationFactor: 3
# Partition: 0  Leader: 0  Replicas: 0,1,2  Isr: 0,1,2
# Partition: 1  Leader: 1  Replicas: 1,2,0  Isr: 1,2,0
# ...
```

#### âœ… Validation

- [ ] Cluster Kafka 3 brokers Running
- [ ] Controllers 3 nodes Running (KRaft)
- [ ] Entity Operator Running
- [ ] Topic `orders.created` crÃ©Ã© avec 6 partitions et replication factor 3
- [ ] Messages produits et consommÃ©s avec succÃ¨s via CLI
- [ ] Tous les ISR (In-Sync Replicas) Ã  jour

**ğŸ“¸ Screenshot Ã  prendre** : `oc get pods` montrant tous les pods Running

ğŸ’¡ **TIP** : CrÃ©ez un alias pour faciliter l'accÃ¨s aux outils Kafka :

```bash
alias kafka-topics="oc exec -it bhf-kafka-broker-0 -- bin/kafka-topics.sh --bootstrap-server localhost:9092"
alias kafka-console-producer="oc exec -it bhf-kafka-broker-0 -- bin/kafka-console-producer.sh --bootstrap-server localhost:9092"
alias kafka-console-consumer="oc exec -it bhf-kafka-broker-0 -- bin/kafka-console-consumer.sh --bootstrap-server localhost:9092"
```

---

## BLOC 1.2 : PREMIER PRODUCER C# (2h)

### Objectifs du Bloc
- DÃ©velopper un Producer .NET minimaliste
- Comprendre la configuration de base et les trade-offs
- ImplÃ©menter le partitionnement par clÃ©
- GÃ©rer les erreurs et confirmations de livraison
- Optimiser les performances (batching, compression)

---

### 1.2.1 ThÃ©orie : Anatomie d'un Message Kafka

Un message Kafka est composÃ© de plusieurs parties :

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MESSAGE KAFKA                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Key (optional)    : byte[]              â”‚  â†’ DÃ©termine la partition
â”‚ Value             : byte[]              â”‚  â†’ Contenu du message
â”‚ Headers (optional): Map<string, byte[]> â”‚  â†’ MÃ©tadonnÃ©es (trace ID, correlation ID)
â”‚ Timestamp         : long                â”‚  â†’ Horodatage (automatique ou custom)
â”‚ Partition         : int                 â”‚  â†’ CalculÃ© par Kafka (si key fournie)
â”‚ Offset            : long                â”‚  â†’ AssignÃ© par le broker aprÃ¨s Ã©criture
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Key vs Value

| Aspect | Key | Value |
|--------|-----|-------|
| **Obligatoire** | Non (null autorisÃ©) | Oui |
| **Usage** | Partitionnement, compaction | DonnÃ©es mÃ©tier |
| **Exemple** | `customerId`, `orderId`, `deviceId` | JSON de la commande, Avro, Protobuf |
| **Taille max** | RecommandÃ© < 100 bytes | DÃ©faut: 1 MB (configurable) |

ğŸ’¡ **TIP** : Utilisez toujours une clÃ© si vous avez besoin de :
1. **Ordre garanti** des messages (mÃªme clÃ© = mÃªme partition = ordre prÃ©servÃ©)
2. **Compaction** de topic (derniÃ¨re valeur par clÃ© conservÃ©e)
3. **LocalitÃ©** pour les consumers (tous les events d'une entitÃ© ensemble)

#### Partitionnement avec ClÃ©

```csharp
// Exemple 1 : Sans clÃ© â†’ round-robin (sticky partitioner depuis Kafka 2.4+)
await producer.ProduceAsync("orders", new Message<Null, string>
{
    Value = "{...}"  // Ira sur partition choisie par sticky algorithm
});

// Exemple 2 : Avec clÃ© â†’ hash-based
await producer.ProduceAsync("orders", new Message<string, string>
{
    Key = "customer-123",  // Ira TOUJOURS sur la mÃªme partition
    Value = "{...}"
});
```

**Formule de partitionnement** :

```text
partition = murmur2_hash(key) % nombre_partitions
```

**Pourquoi c'est important ?** :
- **Ordre garanti** : tous les Ã©vÃ©nements d'un client arrivent dans l'ordre
- **LocalitÃ©** : un consumer voit toujours les events d'un mÃªme client ensemble
- **Ã‰viter hot partitions** : clÃ©s bien distribuÃ©es = charge Ã©quilibrÃ©e

âš ï¸ **ATTENTION** : Ã‰vitez les clÃ©s dÃ©sÃ©quilibrÃ©es (ex: 80% des messages avec mÃªme clÃ© â†’ hot partition)

ğŸ’¡ **TIP** : Pour des clÃ©s trÃ¨s variÃ©es (ex: millions de customer IDs), utilisez un hash de la clÃ© comme clÃ© Kafka pour garantir distribution uniforme.

---

### 1.2.2 Configuration Producer .NET

#### NuGet Packages Requis

```xml
<!-- Dans votre .csproj -->
<PackageReference Include="Confluent.Kafka" Version="2.3.0" />
<PackageReference Include="Microsoft.Extensions.Logging" Version="8.0.0" />
<PackageReference Include="Microsoft.Extensions.Hosting" Version="8.0.0" />
```

#### Configuration Minimale

```csharp
using Confluent.Kafka;

var config = new ProducerConfig
{
    // ===== OBLIGATOIRE =====
    BootstrapServers = "bhf-kafka-kafka-bootstrap:9092",
    
    // ===== IDENTIFICATION =====
    ClientId = "dotnet-producer-v1",  // Pour logs et monitoring
    
    // ===== GARANTIES DE LIVRAISON =====
    Acks = Acks.All,  // Attendre confirmation de tous les ISR (production)
    // Acks.None (0) : Aucune attente, latence minimale, perte possible
    // Acks.Leader (1) : Attendre leader uniquement, risque si leader crash
    
    // ===== RETRY =====
    MessageSendMaxRetries = 3,
    RetryBackoffMs = 1000,  // 1 seconde entre chaque retry
    RequestTimeoutMs = 30000,  // 30 secondes timeout par requÃªte
    
    // ===== IDEMPOTENCE (RECOMMANDÃ‰ PRODUCTION) =====
    EnableIdempotence = false  // On verra Ã§a plus tard
};
```

ğŸ’¡ **TIP** : Pour production, utilisez toujours `Acks = Acks.All` avec `min.insync.replicas >= 2` pour garantir durabilitÃ©.

#### ParamÃ¨tres de Performance

```csharp
var config = new ProducerConfig
{
    // ... config de base ...
    
    // ===== BATCHING (Grouper messages pour efficacitÃ©) =====
    LingerMs = 10,         // Attendre 10ms pour grouper messages
    BatchSize = 16384,     // Taille max d'un batch (16 KB)
    
    // ===== COMPRESSION (RÃ©duire bande passante) =====
    CompressionType = CompressionType.Lz4,  // lz4, snappy, gzip, zstd
    
    // ===== BUFFER MÃ‰MOIRE =====
    QueueBufferingMaxMessages = 100000,  // Max messages en attente
    QueueBufferingMaxKbytes = 1048576,   // Max 1 GB en mÃ©moire
    
    // ===== MAX IN-FLIGHT =====
    MaxInFlight = 5  // Max requÃªtes non-ackÃ©es en parallÃ¨le
};
```

ğŸ’¡ **TIP** : Trade-off latence vs throughput :
- **Latence critique** : `LingerMs = 0`, `BatchSize = 16384`, `CompressionType = None`
- **Throughput Ã©levÃ©** : `LingerMs = 10-100`, `BatchSize = 100000`, `CompressionType = Lz4`

---

### LAB 1.2A : Producer Synchrone Basique

#### Objectif
CrÃ©er une application console .NET qui envoie des messages simples (string) Ã  Kafka avec gestion d'erreurs.

#### Structure du Projet

```text
KafkaProducerBasic/
â”œâ”€â”€ KafkaProducerBasic.csproj
â”œâ”€â”€ Program.cs
â”œâ”€â”€ appsettings.json
â””â”€â”€ Dockerfile
```

#### Code : Program.cs

using Confluent.Kafka;
using Microsoft.Extensions.Logging;

// ===== CONFIGURATION =====
var loggerFactory = LoggerFactory.Create(builder =>
{
    builder.AddConsole();
    builder.SetMinimumLevel(LogLevel.Information);
});
var logger = loggerFactory.CreateLogger<Program>();

var config = new ProducerConfig
{
    BootstrapServers = Environment.GetEnvironmentVariable("KAFKA_BOOTSTRAP_SERVERS") 
                       ?? "bhf-kafka-kafka-bootstrap:9092",
    ClientId = "dotnet-basic-producer",
    Acks = Acks.All,
    MessageSendMaxRetries = 3,
    RetryBackoffMs = 1000,
    RequestTimeoutMs = 30000
};

// ===== CRÃ‰ATION DU PRODUCER =====
using var producer = new ProducerBuilder<Null, string>(config)
    .SetErrorHandler((_, e) => 
    {
        logger.LogError("Producer error: Code={Code}, Reason={Reason}, IsFatal={IsFatal}", 
            e.Code, e.Reason, e.IsFatal);
        if (e.IsFatal)
        {
            logger.LogCritical("Fatal error detected. Exiting...");
            Environment.Exit(1);
        }
    })
    .SetLogHandler((_, logMessage) => 
    {
        var logLevel = logMessage.Level switch
        {
            SyslogLevel.Emergency or SyslogLevel.Alert or SyslogLevel.Critical => LogLevel.Critical,
            SyslogLevel.Error => LogLevel.Error,
            SyslogLevel.Warning => LogLevel.Warning,
            SyslogLevel.Notice or SyslogLevel.Info => LogLevel.Information,
            _ => LogLevel.Debug
        };
        logger.Log(logLevel, "Kafka internal log: {Message}", logMessage.Message);
    })
    .Build();

logger.LogInformation("Producer started. Connecting to {Brokers}", config.BootstrapServers);

// ===== ENVOI DE MESSAGES =====
const string topicName = "orders.created";

try
{
    for (int i = 1; i <= 10; i++)
    {
        var messageValue = $"{{\"orderId\": \"ORD-{i:D4}\", \"timestamp\": \"{DateTime.UtcNow:o}\", \"amount\": {100 + i * 10}}}";
        
        logger.LogInformation("Sending message {Index}: {Message}", i, messageValue);
        
        // ProduceAsync retourne une Task<DeliveryResult>
        var deliveryResult = await producer.ProduceAsync(topicName, new Message<Null, string>
        {
            Value = messageValue,
            // Headers optionnels (mÃ©tadonnÃ©es)
            Headers = new Headers
            {
                { "correlation-id", System.Text.Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) },
                { "source", System.Text.Encoding.UTF8.GetBytes("dotnet-producer") }
            }
        });
        
        // Confirmation de livraison
        logger.LogInformation(
            "âœ“ Message {Index} delivered â†’ Topic: {Topic}, Partition: {Partition}, Offset: {Offset}, Timestamp: {Timestamp}",
            i,
            deliveryResult.Topic,
            deliveryResult.Partition.Value,
            deliveryResult.Offset.Value,
            deliveryResult.Timestamp.UtcDateTime
        );
        
        await Task.Delay(500);  // Pause 500ms entre chaque message
    }
    
    logger.LogInformation("All messages sent successfully!");
}
catch (ProduceException<Null, string> ex)
{
    logger.LogError(ex, "Failed to produce message");
    logger.LogError("Error Code: {ErrorCode}, Reason: {Reason}, IsFatal: {IsFatal}", 
        ex.Error.Code, ex.Error.Reason, ex.Error.IsFatal);
}
catch (Exception ex)
{
    logger.LogError(ex, "Unexpected error");
}
finally
{
    // IMPORTANT : Flush des messages en attente avant fermeture
    logger.LogInformation("Flushing pending messages...");
    producer.Flush(TimeSpan.FromSeconds(10));
    logger.LogInformation("Producer closed gracefully.");
}
```

ğŸ’¡ **TIP** : Utilisez toujours `Flush()` avant de fermer le producer pour Ã©viter la perte de messages en attente.

âš ï¸ **ATTENTION** : `ProduceAsync()` est non-bloquant. Le message est mis en buffer et envoyÃ© de maniÃ¨re asynchrone. Utilisez `await` ou `Flush()` pour garantir l'envoi.

#### Configuration : appsettings.json

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft": "Warning",
      "Confluent.Kafka": "Information"
    }
  },
  "Kafka": {
    "BootstrapServers": "bhf-kafka-kafka-bootstrap:9092",
    "ClientId": "dotnet-producer",
    "TopicName": "orders.created"
  }
}
```

#### DÃ©ploiement sur OpenShift

**Dockerfile** :

```dockerfile
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /app

# Copy csproj and restore dependencies (layer caching)
COPY *.csproj .
RUN dotnet restore

# Copy everything else and build
COPY . .
RUN dotnet publish -c Release -o out --no-restore

FROM mcr.microsoft.com/dotnet/runtime:8.0
WORKDIR /app
COPY --from=build /app/out .

# Non-root user (OpenShift security)
USER 1001

ENTRYPOINT ["dotnet", "KafkaProducerBasic.dll"]
```

**Build & Push** :
# Build de l'image
docker build -t kafka-producer-basic:v1 .

# Tag pour registry OpenShift interne
docker tag kafka-producer-basic:v1 \
  image-registry.openshift-image-registry.svc:5000/kafka/producer-basic:v1

# Login au registry OpenShift
oc registry login

# Push vers registry OpenShift
docker push image-registry.openshift-image-registry.svc:5000/kafka/producer-basic:v1

ğŸ’¡ **TIP** : Si `oc registry login` Ã©choue, crÃ©ez un secret manuel :
oc create secret docker-registry my-pull-secret \
  --docker-server=image-registry.openshift-image-registry.svc:5000 \
  --docker-username=$(oc whoami) \
  --docker-password=$(oc whoami -t)

**Deployment YAML** :
apiVersion: apps/v1
kind: Deployment
metadata:
  name: producer-basic
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: producer-basic
  template:
    metadata:
      labels:
        app: producer-basic
        version: v1
    spec:
      containers:
      - name: producer
        image: image-registry.openshift-image-registry.svc:5000/kafka-training/producer-basic:v1
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "bhf-kafka-kafka-bootstrap:9092"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

**DÃ©ployer** :
oc apply -f deployment.yaml -n kafka

# Suivre les logs
oc logs -f deployment/producer-basic -n kafka

#### âœ… Validation

Observer dans les logs :
info: Program[0]
      âœ“ Message 1 delivered â†’ Topic: orders.created, Partition: 3, Offset: 0
info: Program[0]
      âœ“ Message 2 delivered â†’ Topic: orders.created, Partition: 1, Offset: 0
info: Program[0]
      âœ“ Message 3 delivered â†’ Topic: orders.created, Partition: 5, Offset: 0

**Points Ã  noter** :
- Les messages se rÃ©partissent sur les 6 partitions (round-robin car pas de clÃ©)
- L'offset commence Ã  0 pour chaque partition (si topic vide)
- Pas d'erreurs de connexion
- Latence d'envoi : ~5-10ms par message

ğŸ’¡ **TIP** : Pour voir les messages produits :
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders.created \
  --from-beginning \
  --max-messages 10

---

### LAB 1.2B : Producer avec ClÃ© (Partitionnement DÃ©terministe)

#### Objectif
Comprendre comment la clÃ© dÃ©termine la partition et garantit l'ordre des messages pour une mÃªme entitÃ©.

#### Code : Program.cs (version avec clÃ©)

using Confluent.Kafka;
using Microsoft.Extensions.Logging;

var loggerFactory = LoggerFactory.Create(builder =>
{
    builder.AddConsole();
    builder.SetMinimumLevel(LogLevel.Information);
});
var logger = loggerFactory.CreateLogger<Program>();

var config = new ProducerConfig
{
    BootstrapServers = "bhf-kafka-kafka-bootstrap:9092",
    ClientId = "dotnet-keyed-producer",
    Acks = Acks.All
};

using var producer = new ProducerBuilder<string, string>(config)  // <string, string> pour Key/Value
    .SetErrorHandler((_, e) => logger.LogError("Error: {Reason}", e.Reason))
    .Build();

const string topicName = "orders.created";

// Simuler 5 clients diffÃ©rents
var customers = new[] { "customer-A", "customer-B", "customer-C", "customer-D", "customer-E" };

try
{
    for (int i = 1; i <= 30; i++)
    {
        // Chaque client a plusieurs commandes
        var customerId = customers[i % 5];
        var orderId = $"ORD-{customerId}-{i:D4}";
        var messageValue = $"{{\"orderId\": \"{orderId}\", \"customerId\": \"{customerId}\", \"amount\": {100 + i * 10}}}";
        
        logger.LogInformation("Sending order {OrderId} for customer {CustomerId}", orderId, customerId);
        
        var deliveryResult = await producer.ProduceAsync(topicName, new Message<string, string>
        {
            Key = customerId,  // LA CLÃ‰ DÃ‰TERMINE LA PARTITION
            Value = messageValue,
            Timestamp = Timestamp.Default  // Utiliser timestamp actuel
        });
        
        logger.LogInformation(
            "âœ“ Delivered â†’ Key: {Key}, Partition: {Partition}, Offset: {Offset}",
            customerId,
            deliveryResult.Partition.Value,
            deliveryResult.Offset.Value
        );
        
        await Task.Delay(200);
    }
}
catch (ProduceException<string, string> ex)
{
    logger.LogError(ex, "Failed to produce message");
}
finally
{
    producer.Flush(TimeSpan.FromSeconds(10));
    logger.LogInformation("Producer closed.");
}

#### Analyse des RÃ©sultats

**Logs attendus** :
âœ“ Delivered â†’ Key: customer-A, Partition: 3, Offset: 0
âœ“ Delivered â†’ Key: customer-B, Partition: 1, Offset: 0
âœ“ Delivered â†’ Key: customer-C, Partition: 5, Offset: 0
âœ“ Delivered â†’ Key: customer-D, Partition: 2, Offset: 0
âœ“ Delivered â†’ Key: customer-E, Partition: 4, Offset: 0
âœ“ Delivered â†’ Key: customer-A, Partition: 3, Offset: 1  â† MÃªme partition !
âœ“ Delivered â†’ Key: customer-B, Partition: 1, Offset: 1  â† MÃªme partition !
âœ“ Delivered â†’ Key: customer-C, Partition: 5, Offset: 1  â† MÃªme partition !

**Observation clÃ©** :
- Tous les messages avec `Key = "customer-A"` vont **toujours** sur **Partition 3**
- Tous les messages avec `Key = "customer-B"` vont **toujours** sur **Partition 1**
- L'ordre des messages est prÃ©servÃ© pour chaque client

**Formule de partitionnement** :
// Kafka utilise Murmur2 hash
partition = Math.Abs(Murmur2.Hash(Encoding.UTF8.GetBytes(key))) % numberOfPartitions

ğŸ’¡ **TIP** : Vous pouvez prÃ©dire la partition d'une clÃ© :
using Confluent.Kafka;

var partitioner = new DefaultPartitioner();
var partition = partitioner.Partition(
    "orders.created", 
    6, // nombre de partitions
    Encoding.UTF8.GetBytes("customer-A"), 
    null, 
    new ReadOnlySpan<byte>()
);
Console.WriteLine($"customer-A ira sur partition {partition}");

#### Exercice Pratique

**DÃ©fi** : Modifier le code pour :
1. Envoyer 60 messages au total (10 clients : customer-A Ã  customer-J)
2. Observer sur quelles partitions ils atterrissent
3. Calculer la distribution (combien de clients par partition ?)
4. Identifier si certaines partitions reÃ§oivent plus de messages (hot partitions ?)

**Solution** :
var customers = Enumerable.Range(0, 10).Select(i => $"customer-{(char)('A' + i)}").ToArray();
var partitionCounts = new Dictionary<int, int>();

for (int i = 1; i <= 60; i++)
{
    var customerId = customers[i % 10];
    var result = await producer.ProduceAsync(topicName, new Message<string, string>
    {
        Key = customerId,
        Value = $"{{...}}"
    });
    
    partitionCounts[result.Partition.Value] = 
        partitionCounts.GetValueOrDefault(result.Partition.Value, 0) + 1;
}

foreach (var kvp in partitionCounts.OrderBy(x => x.Key))
{
    Console.WriteLine($"Partition {kvp.Key}: {kvp.Value} messages");
}

#### âœ… Validation

- [ ] Comprendre que Key â†’ Partition est **dÃ©terministe** et **reproductible**
- [ ] MÃªme clÃ© = mÃªme partition = **ordre prÃ©servÃ©** pour cette clÃ©
- [ ] Distribution des clÃ©s doit Ãªtre uniforme pour Ã©viter hot partitions
- [ ] Utile pour Ã©vÃ©nements liÃ©s (commandes d'un mÃªme client, transactions d'un compte bancaire)

ğŸ’¡ **TIP** : Si vous avez un identifiant numÃ©rique (ex: customerId = 12345), convertissez-le en string pour la clÃ© Kafka : `customerId.ToString()`

---

### 1.2.3 Gestion des Erreurs Producer

#### Types d'Erreurs Kafka

| Type | Retriable ? | ErrorCode | Exemple | Action |
|------|-------------|-----------|---------|--------|
| **Transient (rÃ©cupÃ©rable)** | âœ… Oui | `NotEnoughReplicasException`, `LeaderNotAvailableException`, `NetworkException` | Broker temporairement indisponible | Retry automatique |
| **Permanent (non rÃ©cupÃ©rable)** | âŒ Non | `RecordTooLargeException`, `InvalidTopicException`, `UnknownTopicOrPartition` | Message trop grand, topic inexistant | Abandon ou Dead Letter Queue |
| **Configuration** | âŒ Non | `AuthenticationException`, `AuthorizationException`, `SerializationException` | Credentials invalides, sÃ©rialisation Ã©chouÃ©e | Fix configuration |

#### Pattern de Gestion d'Erreurs ComplÃ¨te

var config = new ProducerConfig
{
    BootstrapServers = "...",
    
    // ===== RETRY AUTOMATIQUE =====
    MessageSendMaxRetries = 3,  // Nombre de retries pour erreurs retriables
    RetryBackoffMs = 1000,      // 1 seconde entre retries
    RequestTimeoutMs = 30000,   // 30 secondes timeout par requÃªte
    
    // ===== TIMEOUT GLOBAL =====
    TransactionTimeoutMs = 60000  // 60 secondes max pour transaction complÃ¨te
};

using var producer = new ProducerBuilder<string, string>(config)
    .SetErrorHandler((_, error) =>
    {
        // Callback appelÃ© pour erreurs non-fatales et fatales
        if (error.IsFatal)
        {
            logger.LogCritical(
                "Fatal Kafka error: Code={Code}, Reason={Reason}. Producer cannot continue.",
                error.Code, error.Reason
            );
            // En production : alerter Ã©quipe ops, arrÃªter gracieusement
            Environment.Exit(1);
        }
        else
        {
            logger.LogWarning(
                "Non-fatal Kafka error: Code={Code}, Reason={Reason}. Will retry if retriable.",
                error.Code, error.Reason
            );
        }
    })
    .Build();

try
{
    var result = await producer.ProduceAsync(topic, message);
    logger.LogInformation("Message sent successfully to partition {Partition}", result.Partition);
}
catch (ProduceException<string, string> ex)
{
    // Exception levÃ©e aprÃ¨s Ã©chec de tous les retries
    logger.LogError(ex, 
        "Failed to produce message after {Retries} retries. ErrorCode: {ErrorCode}, Reason: {Reason}",
        config.MessageSendMaxRetries, ex.Error.Code, ex.Error.Reason
    );
    
    // DÃ©cision basÃ©e sur le type d'erreur
    if (ex.Error.Code == ErrorCode.Local_MsgTimedOut ||
        ex.Error.Code == ErrorCode.Local_QueueFull)
    {
        // Erreur transiente qui a Ã©puisÃ© les retries
        logger.LogWarning("Transient error persisted. Consider increasing retry count or timeout.");
        await SendToRetryQueueAsync(message);
    }
    else if (ex.Error.Code == ErrorCode.MsgSizeTooLarge)
    {
        // Erreur permanente : message trop grand
        logger.LogError("Message size exceeds max.message.bytes. Sending to DLQ.");
        await SendToDeadLetterQueueAsync(message, ex);
    }
    else
    {
        // Autre erreur : DLQ par dÃ©faut
        await SendToDeadLetterQueueAsync(message, ex);
    }
}
catch (Exception ex)
{
    // Erreur inattendue (ex: serialization failure)
    logger.LogError(ex, "Unexpected error during message production");
    await SendToDeadLetterQueueAsync(message, ex);
}

#### Dead Letter Queue (DLQ) Pattern

private static async Task SendToDeadLetterQueueAsync(
    Message<string, string> failedMessage, 
    Exception originalException)
{
    // CrÃ©er producer sÃ©parÃ© pour DLQ (ou rÃ©utiliser existant)
    using var dlqProducer = new ProducerBuilder<string, string>(new ProducerConfig
    {
        BootstrapServers = config.BootstrapServers,
        ClientId = "dlq-producer"
    }).Build();
    
    var dlqMessage = new Message<string, string>
    {
        Key = failedMessage.Key,
        Value = failedMessage.Value,
        Headers = new Headers
        {
            // MÃ©tadonnÃ©es pour debugging
            { "original-topic", Encoding.UTF8.GetBytes("orders.created") },
            { "error-timestamp", Encoding.UTF8.GetBytes(DateTime.UtcNow.ToString("o")) },
            { "error-type", Encoding.UTF8.GetBytes(originalException.GetType().Name) },
            { "error-message", Encoding.UTF8.GetBytes(originalException.Message) },
            { "retry-count", Encoding.UTF8.GetBytes("3") },
            { "correlation-id", Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) }
        }
    };
    
    try
    {
        await dlqProducer.ProduceAsync("orders.dlq", dlqMessage);
        logger.LogWarning("Message sent to DLQ: Key={Key}", failedMessage.Key);
    }
    catch (Exception dlqEx)
    {
        // Si DLQ Ã©choue aussi, logger dans fichier ou DB
        logger.LogCritical(dlqEx, "Failed to send message to DLQ. Message lost: {Key}", failedMessage.Key);
        // En production : Ã©crire dans fichier local ou base de donnÃ©es
        await WriteToLocalFailureLog(failedMessage, originalException, dlqEx);
    }
}

private static async Task WriteToLocalFailureLog(
    Message<string, string> message, 
    Exception error1, 
    Exception error2)
{
    var logEntry = new
    {
        Timestamp = DateTime.UtcNow,
        Key = message.Key,
        Value = message.Value,
        OriginalError = error1.ToString(),
        DlqError = error2.ToString()
    };
    
    var logFile = $"/var/log/kafka-failures/{DateTime.UtcNow:yyyyMMdd}.log";
    await File.AppendAllTextAsync(logFile, 
        System.Text.Json.JsonSerializer.Serialize(logEntry) + Environment.NewLine
    );
}

ğŸ’¡ **TIP** : En production, configurez une alerte sur le topic DLQ pour Ãªtre notifiÃ© des Ã©checs.

âš ï¸ **ATTENTION** : Ne bloquez jamais le producer principal Ã  cause d'un Ã©chec DLQ. Utilisez fire-and-forget ou circuit breaker.

---

### ğŸ¯ RÃ©capitulatif Bloc 1.2

**Concepts maÃ®trisÃ©s** :
- âœ… Structure complÃ¨te d'un message Kafka (Key, Value, Headers, Timestamp, Partition, Offset)
- âœ… Partitionnement dÃ©terministe par clÃ© (hash-based)
- âœ… Configuration de base et avancÃ©e d'un Producer .NET
- âœ… Envoi synchrone avec `ProduceAsync` et gestion du `DeliveryResult`
- âœ… Gestion des erreurs (retriable vs permanent vs configuration)
- âœ… Pattern Dead Letter Queue pour messages Ã©chouÃ©s

**Code production-ready acquis** :
- Producer avec logging structurÃ©
- Error handling robuste avec classification des erreurs
- Configuration tunable (latence vs throughput)
- DLQ pattern pour rÃ©silience

**Tips clÃ©s Ã  retenir** :
1. **Toujours utiliser une clÃ©** si vous avez besoin d'ordre ou de localitÃ©
2. **Flush() avant fermeture** pour Ã©viter perte de messages en attente
3. **Acks = All en production** avec min.insync.replicas >= 2
4. **Retry automatique** pour erreurs retriables, DLQ pour erreurs permanentes
5. **Monitoring des DLQ** est critique en production

---

## BLOC 1.3 : PREMIER CONSUMER C# (2h30)

### Objectifs du Bloc
- DÃ©velopper un Consumer .NET robuste avec gestion d'Ã©tat
- Comprendre le polling loop et l'auto-commit vs manual-commit
- ImplÃ©menter le scaling horizontal avec Consumer Group
- Observer le rebalancing en action et gÃ©rer ses effets
- GÃ©rer le consumer lag

---

### 1.3.1 ThÃ©orie : Anatomie d'un Consumer

#### Le Poll Loop : CÅ“ur du Consumer

Un consumer Kafka fonctionne en **polling continu** :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CONSUMER POLL LOOP               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Subscribe  â”‚  â† S'abonner au topic (ou assign partitions)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Poll(timeout)  â”‚ â† Demander messages au broker (bloquant)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Process Records â”‚ â† Traiter chaque message (logique mÃ©tier)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Commit Offsets  â”‚ â† Sauvegarder position (auto ou manuel)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â””â”€â”€â”€â”€â”€â”€â†’ Boucler vers Poll

ğŸ’¡ **TIP** : Le poll loop doit Ãªtre **rapide et non-bloquant**. Si traitement > 5 minutes, augmentez `MaxPollIntervalMs`.

#### Configuration Consumer

var config = new ConsumerConfig
{
    // ===== OBLIGATOIRE =====
    BootstrapServers = "bhf-kafka-kafka-bootstrap:9092",
    
    // ===== IDENTIFIANT DU GROUPE =====
    GroupId = "inventory-service",  // Tous les consumers avec ce GroupId partagent les partitions
    
    // ===== POINT DE DÃ‰PART =====
    AutoOffsetReset = AutoOffsetReset.Earliest,  
    // Earliest: lire depuis le dÃ©but si pas d'offset sauvegardÃ©
    // Latest: lire nouveaux messages uniquement (dÃ©faut)
    // Error: lever exception si pas d'offset
    
    // ===== GESTION DES OFFSETS =====
    EnableAutoCommit = true,        // true = commit auto toutes les 5 secondes
    AutoCommitIntervalMs = 5000,    // Intervalle de commit (en ms)
    
    // ===== IDENTIFICATION =====
    ClientId = $"inventory-worker-{Environment.MachineName}",
    
    // ===== HEARTBEAT & SESSION =====
    SessionTimeoutMs = 10000,      // 10 secondes (consumer Ã©jectÃ© si pas de heartbeat)
    HeartbeatIntervalMs = 3000,    // 3 secondes (envoyer heartbeat)
    
    // ===== MAX POLL INTERVAL =====
    MaxPollIntervalMs = 300000     // 5 minutes (temps max entre 2 polls)
};

ğŸ’¡ **TIP** : Relation entre heartbeat et session :
HeartbeatIntervalMs < SessionTimeoutMs / 3
Exemple: 3000ms < 10000ms / 3 âœ“

âš ï¸ **ATTENTION** : `MaxPollIntervalMs` doit Ãªtre supÃ©rieur au temps de traitement d'un batch complet. Sinon, le consumer sera Ã©jectÃ© du groupe.

#### Offset Auto-Commit : Comportement

Timeline avec EnableAutoCommit = true :

T=0s    : Poll() retourne 100 messages, consumer commence traitement
T=3s    : Traitement de 60 messages terminÃ©
T=5s    : Auto-commit â†’ offsets des 100 messages sauvegardÃ©s (mÃªme ceux pas encore traitÃ©s !)
T=7s    : Crash du consumer (40 messages en cours de traitement)
T=10s   : RedÃ©marrage consumer â†’ reprend depuis offset 100 
          â†’ 40 messages perdus âŒ

**Trade-off** :
- âœ… **Avantage** : SimplicitÃ© (pas de code de commit), performance (moins d'appels rÃ©seau)
- âš ï¸ **Risque** : Perte de messages si crash entre commit et fin de traitement
- ğŸ¯ **Usage** : Acceptable pour use cases non critiques (logs, mÃ©triques, analytics)

ğŸ’¡ **TIP** : Pour use cases critiques (paiements, commandes), utilisez **manual commit** aprÃ¨s traitement rÃ©ussi.

---

### 1.3.2 Consumer Group & Rebalancing

#### Scaling Horizontal avec Consumer Groups

Un **Consumer Group** permet de parallÃ©liser la consommation d'un topic.

Topic: orders.created (6 partitions)

Scenario 1 : 1 consumer dans le groupe "inventory-service"
  Consumer-1 â†’ lit partitions 0, 1, 2, 3, 4, 5 (toutes)

Scenario 2 : 2 consumers dans le groupe "inventory-service"
  Consumer-1 â†’ lit partitions 0, 1, 2
  Consumer-2 â†’ lit partitions 3, 4, 5

Scenario 3 : 3 consumers dans le groupe "inventory-service"
  Consumer-1 â†’ lit partitions 0, 1
  Consumer-2 â†’ lit partitions 2, 3
  Consumer-3 â†’ lit partitions 4, 5

Scenario 4 : 6 consumers dans le groupe "inventory-service"
  Consumer-1 â†’ lit partition 0
  Consumer-2 â†’ lit partition 1
  Consumer-3 â†’ lit partition 2
  Consumer-4 â†’ lit partition 3
  Consumer-5 â†’ lit partition 4
  Consumer-6 â†’ lit partition 5

Scenario 5 : 8 consumers dans le groupe "inventory-service"
  Consumer-1 Ã  Consumer-6 â†’ chacun lit 1 partition
  Consumer-7 et Consumer-8 â†’ INACTIFS (plus de partitions disponibles)

**RÃ¨gle d'or** : `Nombre de consumers â‰¤ Nombre de partitions` pour utilisation optimale.

ğŸ’¡ **TIP** : Dimensionnez le nombre de partitions en anticipant le scaling futur :
- Trafic actuel : 1000 msgs/sec â†’ 3 partitions suffisent
- Trafic prÃ©vu dans 1 an : 10000 msgs/sec â†’ crÃ©ez 12 partitions dÃ¨s le dÃ©but

#### Rebalancing : Redistribution Automatique

**Triggers de rebalancing** :
1. Nouveau consumer rejoint le groupe
2. Consumer existant quitte le groupe (crash, shutdown gracieux, heartbeat timeout)
3. Nouveau topic ajoutÃ© (si subscription pattern avec regex)
4. Partition ajoutÃ©e au topic (rare)

**Phase de rebalancing** :
1. Group Coordinator dÃ©tecte changement (heartbeat manquant ou JoinGroup request)
2. PAUSE de tous les consumers du groupe (stop-the-world)
3. RÃ©assignation des partitions selon stratÃ©gie (RoundRobin/Range/CooperativeSticky)
4. Consumers reprennent la consommation avec nouvelles partitions

**Exemple visuel avec timestamps** :
T=0 : 2 consumers actifs
  Consumer-1 â†’ Partitions [0, 1, 2]
  Consumer-2 â†’ Partitions [3, 4, 5]
  
  Consumer-2 traite messages des partitions 3, 4, 5...

T=10 : Consumer-2 crash (plus de heartbeat)

T=20 : Group Coordinator dÃ©tecte timeout (SessionTimeoutMs = 10s)
  âš ï¸ Rebalancing dÃ©clenchÃ©
  Consumer-1 â†’ PAUSE (arrÃªte consommation)

T=22 : Rebalancing terminÃ©
  Consumer-1 â†’ Partitions [0, 1, 2, 3, 4, 5]  â† A rÃ©cupÃ©rÃ© les partitions de Consumer-2
  Consumer-1 â†’ REPREND consommation

T=22-T=30 : Consumer-1 traite seul les 6 partitions (throughput rÃ©duit de moitiÃ©)

âš ï¸ **ATTENTION** : Pendant le rebalancing (T=20 Ã  T=22), **aucun** message n'est consommÃ©. C'est le "stop-the-world" du consumer group.

**Configuration du rebalancing** :
var config = new ConsumerConfig
{
    // ...
    
    // ===== TIMEOUT DE SESSION =====
    SessionTimeoutMs = 10000,  // 10 secondes (dÃ©faut: 45s)
    // RÃ©duire = dÃ©tection rapide des crashs
    // Augmenter = tolÃ©rer latence rÃ©seau/GC pauses
    
    // ===== INTERVALLE DE HEARTBEAT =====
    HeartbeatIntervalMs = 3000,  // 3 secondes (doit Ãªtre < SessionTimeout/3)
    
    // ===== MAX POLL INTERVAL =====
    MaxPollIntervalMs = 300000,  // 5 minutes
    // Si temps entre 2 polls dÃ©passe cette valeur â†’ consumer Ã©jectÃ© du groupe
    // Augmenter si traitement lent (ex: 10 minutes pour batch processing)
    
    // ===== STRATÃ‰GIE D'ASSIGNATION =====
    PartitionAssignmentStrategy = PartitionAssignmentStrategy.CooperativeSticky
    // RoundRobin: distribution circulaire Ã©quitable
    // Range: partitions consÃ©cutives par consumer
    // CooperativeSticky: minimise rebalancing (recommandÃ© production)
};

ğŸ’¡ **TIP** : StratÃ©gie **CooperativeSticky** (depuis Kafka 2.4+) :
- Ã‰vite le "stop-the-world" complet
- Seules les partitions affectÃ©es sont rÃ©assignÃ©es
- Les autres consumers continuent de consommer
- **10x plus rapide** que RoundRobin/Range pour grands groupes

---

### LAB 1.3A : Consumer Basique (Auto-Commit)

#### Objectif
CrÃ©er un consumer .NET qui lit les messages du topic `orders.created` et les traite avec logging dÃ©taillÃ©.

#### Code : Program.cs

using Confluent.Kafka;
using Microsoft.Extensions.Logging;

var loggerFactory = LoggerFactory.Create(builder =>
{
    builder.AddConsole();
    builder.SetMinimumLevel(LogLevel.Information);
});
var logger = loggerFactory.CreateLogger<Program>();

// ===== CONFIGURATION =====
var config = new ConsumerConfig
{
    BootstrapServers = Environment.GetEnvironmentVariable("KAFKA_BOOTSTRAP_SERVERS") 
                       ?? "bhf-kafka-kafka-bootstrap:9092",
    GroupId = Environment.GetEnvironmentVariable("KAFKA_GROUP_ID") 
              ?? "inventory-service",
    ClientId = $"inventory-worker-{Environment.MachineName}-{Guid.NewGuid():N}",
    
    // Lire depuis le dÃ©but si pas d'offset sauvegardÃ©
    AutoOffsetReset = AutoOffsetReset.Earliest,
    
    // Auto-commit des offsets toutes les 5 secondes
    EnableAutoCommit = true,
    AutoCommitIntervalMs = 5000,
    
    // Timeout de session (rebalancing si heartbeat manquant)
    SessionTimeoutMs = 10000,
    HeartbeatIntervalMs = 3000,
    
    // Max 5 minutes entre 2 polls
    MaxPollIntervalMs = 300000,
    
    // StratÃ©gie de rebalancing (CooperativeSticky recommandÃ©)
    PartitionAssignmentStrategy = PartitionAssignmentStrategy.CooperativeSticky
};

// ===== CRÃ‰ATION DU CONSUMER =====
using var consumer = new ConsumerBuilder<string, string>(config)
    .SetErrorHandler((_, e) => 
    {
        logger.LogError("Consumer error: Code={Code}, Reason={Reason}, IsFatal={IsFatal}", 
            e.Code, e.Reason, e.IsFatal);
    })
    .SetPartitionsAssignedHandler((c, partitions) =>
    {
        // Callback appelÃ© lors de l'assignation de partitions (aprÃ¨s rebalancing)
        logger.LogInformation("âœ“ Partitions assigned: {Partitions}",
            string.Join(", ", partitions.Select(p => $"{p.Topic}[{p.Partition.Value}]")));
        
        // Log des offsets actuels pour chaque partition
        foreach (var partition in partitions)
        {
            var committed = c.Committed(new[] { partition }, TimeSpan.FromSeconds(5));
            var offset = committed.FirstOrDefault()?.Offset ?? Offset.Unset;
            logger.LogInformation("  â†’ Partition {Partition}: starting from offset {Offset}", 
                partition.Partition.Value, offset);
        }
    })
    .SetPartitionsRevokedHandler((c, partitions) =>
    {
        // Callback appelÃ© lors de la rÃ©vocation de partitions (avant rebalancing)
        logger.LogWarning("âš ï¸ Partitions revoked: {Partitions}",
            string.Join(", ", partitions.Select(p => $"{p.Topic}[{p.Partition.Value}]")));
    })
    .SetPartitionsLostHandler((c, partitions) =>
    {
        // Callback appelÃ© si partitions perdues (ex: timeout)
        logger.LogError("âŒ Partitions lost: {Partitions}",
            string.Join(", ", partitions.Select(p => $"{p.Topic}[{p.Partition.Value}]")));
    })
    .SetStatisticsHandler((_, stats) =>
    {
        // Stats Kafka internes (toutes les 60 secondes par dÃ©faut)
        // Utile pour monitoring consumer lag, throughput, etc.
        logger.LogDebug("Consumer stats: {Stats}", stats);
    })
    .Build();

// ===== SUBSCRIPTION =====
const string topicName = "orders.created";
consumer.Subscribe(topicName);
logger.LogInformation("Consumer started. Subscribed to topic '{Topic}', Group: '{Group}'", 
    topicName, config.GroupId);

// ===== POLL LOOP =====
var cancellationTokenSource = new CancellationTokenSource();
Console.CancelKeyPress += (_, e) =>
{
    e.Cancel = true;  // EmpÃªcher arrÃªt brutal
    cancellationTokenSource.Cancel();
    logger.LogInformation("Shutdown requested (Ctrl+C). Graceful shutdown in progress...");
};

try
{
    while (!cancellationTokenSource.Token.IsCancellationRequested)
    {
        try
        {
            // Poll avec timeout de 100ms
            var consumeResult = consumer.Consume(cancellationTokenSource.Token);
            
            if (consumeResult == null)
                continue;  // Timeout, pas de message disponible
            
            // Traitement du message
            logger.LogInformation(
                "ğŸ“¦ Message received â†’ Topic: {Topic}, Partition: {Partition}, Offset: {Offset}, Key: {Key}, Timestamp: {Timestamp}",
                consumeResult.Topic,
                consumeResult.Partition.Value,
                consumeResult.Offset.Value,
                consumeResult.Message.Key ?? "(null)",
                consumeResult.Message.Timestamp.UtcDateTime
            );
            
            logger.LogDebug("  Value: {Value}", consumeResult.Message.Value);
            
            // Log des headers si prÃ©sents
            if (consumeResult.Message.Headers != null && consumeResult.Message.Headers.Count > 0)
            {
                var headers = consumeResult.Message.Headers
                    .Select(h => $"{h.Key}={Encoding.UTF8.GetString(h.GetValueBytes())}");
                logger.LogDebug("  Headers: {Headers}", string.Join(", ", headers));
            }
            
            // Simuler traitement (ex: mise Ã  jour inventaire)
            await ProcessOrderAsync(consumeResult.Message.Value);
            
            // NOTE : Pas besoin de commit manuel (EnableAutoCommit = true)
            // Le commit se fera automatiquement toutes les 5 secondes
        }
        catch (ConsumeException ex)
        {
            logger.LogError(ex, "Consume error: {Reason}", ex.Error.Reason);
            
            // Ne pas crasher le consumer pour une erreur de consume
            // Continuer le poll loop
        }
    }
}
catch (OperationCanceledException)
{
    logger.LogInformation("Poll loop interrupted (graceful shutdown)");
}
finally
{
    // Fermeture propre : commit final + quitter le groupe + trigger rebalancing
    logger.LogInformation("Closing consumer...");
    consumer.Close();
    logger.LogInformation("Consumer closed gracefully.");
}

// ===== FONCTION DE TRAITEMENT =====
async Task ProcessOrderAsync(string orderJson)
{
    try
    {
        // Simuler traitement (ex: requÃªte DB pour rÃ©server stock)
        await Task.Delay(100);  // 100ms de traitement
        
        // Parser JSON (basique pour dÃ©mo, utilisez System.Text.Json.JsonSerializer en production)
        var orderId = ExtractOrderId(orderJson);
        
        logger.LogInformation("  âœ“ Inventory updated for order {OrderId}", orderId);
    }
    catch (Exception ex)
    {
        logger.LogError(ex, "Error processing order");
        // En production : retry, DLQ, alerting, etc.
    }
}

string ExtractOrderId(string json)
{
    // Extraction basique (Ã  remplacer par JSON deserialization)
    var match = System.Text.RegularExpressions.Regex.Match(json, @"""orderId"":\s*""([^""]+)""");
    return match.Success ? match.Groups[1].Value : "unknown";
}

ğŸ’¡ **TIP** : Utilisez `SetPartitionsAssignedHandler` pour initialiser des ressources locales (connexion DB, cache) avant de consommer.

#### DÃ©ploiement sur OpenShift

**Dockerfile** (mÃªme structure que Producer) :
FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /app
COPY *.csproj .
RUN dotnet restore
COPY . .
RUN dotnet publish -c Release -o out

FROM mcr.microsoft.com/dotnet/runtime:8.0
WORKDIR /app
COPY --from=build /app/out .
USER 1001
ENTRYPOINT ["dotnet", "KafkaConsumerBasic.dll"]

**Deployment YAML** :
apiVersion: apps/v1
kind: Deployment
metadata:
  name: consumer-basic
  namespace: kafka
spec:
  replicas: 1  # On commencera avec 1, puis scalera
  selector:
    matchLabels:
      app: consumer-basic
  template:
    metadata:
      labels:
        app: consumer-basic
        version: v1
    spec:
      containers:
      - name: consumer
        image: image-registry.openshift-image-registry.svc:5000/kafka/consumer-basic:v1
        env:
        - name: KAFKA_BOOTSTRAP_SERVERS
          value: "bhf-kafka-kafka-bootstrap:9092"
        - name: KAFKA_GROUP_ID
          value: "inventory-service"
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        # Liveness probe pour dÃ©tecter consumer bloquÃ©
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - pgrep -f "dotnet.*KafkaConsumerBasic.dll" || exit 1
          initialDelaySeconds: 30
          periodSeconds: 10
          failureThreshold: 3

**DÃ©ployer** :
# Build & push image
docker build -t consumer-basic:v1 .
docker tag consumer-basic:v1 image-registry.openshift-image-registry.svc:5000/kafka/consumer-basic:v1
docker push image-registry.openshift-image-registry.svc:5000/kafka/consumer-basic:v1

# DÃ©ployer
oc apply -f deployment.yaml -n kafka

# Observer les logs
oc logs -f deployment/consumer-basic -n kafka

#### âœ… Validation

Logs attendus :
info: Program[0]
      Consumer started. Subscribed to topic 'orders.created', Group: 'inventory-service'
info: Program[0]
      âœ“ Partitions assigned: orders.created[0], orders.created[1], orders.created[2], orders.created[3], orders.created[4], orders.created[5]
info: Program[0]
        â†’ Partition 0: starting from offset 0
info: Program[0]
        â†’ Partition 1: starting from offset 0
info: Program[0]
      ğŸ“¦ Message received â†’ Topic: orders.created, Partition: 3, Offset: 0, Key: customer-A, Timestamp: 2026-02-05T10:30:45.123Z
info: Program[0]
        âœ“ Inventory updated for order ORD-customer-A-0001

**Points Ã  noter** :
- Les 6 partitions sont assignÃ©es au seul consumer
- Messages consommÃ©s dans l'ordre d'offset (par partition, pas global)
- Auto-commit toutes les 5 secondes (pas visible dans logs par dÃ©faut)
- Latence de traitement : ~100ms par message

ğŸ’¡ **TIP** : Pour voir le consumer lag en temps rÃ©el :
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group inventory-service \
  --describe

# Output :
# GROUP           TOPIC          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
# inventory-...   orders.created 0          15              15              0
# inventory-...   orders.created 1          12              12              0

---

### LAB 1.3B : Consumer Group Scaling & Rebalancing

#### Objectif
Observer le rebalancing en dÃ©ployant 2 consumers dans le mÃªme groupe, puis en tuant un consumer, puis en scaling Ã  6 replicas.

#### Ã‰tape 1 : Scaler le Deployment Ã  2 Replicas

# Scaler Ã  2 instances
oc scale deployment/consumer-basic --replicas=2 -n kafka

# Observer les pods
oc get pods -l app=consumer-basic -n kafka
# Devrait montrer 2 pods Running

# Suivre les logs des 2 pods simultanÃ©ment
oc logs -f deployment/consumer-basic --all-containers=true --max-log-requests=10 -n kafka

#### Logs Attendus : Rebalancing Automatique

**Pod 1** (consumer existant) :
info: Program[0]
      âœ“ Partitions assigned: orders.created[0], orders.created[1], orders.created[2], orders.created[3], orders.created[4], orders.created[5]
info: Program[0]
      ğŸ“¦ Message received â†’ Partition: 2, Offset: 5, Key: customer-C...
warn: Program[0]
      âš ï¸ Partitions revoked: orders.created[0], orders.created[1], orders.created[2], orders.created[3], orders.created[4], orders.created[5]
info: Program[0]
      âœ“ Partitions assigned: orders.created[0], orders.created[1], orders.created[2]  â† RÃ©duit Ã  3 partitions
info: Program[0]
      ğŸ“¦ Message received â†’ Partition: 0, Offset: 10...

**Pod 2** (nouveau consumer) :
info: Program[0]
      Consumer started. Subscribed to topic 'orders.created', Group: 'inventory-service'
info: Program[0]
      âœ“ Partitions assigned: orders.created[3], orders.created[4], orders.created[5]  â† RÃ©cupÃ¨re 3 partitions
info: Program[0]
        â†’ Partition 3: starting from offset 8
        â†’ Partition 4: starting from offset 6
        â†’ Partition 5: starting from offset 12
info: Program[0]
      ğŸ“¦ Message received â†’ Partition: 3, Offset: 8, Key: customer-D...

**Observation** :
- âœ… Pod 1 a d'abord les 6 partitions (consumer unique)
- âš ï¸ Rebalancing dÃ©clenchÃ© quand Pod 2 rejoint le groupe (JoinGroup request)
- âœ… Distribution finale : Pod 1 â†’ [0,1,2], Pod 2 â†’ [3,4,5]
- â±ï¸ DurÃ©e du rebalancing : ~2-3 secondes (stop-the-world)

ğŸ’¡ **TIP** : Avec **CooperativeSticky**, le rebalancing est beaucoup plus rapide car Pod 1 garde [0,1,2] sans interruption, seul [3,4,5] est rÃ©assignÃ©.

#### Ã‰tape 2 : Simuler Crash d'un Consumer

# Identifier les pods
oc get pods -l app=consumer-basic

# Tuer un pod (simuler crash brutal)
oc delete pod consumer-basic-xxxx-yyyy --grace-period=0 --force

# Observer les logs du pod survivant
oc logs -f deployment/consumer-basic --tail=50

**Logs du pod survivant** :
info: Program[0]
      ğŸ“¦ Message received â†’ Partition: 1, Offset: 25...
# (Pod 2 crashe ici)

# AprÃ¨s SessionTimeoutMs (10 secondes)
warn: Program[0]
      âš ï¸ Partitions revoked: orders.created[0], orders.created[1], orders.created[2]
info: Program[0]
      âœ“ Partitions assigned: orders.created[0], orders.created[1], orders.created[2], orders.created[3], orders.created[4], orders.created[5]  â† RÃ©cupÃ¨re toutes les partitions
info: Program[0]
        â†’ Partition 3: starting from offset 15  â† Reprend depuis dernier offset commitÃ©

**Observation** :
- âš ï¸ Rebalancing dÃ©clenchÃ© aprÃ¨s `SessionTimeoutMs` (10 secondes sans heartbeat de Pod 2)
- âœ… Pod survivant rÃ©cupÃ¨re les 6 partitions automatiquement
- âœ… Aucune perte de messages (offsets commitÃ©s avant crash)
- â±ï¸ DurÃ©e totale : 10 secondes (dÃ©tection) + 2 secondes (rebalancing) = **12 secondes de lag**

ğŸ’¡ **TIP** : RÃ©duire `SessionTimeoutMs` Ã  6000ms (6 secondes) pour dÃ©tecter crashs plus rapidement, mais attention aux faux positifs en cas de GC pause ou latence rÃ©seau.

#### Ã‰tape 3 : Scaler Ã  6 Replicas (Optimal)

# Topic a 6 partitions, scaler Ã  6 consumers
oc scale deployment/consumer-basic --replicas=6 -n kafka

# Observer distribution
oc logs deployment/consumer-basic --tail=10 | grep "Partitions assigned"

# Attendu :
# Pod 1: orders.created[0]
# Pod 2: orders.created[1]
# Pod 3: orders.created[2]
# Pod 4: orders.created[3]
# Pod 5: orders.created[4]
# Pod 6: orders.created[5]

**Distribution optimale** :
Pod 1: Partition [0] uniquement
Pod 2: Partition [1] uniquement
Pod 3: Partition [2] uniquement
Pod 4: Partition [3] uniquement
Pod 5: Partition [4] uniquement
Pod 6: Partition [5] uniquement

Chaque consumer lit exactement 1 partition â†’ **parallÃ©lisme maximal** â†’ throughput maximal.

ğŸ’¡ **TIP** : Formule pour throughput total :
Throughput_total = Throughput_par_consumer Ã— min(N_consumers, N_partitions)

Exemple :
- 1 consumer traite 100 msgs/sec
- Topic a 6 partitions
- Throughput max = 100 Ã— 6 = 600 msgs/sec

#### Ã‰tape 4 : Scaler Ã  8 Replicas (Sur-capacitÃ©)

# Scaler Ã  8 consumers (plus que de partitions)
oc scale deployment/consumer-basic --replicas=8 -n kafka

# Observer les pods
oc get pods -l app=consumer-basic

# Observer logs
oc logs deployment/consumer-basic --tail=20 | grep -E "(Partitions assigned|started)"

**Observation** :
Pod 1 Ã  Pod 6: Chacun a 1 partition
Pod 7: âœ“ Partitions assigned: (empty)  â† AUCUNE partition assignÃ©e
Pod 8: âœ“ Partitions assigned: (empty)  â† AUCUNE partition assignÃ©e

**Pod 7 et 8 sont INACTIFS** car il n'y a plus de partitions disponibles.

ğŸ’¡ **TIP** : Les consumers inactifs ne consomment presque pas de ressources (juste heartbeats), mais c'est du gaspillage. Dimensionnez toujours `N_replicas â‰¤ N_partitions`.

#### âœ… Validation

- [ ] Comprendre que consumers d'un mÃªme `GroupId` partagent les partitions
- [ ] Observer le rebalancing automatique (join/leave/crash)
- [ ] Savoir que `N_consumers > N_partitions` â†’ consumers inactifs
- [ ] Distribution optimale = `N_consumers = N_partitions`
- [ ] DurÃ©e de rebalancing : 10-15 secondes avec RoundRobin, 2-3 secondes avec CooperativeSticky

**ğŸ“¸ Screenshot Ã  prendre** : `oc get pods` avec 6 replicas + logs montrant distribution 1:1

---

### 1.3.3 Gestion des Erreurs Consumer

#### Pattern de Retry avec Exponential Backoff

private static async Task<bool> ProcessWithRetryAsync(
    ConsumeResult<string, string> consumeResult, 
    ILogger logger,
    int maxRetries = 3)
{
    for (int attempt = 1; attempt <= maxRetries; attempt++)
    {
        try
        {
            await ProcessOrderAsync(consumeResult.Message.Value);
            return true;  // SuccÃ¨s
        }
        catch (Exception ex)
        {
            logger.LogWarning(ex, 
                "Processing attempt {Attempt}/{MaxRetries} failed for partition {Partition}, offset {Offset}",
                attempt, maxRetries, consumeResult.Partition.Value, consumeResult.Offset.Value);
            
            if (attempt < maxRetries)
            {
                // Exponential backoff : 1s, 2s, 4s, 8s...
                var delay = TimeSpan.FromSeconds(Math.Pow(2, attempt - 1));
                logger.LogInformation("  Retrying in {Delay} seconds...", delay.TotalSeconds);
                await Task.Delay(delay);
            }
            else
            {
                // Ã‰chec aprÃ¨s tous les retries
                logger.LogError(ex, "Processing failed definitively after {MaxRetries} retries", maxRetries);
                return false;
            }
        }
    }
    
    return false;  // Ã‰chec aprÃ¨s tous les retries
}

ğŸ’¡ **TIP** : Ajoutez du **jitter** au backoff pour Ã©viter les retry storms :
var delay = TimeSpan.FromSeconds(Math.Pow(2, attempt - 1) + Random.Shared.NextDouble());

#### Dead Letter Queue (DLQ) pour Consumer

private static async Task SendToDeadLetterQueueAsync(ConsumeResult<string, string> failedMessage)
{
    using var dlqProducer = new ProducerBuilder<string, string>(new ProducerConfig
    {
        BootstrapServers = config.BootstrapServers,
        ClientId = "consumer-dlq-producer"
    }).Build();
    
    var dlqValue = new
    {
        OriginalTopic = failedMessage.Topic,
        OriginalPartition = failedMessage.Partition.Value,
        OriginalOffset = failedMessage.Offset.Value,
        OriginalKey = failedMessage.Message.Key,
        OriginalValue = failedMessage.Message.Value,
        OriginalTimestamp = failedMessage.Message.Timestamp.UtcDateTime,
        ErrorTimestamp = DateTime.UtcNow,
        ErrorReason = "Max retries exceeded (3 attempts)",
        ConsumerGroupId = config.GroupId,
        ConsumerClientId = config.ClientId
    };
    
    var dlqMessage = new Message<string, string>
    {
        Key = failedMessage.Message.Key,
        Value = System.Text.Json.JsonSerializer.Serialize(dlqValue),
        Headers = new Headers(failedMessage.Message.Headers)  // Copier headers originaux
        {
            // Ajouter headers DLQ
            { "dlq-timestamp", Encoding.UTF8.GetBytes(DateTime.UtcNow.ToString("o")) },
            { "dlq-reason", Encoding.UTF8.GetBytes("max-retries-exceeded") },
            { "dlq-original-partition", BitConverter.GetBytes(failedMessage.Partition.Value) },
            { "dlq-original-offset", BitConverter.GetBytes((long)failedMessage.Offset.Value) }
        }
    };
    
    await dlqProducer.ProduceAsync("orders.dlq", dlqMessage);
    logger.LogWarning("Message sent to DLQ: Key={Key}, OriginalOffset={Offset}", 
        failedMessage.Message.Key, failedMessage.Offset.Value);
}

ğŸ’¡ **TIP** : CrÃ©ez un consumer sÃ©parÃ© pour monitorer le DLQ et alerter l'Ã©quipe ops :
# VÃ©rifier nombre de messages dans DLQ
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 \
  --topic orders.dlq \
  --time -1

# Si offset > 0 â†’ alerter Ã©quipe

---

### ğŸ¯ RÃ©capitulatif Bloc 1.3

**Concepts maÃ®trisÃ©s** :
- âœ… Poll loop et cycle de vie du Consumer
- âœ… Auto-commit vs manual commit (avantages/risques)
- âœ… Consumer Group et partitionnement automatique
- âœ… Rebalancing automatique (triggers, durÃ©e, stratÃ©gies)
- âœ… Scaling horizontal optimal (N consumers = N partitions)
- âœ… Gestion d'erreurs avec retry + DLQ

**Skills pratiques** :
- Consumer .NET production-ready avec handlers de partition
- DÃ©ploiement multi-replicas sur OpenShift
- Observation du rebalancing en temps rÃ©el
- Monitoring du consumer lag

**Tips clÃ©s Ã  retenir** :
1. **Auto-commit OK pour logs/mÃ©triques**, manual commit pour use cases critiques
2. **CooperativeSticky minimise downtime** pendant rebalancing
3. **Dimensionner partitions = max consumers** prÃ©vu dans 1-2 ans
4. **SessionTimeoutMs trade-off** : faible = dÃ©tection rapide, Ã©levÃ© = tolÃ©rance rÃ©seau
5. **Toujours DLQ** pour messages non-traitables aprÃ¨s retries

---

## BLOC 1.4 : RÃ‰CAPITULATIF & Q&A (1h)

### 1.4.1 DÃ©mo End-to-End ComplÃ¨te

#### Architecture du SystÃ¨me

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OrderAPI      â”‚  â† API REST .NET (Producer)
â”‚   (Producer)    â”‚     POST /orders
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Kafka Topic       â”‚
â”‚  orders.created    â”‚  â† 6 partitions, replication.factor=3
â”‚  (6 partitions)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InventoryServiceâ”‚ â”‚ PaymentService  â”‚ â”‚NotificationSvc  â”‚
â”‚   (Consumer)    â”‚ â”‚   (Consumer)    â”‚ â”‚   (Consumer)    â”‚
â”‚   Group: inv    â”‚ â”‚   Group: pay    â”‚ â”‚   Group: notif  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
         â–¼                  â–¼                  â–¼
    PostgreSQL          Stripe API         SendGrid API

#### Code : OrderAPI (Minimal Producer)

// Program.cs - API REST avec Kafka Producer intÃ©grÃ©
using Confluent.Kafka;
using Microsoft.AspNetCore.Mvc;

var builder = WebApplication.CreateBuilder(args);

// Singleton Producer (rÃ©utilisÃ© pour toutes les requÃªtes - IMPORTANT)
builder.Services.AddSingleton<IProducer<string, string>>(sp =>
{
    var config = new ProducerConfig
    {
        BootstrapServers = Environment.GetEnvironmentVariable("KAFKA_BOOTSTRAP_SERVERS") 
                           ?? "bhf-kafka-kafka-bootstrap:9092",
        ClientId = "order-api",
        Acks = Acks.All,
        EnableIdempotence = true,  // Ã‰viter duplicatas
        LingerMs = 10,             // Batching pour performance
        CompressionType = CompressionType.Lz4
    };
    return new ProducerBuilder<string, string>(config).Build();
});

// Logging
builder.Services.AddLogging(logging =>
{
    logging.AddConsole();
    logging.SetMinimumLevel(LogLevel.Information);
});

var app = builder.Build();

// Healthcheck endpoint
app.MapGet("/health", () => Results.Ok(new { status = "healthy", timestamp = DateTime.UtcNow }));

// Endpoint POST /orders
app.MapPost("/orders", async (
    [FromBody] OrderDto order,
    [FromServices] IProducer<string, string> producer,
    [FromServices] ILogger<Program> logger) =>
{
    // Validation
    if (string.IsNullOrEmpty(order.CustomerId) || order.Items == null || !order.Items.Any())
    {
        return Results.BadRequest(new { error = "Invalid order: CustomerId and Items are required" });
    }
    
    var orderId = Guid.NewGuid().ToString();
    var orderEvent = new
    {
        orderId,
        order.CustomerId,
        order.Items,
        order.TotalAmount,
        CreatedAt = DateTime.UtcNow
    };
    
    var messageValue = System.Text.Json.JsonSerializer.Serialize(orderEvent);
    
    try
    {
        // Publier Ã©vÃ©nement dans Kafka (clÃ© = customerId pour ordre garanti)
        var result = await producer.ProduceAsync("orders.created", new Message<string, string>
        {
            Key = order.CustomerId,  // Partitionnement par client
            Value = messageValue,
            Headers = new Headers
            {
                { "correlation-id", Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) },
                { "source", Encoding.UTF8.GetBytes("order-api") },
                { "timestamp", Encoding.UTF8.GetBytes(DateTime.UtcNow.ToString("o")) }
            }
        });
        
        logger.LogInformation(
            "Order created: OrderId={OrderId}, CustomerId={CustomerId}, Partition={Partition}, Offset={Offset}",
            orderId, order.CustomerId, result.Partition.Value, result.Offset.Value
        );
        
        // Retourner 202 Accepted (traitement asynchrone)
        return Results.Accepted($"/orders/{orderId}", new 
        { 
            orderId, 
            status = "Processing",
            message = "Order accepted and sent to processing queue"
        });
    }
    catch (ProduceException<string, string> ex)
    {
        logger.LogError(ex, "Failed to produce order event: {ErrorCode}", ex.Error.Code);
        return Results.Problem(
            title: "Order processing failed",
            detail: $"Unable to queue order: {ex.Error.Reason}",
            statusCode: 503
        );
    }
});

app.Run();

// DTOs
record OrderDto(string CustomerId, List<string> Items, decimal TotalAmount);

ğŸ’¡ **TIP** : Utilisez toujours un **singleton Producer** en ASP.NET Core. CrÃ©er un producer par requÃªte est extrÃªmement inefficace (connexion TCP Ã  chaque fois).

#### DÃ©ploiement & Test

**1. DÃ©ployer OrderAPI** :
oc apply -f order-api-deployment.yaml
oc expose svc/order-api

# RÃ©cupÃ©rer URL
ORDER_API_URL=$(oc get route order-api -o jsonpath='{.spec.host}')
echo "Order API URL: https://$ORDER_API_URL"

**2. Envoyer commandes** :
# Test 1 : Commande valide
curl -X POST https://$ORDER_API_URL/orders \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "customer-123",
    "items": ["PROD-A", "PROD-B", "PROD-C"],
    "totalAmount": 299.99
  }'

# RÃ©ponse attendue : 
# {"orderId":"xxx","status":"Processing","message":"Order accepted and sent to processing queue"}

# Test 2 : Plusieurs commandes du mÃªme client (ordre garanti)
for i in {1..5}; do
  curl -X POST https://$ORDER_API_URL/orders \
    -H "Content-Type: application/json" \
    -d "{
      \"customerId\": \"customer-456\",
      \"items\": [\"PROD-$i\"],
      \"totalAmount\": $((100 + i * 50))
    }"
  echo ""
  sleep 0.5
done

**3. Observer les Consumers** :
# InventoryService logs
oc logs -f deployment/inventory-service

# Devrait montrer :
# ğŸ“¦ Message received â†’ Key: customer-123, Partition: 3, Offset: 15
# ğŸ“¦ Message received â†’ Key: customer-456, Partition: 1, Offset: 8
# ğŸ“¦ Message received â†’ Key: customer-456, Partition: 1, Offset: 9  â† MÃªme partition !

**4. VÃ©rifier dans Kafka** :
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders.created \
  --from-beginning \
  --property print.key=true \
  --property key.separator=" â†’ " \
  --max-messages 10

# Output :
# customer-123 â†’ {"orderId":"...","customerId":"customer-123",...}
# customer-456 â†’ {"orderId":"...","customerId":"customer-456",...}

#### âœ… Validation End-to-End

- [ ] OrderAPI accepte requÃªte HTTP et retourne 202 Accepted en <100ms
- [ ] Message apparaÃ®t dans topic `orders.created` en <50ms
- [ ] InventoryService consomme le message en <200ms
- [ ] PaymentService et NotificationService consomment aussi (consumer groups diffÃ©rents)
- [ ] Latence totale (HTTP request â†’ tous consumers) < 500ms
- [ ] MÃªme client â†’ mÃªme partition â†’ ordre prÃ©servÃ©

ğŸ’¡ **TIP** : Utilisez distributed tracing (OpenTelemetry) pour visualiser le flow complet :
HTTP POST â†’ OrderAPI (50ms) â†’ Kafka (20ms) â†’ InventoryService (100ms) â†’ Total: 170ms

---

### 1.4.2 Quiz RÃ©capitulatif Jour 1

#### Question 1 : Concepts Fondamentaux

**Q** : Un topic Kafka avec 12 partitions et un consumer group de 5 consumers. Combien de partitions chaque consumer gÃ©rera-t-il ?

<details>
<summary>RÃ©ponse</summary>

**RÃ©ponse** : 3 consumers auront 3 partitions chacun, et 2 consumers auront 2 partitions chacun (distribution 3-3-2-2-2 ou similaire selon stratÃ©gie).

**Explication** : 
- 12 partitions / 5 consumers = 2.4 â†’ distribution inÃ©gale
- RoundRobin : partitions rÃ©parties circulairement â†’ 3-2-3-2-2
- Range : partitions consÃ©cutives par consumer â†’ 3-3-2-2-2
- CooperativeSticky : conserve assignations prÃ©cÃ©dentes + Ã©quilibre
</details>

#### Question 2 : Partitionnement

**Q** : Quel code garantit que tous les Ã©vÃ©nements du client "customer-456" arrivent dans l'ordre **ET** que la charge est distribuÃ©e Ã©quitablement entre partitions ?

A)
await producer.ProduceAsync("orders", new Message<Null, string> { Value = orderJson });

B)
await producer.ProduceAsync("orders", new Message<string, string> { Key = "customer-456", Value = orderJson });

C)
var partition = customerId.GetHashCode() % 12;
await producer.ProduceAsync("orders", new Message<string, string> { Key = "customer-456", Value = orderJson },
    new TopicPartition("orders", partition));

<details>
<summary>RÃ©ponse</summary>

**RÃ©ponse** : B

**Explication** : 
- **A** : Pas de clÃ© â†’ round-robin â†’ ordre non garanti pour un mÃªme client
- **B** : ClÃ© = customerId â†’ hash-based partitioning â†’ ordre garanti + distribution Ã©quitable si clÃ©s variÃ©es âœ“
- **C** : Assignation manuelle de partition â†’ Ã©vite le hash Kafka â†’ peut crÃ©er hot partitions si customerId mal distribuÃ©s

ğŸ’¡ **Conseil** : Laissez toujours Kafka gÃ©rer le partitionnement via le hash de la clÃ©, sauf cas trÃ¨s spÃ©cifiques.
</details>

#### Question 3 : Auto-Commit

**Q** : Un consumer avec `EnableAutoCommit = true` et `AutoCommitIntervalMs = 5000` traite 200 messages/seconde (5ms par message). Il crash aprÃ¨s 8 secondes de traitement. Combien de messages seront retraitÃ©s au redÃ©marrage ?

<details>
<summary>RÃ©ponse</summary>

**RÃ©ponse** : ~600 messages (messages traitÃ©s entre T=5s et T=8s)

**Explication** : 
- T=0s : Poll() â†’ dÃ©marrage
- T=5s : Auto-commit â†’ offsets sauvegardÃ©s pour messages dÃ©jÃ  traitÃ©s (0 Ã  1000)
- T=5s Ã  T=8s : Traitement de 200 msg/s Ã— 3s = 600 messages supplÃ©mentaires
- T=8s : Crash â†’ ces 600 messages pas encore commitÃ©s
- T=redÃ©marrage : Reprend depuis dernier commit (offset Ã  T=5s) â†’ retraite les 600 messages

ğŸ’¡ **Conseil** : Pour use cases critiques, utilisez **manual commit aprÃ¨s traitement** pour Ã©viter duplication.
</details>

#### Question 4 : Rebalancing

**Q** : Un consumer group de 4 consumers consomme un topic de 6 partitions avec `SessionTimeoutMs = 10000`. Un consumer crash. Combien de temps avant que les partitions soient rÃ©assignÃ©es ?

A) ImmÃ©diatement (< 1 seconde)  
B) ~3 secondes  
C) ~10 secondes  
D) ~15 secondes

<details>
<summary>RÃ©ponse</summary>

**RÃ©ponse** : C (~10 secondes)

**Explication** : 
- Consumer crash â†’ plus de heartbeat envoyÃ©
- Group Coordinator attend `SessionTimeoutMs` (10 secondes) avant de considÃ©rer consumer mort
- AprÃ¨s timeout â†’ rebalancing dÃ©clenchÃ© (2-3 secondes supplÃ©mentaires)
- Total : ~12-13 secondes

ğŸ’¡ **Conseil** : RÃ©duire `SessionTimeoutMs` Ã  6000ms pour dÃ©tecter crashs plus rapidement, mais attention aux faux positifs (GC pause, latence rÃ©seau).
</details>

---

### 1.4.3 Questions FrÃ©quentes (FAQ Approfondie)

#### Q : DiffÃ©rence entre Kafka et RabbitMQ / Azure Service Bus ?

| Aspect | Kafka | RabbitMQ | Azure Service Bus |
|--------|-------|----------|-------------------|
| **Paradigme** | Event streaming (distributed log) | Message queue (AMQP) | Message queue (AMQP) |
| **Persistence** | Tous messages persistÃ©s (7j par dÃ©faut) | Messages supprimÃ©s aprÃ¨s consommation | Messages supprimÃ©s aprÃ¨s consommation |
| **Performance** | 100K-1M+ msgs/sec par broker | 10K-20K msgs/sec | 1K-10K msgs/sec |
| **RÃ©jouabilitÃ©** | Natif (lire historique Ã  volontÃ©) | Non (messages Ã©phÃ©mÃ¨res) | Non (sauf archive) |
| **Multi-consumer** | Natif (consumer groups) | NÃ©cessite exchanges fanout | NÃ©cessite topic subscriptions |
| **Ordre garanti** | Par partition (avec clÃ©) | Par queue (single consumer) | Par session (single consumer) |
| **Use cases** | Event sourcing, analytics, streaming, high-throughput | Task queues, RPC, routing complexe, low-latency | IntÃ©gration cloud Azure, messaging patterns |
| **ComplexitÃ© opÃ©rationnelle** | Ã‰levÃ©e (cluster, ZooKeeper/KRaft) | Moyenne (cluster simple) | Faible (managed service) |

ğŸ’¡ **Conseil** : Utilisez Kafka si vous avez besoin de :
1. **RÃ©jouabilitÃ©** (retraiter l'historique)
2. **Multi-consumer** natif (plusieurs services lisent mÃªme topic)
3. **Throughput Ã©levÃ©** (>50K msgs/sec)
4. **Event sourcing** (log immutable des Ã©vÃ©nements)

Utilisez RabbitMQ/Service Bus si vous avez besoin de :
1. **Routing complexe** (exchanges, bindings)
2. **PrioritÃ© de messages**
3. **Request-reply pattern**
4. **Faible latence** (<1ms)

#### Q : Pourquoi pas juste une base de donnÃ©es avec polling ?

**Approche base de donnÃ©es** :
-- Consumer 1 poll toutes les secondes
SELECT * FROM orders WHERE processed = false ORDER BY created_at LIMIT 100;
UPDATE orders SET processed = true WHERE id IN (...);

**ProblÃ¨mes** :
- â±ï¸ **Latence Ã©levÃ©e** : Polling toutes les N secondes (vs push en temps rÃ©el avec Kafka)
- ğŸ“‰ **Contention** : Lock sur table centrale si plusieurs consumers
- ğŸ’¾ **Pas d'historique** : UPDATE = perte de l'Ã©tat original
- ğŸ”’ **Transactions coÃ»teuses** : BEGIN/COMMIT pour chaque batch
- âŒ **Pas de rejouabilitÃ©** : Impossible de "rejouer" sans backup/restore
- ğŸ“ˆ **ScalabilitÃ© limitÃ©e** : Table unique = bottleneck

**Kafka** :
- âœ… Push en temps rÃ©el (< 50ms latence)
- âœ… Pas de contention (partitions indÃ©pendantes)
- âœ… Historique immutable (7 jours conservÃ©s)
- âœ… RejouabilitÃ© (consumer peut revenir en arriÃ¨re)
- âœ… ScalabilitÃ© horizontale (ajouter partitions + consumers)

ğŸ’¡ **Conseil** : Utilisez Kafka comme **source of truth** et base de donnÃ©es comme **materialized view** (CQRS pattern).

#### Q : Comment choisir le nombre de partitions ?

**Formule empirique** :
Nombre de partitions = max(
  Throughput_cible (MB/s) / Throughput_par_partition (MB/s),
  Nombre_de_consumers_max_souhaitÃ©s,
  Nombre_de_brokers Ã— 2
)

Exemple :
- Throughput cible : 500 MB/s (5000 msgs/sec Ã— 100 KB/msg)
- Throughput par partition : ~50 MB/s (une partition = un disque = 50 MB/s sustained)
- Consumers max : 20 (pic de charge)
- Brokers : 3

Partitions = max(500/50, 20, 3Ã—2) = max(10, 20, 6) = 20 partitions

**RÃ¨gles d'or** :
1. **Commencer large** : 12-24 partitions pour production (mÃªme si traffic faible au dÃ©but)
2. **Multiple de brokers** : 12 partitions sur 3 brokers = 4 partitions/broker (Ã©quilibrÃ©)
3. **Anticiper croissance** : Trafic actuel Ã— 3-5 pour dimensionner futur
4. **Ne JAMAIS rÃ©duire** : Impossible sans recrÃ©er topic (breaking change)
5. **Augmenter facile** : `ALTER TOPIC` mais dÃ©clenche rebalancing

âš ï¸ **ATTENTION** : Trop de partitions (>1000 par broker) peut impacter :
- Latence d'Ã©lection de leader (si broker crash)
- MÃ©moire utilisÃ©e (metadata)
- Temps de startup du broker

ğŸ’¡ **Conseil** : Pour la formation, utilisez 6-12 partitions. En production, 24-48 partitions est courant pour topics Ã  fort trafic.

#### Q : Que se passe-t-il si un consumer est trÃ¨s lent ?

**SymptÃ´me** : Consumer lag augmente (Ã©cart entre offset consommÃ© et offset courant)

# VÃ©rifier le lag
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group inventory-service \
  --describe

# Output :
# TOPIC          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    CONSUMER-ID
# orders.created 0          1000            5000            4000   consumer-1  â† LAG Ã‰LEVÃ‰ !

**Solutions par ordre de prioritÃ©** :

**1. Scaler horizontalement (solution immÃ©diate)** :
# Ajouter replicas (jusqu'Ã  N partitions)
oc scale deployment/inventory-service --replicas=6

# Lag devrait diminuer rapidement (6 consumers au lieu de 3)

**2. Optimiser traitement (solution court terme)** :
// Avant : traitement sÃ©quentiel
foreach (var result in batch)
{
    await ProcessAsync(result);  // 100ms par message
}
// Throughput : 10 msgs/sec

// AprÃ¨s : traitement parallÃ¨le
var tasks = batch.Select(r => ProcessAsync(r));
await Task.WhenAll(tasks);
// Throughput : 100 msgs/sec âœ“ (si I/O-bound)

**3. Batch processing (solution moyen terme)** :
// Traiter 100 messages en 1 requÃªte DB au lieu de 100 requÃªtes
var batch = ConsumeN(100);
await ProcessBatchAsync(batch);  // 1 bulk insert â†’ 10x plus rapide

**4. Augmenter partitions (solution long terme)** :
# Si consumers saturÃ©s mÃªme aprÃ¨s optimisation
oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --alter \
  --topic orders.created \
  --partitions 24  # Ã‰tait 6, maintenant 24

# Scaler consumers Ã  24
oc scale deployment/inventory-service --replicas=24

**5. SÃ©parer consumers par criticitÃ©** :
Topic: orders.created

Consumer Group "critical" (3 replicas, traitement prioritaire)
  â†’ Traite seulement commandes VIP (filtre dans code)

Consumer Group "standard" (10 replicas, traitement standard)
  â†’ Traite toutes les commandes

Consumer Group "analytics" (1 replica, traitement lent OK)
  â†’ AgrÃ©gations et analytics

ğŸ’¡ **Conseil** : Configurez des **alertes sur le lag** :
IF consumer_lag > 1000 messages pendant 5 minutes
THEN alert Ã©quipe ops

---

### 1.4.4 Exercice de Consolidation

#### Ã‰noncÃ© : SystÃ¨me de RÃ©servation de Restaurant

**Contexte** : Vous devez implÃ©menter un systÃ¨me de rÃ©servation temps rÃ©el pour une chaÃ®ne de 50 restaurants.

**Exigences fonctionnelles** :
- API REST accepte rÃ©servations (restaurantId, customerId, date, nbPersons, specialRequests)
- Service d'inventaire vÃ©rifie disponibilitÃ© des tables en temps rÃ©el
- Service de confirmation envoie email + SMS au client
- Service analytics agrÃ¨ge statistiques (rÃ©servations/heure, taux de remplissage)
- Service de recommandation met Ã  jour profil client

**Exigences non-fonctionnelles** :
- Throughput : 1000 rÃ©servations/seconde (peak lunch/dinner)
- Latence : API rÃ©pond en < 100ms (202 Accepted)
- DisponibilitÃ© : 99.9% (pas de perte de rÃ©servation)
- Ordre : RÃ©servations d'un mÃªme restaurant doivent Ãªtre traitÃ©es dans l'ordre

**Questions** :

**1. Architecture** : Dessinez le diagramme (Producer, Topics, Consumers, Groupes)

**2. Partitionnement** : Quelle clÃ© utiliser pour les messages du topic `reservations.created` ?
   - A) `customerId`
   - B) `restaurantId`
   - C) `date`
   - D) Pas de clÃ© (round-robin)
   - E) Combinaison `{restaurantId}-{date}`

**3. Consumer Groups** : Combien de groupes diffÃ©rents ? Quels noms ? Combien de replicas chacun ?

**4. Dimensionnement** : 
   - Combien de partitions pour le topic `reservations.created` ?
   - Combien de brokers Kafka ?
   - Configuration `replication.factor` et `min.insync.replicas` ?

**5. Gestion d'erreurs** : 
   - Que faire si le service d'inventaire dÃ©tecte une double-rÃ©servation ?
   - OÃ¹ stocker les rÃ©servations Ã©chouÃ©es ?

<details>
<summary>Solution ComplÃ¨te</summary>

**1. Architecture** :
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ReservationAPI  â”‚ (Producer .NET)
â”‚   POST /reserve â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Topic: reservations.created    â”‚
â”‚ (24 partitions, RF=3)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼              â–¼              â–¼              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚Inventoryâ”‚   â”‚Confirm  â”‚   â”‚Analyticsâ”‚   â”‚Recomm   â”‚
    â”‚Service  â”‚   â”‚Service  â”‚   â”‚Service  â”‚   â”‚Service  â”‚
    â”‚Group:invâ”‚   â”‚Group:cfmâ”‚   â”‚Grp:anlt â”‚   â”‚Grp:rec  â”‚
    â”‚6 replicasâ”‚   â”‚3 replicasâ”‚   â”‚1 replicaâ”‚   â”‚2 replicasâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼              â–¼
    PostgreSQL    SendGrid API   ClickHouse   Redis Cache

**2. Partitionnement** : **B) `restaurantId`**

**Justification** :
- âœ… **Ordre garanti** : Toutes les rÃ©servations d'un mÃªme restaurant sur mÃªme partition â†’ traitement sÃ©quentiel â†’ Ã©vite double-booking
- âœ… **Distribution Ã©quitable** : 50 restaurants sur 24 partitions = ~2 restaurants/partition
- âœ… **ScalabilitÃ©** : Peut monter Ã  24 replicas du service d'inventaire (1 replica/partition)

Pourquoi pas les autres ?
- âŒ A) `customerId` : Un client peut rÃ©server dans plusieurs restaurants â†’ pas d'ordre garanti par restaurant
- âŒ C) `date` : MÃªme date = mÃªme partition â†’ hot partition (lunch/dinner rush)
- âŒ D) Round-robin : Pas d'ordre â†’ risque de double-booking
- âŒ E) `{restaurantId}-{date}` : Trop granulaire â†’ trop de clÃ©s uniques â†’ distribution inÃ©gale

**3. Consumer Groups** :

| Groupe | Replicas | CriticitÃ© | Traitement |
|--------|----------|-----------|------------|
| `inventory-checkers` | 6 | Haute | VÃ©rifier dispo tables (< 100ms) |
| `confirmation-senders` | 3 | Haute | Envoyer email/SMS (< 500ms) |
| `analytics-aggregators` | 1 | Faible | AgrÃ©ger stats (~ 1s) |
| `recommendation-updaters` | 2 | Moyenne | Mettre Ã  jour profil (~ 200ms) |

**4. Dimensionnement** :

**Partitions** :
Throughput cible : 1000 rÃ©servations/sec (peak)
Taille moyenne message : 500 bytes
Throughput : 1000 Ã— 0.5 KB = 500 KB/sec

Throughput par partition : ~10 MB/sec = 10,000 KB/sec
Partitions nÃ©cessaires (throughput) : 500 / 10,000 = 0.05 â†’ 1 partition suffit

Mais : Parallelisme souhaitÃ© = 12 consumers inventory (peak)
Donc : min 12 partitions

Recommandation : 24 partitions
- Permet 24 consumers parallÃ¨les
- Anticipe croissance (2x-3x traffic dans 2 ans)
- Multiple de 3 brokers = 8 partitions/broker (Ã©quilibrÃ©)

**Brokers** :
Recommandation : 3 brokers minimum

Justification :
- Replication.factor = 3 â†’ tolÃ¨re 2 pannes de brokers
- 24 partitions / 3 brokers = 8 partitions/broker
- Chaque broker gÃ¨re ~170 KB/sec (largement sous capacitÃ©)

**Configuration** :
# Topic configuration
replication.factor: 3           # TolÃ©rance Ã  2 pannes
min.insync.replicas: 2          # Garantie durabilitÃ© (2 replicas confirmÃ©es)
retention.ms: 604800000         # 7 jours (replay si besoin)
compression.type: lz4           # Compression lÃ©gÃ¨re

# Producer configuration
acks: all                       # Attendre tous les ISR
enable.idempotence: true        # Pas de duplicatas

**5. Gestion d'erreurs** :

**ScÃ©nario 1 : Double-rÃ©servation dÃ©tectÃ©e** :
// Dans InventoryService consumer
var reservation = ParseReservation(message);

// VÃ©rifier dispo dans PostgreSQL avec lock
using var transaction = await _dbConnection.BeginTransactionAsync();
var isAvailable = await CheckAvailability(reservation.RestaurantId, reservation.Date, transaction);

if (!isAvailable)
{
    // Produire Ã©vÃ©nement de rejet
    await _producer.ProduceAsync("reservations.rejected", new Message<string, string>
    {
        Key = reservation.ReservationId,
        Value = JsonSerializer.Serialize(new
        {
            reservation.ReservationId,
            Reason = "No tables available",
            RejectedAt = DateTime.UtcNow
        })
    });
    
    // Commit offset (message traitÃ©, pas d'erreur)
    consumer.Commit(result);
    return;
}

// RÃ©server table
await ReserveTable(reservation, transaction);
await transaction.CommitAsync();

**ScÃ©nario 2 : Ã‰chec technique (DB down, network timeout)** :
try
{
    await ProcessReservation(message);
    consumer.Commit(result);
}
catch (PostgresException ex) when (ex.IsTransient)
{
    // Erreur transiente â†’ retry avec exponential backoff
    await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, attemptCount)));
    // Ne pas commiter offset â†’ message retraitÃ© au prochain poll
}
catch (Exception ex)
{
    // Erreur permanente â†’ DLQ
    await SendToDLQ(message, ex);
    consumer.Commit(result);  // Commit pour passer au suivant
}

**Dead Letter Queue** :
Topic: reservations.dlq

Messages dans DLQ :
- Original reservation data
- Error reason + stack trace
- Timestamp
- Retry count

Monitoring :
- Alert si messages dans DLQ
- Dashboard montrant nombre de DLQ messages
- Job batch pour retraiter DLQ (aprÃ¨s fix)

</details>

ğŸ’¡ **Conseil** : Cet exercice couvre tous les concepts du Jour 1. Prenez le temps de le rÃ©soudre avant de voir la solution.

---

### ğŸ¯ Bilan Jour 1

**Ce que vous maÃ®trisez maintenant** :
- âœ… Architecture Kafka complÃ¨te (brokers, topics, partitions, offsets, replication)
- âœ… Producer .NET production-ready (gestion d'erreurs, DLQ, performance)
- âœ… Consumer .NET production-ready (auto-commit vs manual, rebalancing, scaling)
- âœ… Consumer Group et parallÃ©lisme horizontal
- âœ… DÃ©ploiement sur OpenShift avec Strimzi
- âœ… Debugging avec logs structurÃ©s et CLI Kafka
- âœ… Dimensionnement (partitions, consumers, brokers)

**Skills pratiques acquis** :
- CrÃ©er un cluster Kafka sur OpenShift
- DÃ©velopper Producer/Consumer .NET idiomatiques
- GÃ©rer les erreurs (retry, DLQ, alerting)
- Observer le rebalancing en temps rÃ©el
- Dimensionner un systÃ¨me Kafka pour production

**Prochaine Ã©tape (Jour 2)** :
- âœ¨ SÃ©rialisation avancÃ©e (Avro, Schema Registry, Ã©volution de schÃ©ma)
- âœ¨ Producer patterns avancÃ©s (idempotence, transactions, exactly-once)
- âœ¨ Consumer patterns avancÃ©s (manual commit, at-least-once, exactly-once)
- âœ¨ **Kafka Connect** (intÃ©gration bases de donnÃ©es, systÃ¨mes externes)
- âœ¨ Use cases microservices concrets

---

# TIPS & BEST PRACTICES JOUR 1

## Tips Producer

ğŸ’¡ **TIP #1** : Toujours utiliser un **singleton Producer** en ASP.NET Core
// âœ… BON : Singleton (rÃ©utilisÃ©)
builder.Services.AddSingleton<IProducer<string, string>>(sp => ...);

// âŒ MAUVAIS : Scoped ou Transient (nouvelle connexion TCP Ã  chaque fois)
builder.Services.AddScoped<IProducer<string, string>>(sp => ...);

ğŸ’¡ **TIP #2** : Flush() avant fermeture pour Ã©viter perte de messages
// TOUJOURS flush avant Dispose
producer.Flush(TimeSpan.FromSeconds(10));
producer.Dispose();

ğŸ’¡ **TIP #3** : Utilisez headers pour correlation IDs et tracing
Headers = new Headers
{
    { "correlation-id", Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) },
    { "trace-id", Encoding.UTF8.GetBytes(Activity.Current?.TraceId.ToString() ?? "") }
}

ğŸ’¡ **TIP #4** : Dimensionnez `BatchSize` selon taille moyenne des messages
// Messages petits (<1 KB) â†’ batch plus grand
BatchSize = 100000,  // 100 KB

// Messages gros (>10 KB) â†’ batch plus petit
BatchSize = 16384    // 16 KB

## Tips Consumer

ğŸ’¡ **TIP #5** : Utilisez **CooperativeSticky** pour rebalancing rapide
PartitionAssignmentStrategy = PartitionAssignmentStrategy.CooperativeSticky

ğŸ’¡ **TIP #6** : Augmentez `MaxPollIntervalMs` si traitement lent
// Si traitement prend 10 minutes par batch
MaxPollIntervalMs = 600000  // 10 minutes (dÃ©faut: 5 minutes)

ğŸ’¡ **TIP #7** : Loggez les callbacks de partition pour debugging
.SetPartitionsAssignedHandler((c, partitions) =>
{
    logger.LogInformation("Partitions assigned: {Partitions}", 
        string.Join(", ", partitions.Select(p => p.Partition.Value)));
})
.SetPartitionsRevokedHandler((c, partitions) =>
{
    logger.LogWarning("Partitions revoked: {Partitions}", 
        string.Join(", ", partitions.Select(p => p.Partition.Value)));
})

ğŸ’¡ **TIP #8** : Graceful shutdown avec CancellationToken
var cts = new CancellationTokenSource();
Console.CancelKeyPress += (_, e) => { e.Cancel = true; cts.Cancel(); };

while (!cts.Token.IsCancellationRequested)
{
    var result = consumer.Consume(cts.Token);
    // ...
}

consumer.Close();  // Trigger rebalancing proprement

## Tips OpÃ©rationnels

ğŸ’¡ **TIP #9** : CrÃ©ez des aliases pour CLI Kafka
alias kafka-topics="oc exec -it bhf-kafka-kafka-0 -- bin/kafka-topics.sh --bootstrap-server localhost:9092"
alias kafka-consumer-groups="oc exec -it bhf-kafka-kafka-0 -- bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092"

ğŸ’¡ **TIP #10** : Monitoring du lag en continu
# Script watch.sh
watch -n 5 'oc exec -it bhf-kafka-kafka-0 -- \
  bin/kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group inventory-service \
  --describe'

ğŸ’¡ **TIP #11** : Testez en local avec Docker Compose avant OpenShift
# docker-compose.yml simple pour dev local
version: '3.8'
services:
  kafka:
    image: confluentinc/cp-kafka:7.6.0
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181

---
