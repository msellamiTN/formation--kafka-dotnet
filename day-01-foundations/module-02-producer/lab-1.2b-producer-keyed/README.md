# LAB 1.2B : Producer avec Cl√© (Partitionnement D√©terministe)

## ‚è±Ô∏è Dur√©e estim√©e : 45 minutes

## üéØ Objectif

Comprendre comment la cl√© d√©termine la partition et garantit l'ordre des messages pour une m√™me entit√© (client, commande, compte bancaire).

### Pourquoi utiliser une cl√© ?

```mermaid
flowchart TB
    subgraph "Sans Cl√© (LAB 1.2A)"
        A1["Msg 1"] --> P1["Partition 0"]
        A2["Msg 2"] --> P2["Partition 1"]
        A3["Msg 3"] --> P3["Partition 2"]
        A4["Msg 4"] --> P4["Partition 0"]
    end
    
    subgraph "Avec Cl√© (LAB 1.2B)"
        B1["Key: customer-A"] --> Q1["Partition 0"]
        B2["Key: customer-A"] --> Q1
        B3["Key: customer-A"] --> Q1
        B4["Key: customer-B"] --> Q2["Partition 1"]
    end
    
    style A1 fill:#ffcc80
    style A2 fill:#ffcc80
    style A3 fill:#ffcc80
    style A4 fill:#ffcc80
    style B1 fill:#81d4fa
    style B2 fill:#81d4fa
    style B3 fill:#81d4fa
    style B4 fill:#a5d6a7
```

**Sans cl√©** : Distribution al√©atoire ‚Üí pas d'ordre garanti  
**Avec cl√©** : M√™me cl√© = m√™me partition ‚Üí ordre garanti pour cette cl√©

## üìö Ce que vous allez apprendre

- Diff√©rence entre messages avec et sans cl√©
- Partitionnement hash-based (algorithme Murmur2)
- Garantie d'ordre par cl√© (m√™me cl√© = m√™me partition)
- Distribution des messages sur les partitions
- √âviter les hot partitions (partitions surcharg√©es)
- Pr√©dire sur quelle partition une cl√© atterrira

---

## üìã Pr√©requis

### Cluster Kafka et topic cr√©√©s

Si ce n'est pas d√©j√† fait, cr√©ez le topic `orders.created` avec 6 partitions (voir LAB 1.2A).

---

## üöÄ Instructions Pas √† Pas

### √âtape 1 : Cr√©er le projet

```bash
cd lab-1.2b-producer-keyed
dotnet new console -n KafkaProducerKeyed
cd KafkaProducerKeyed
dotnet add package Confluent.Kafka --version 2.3.0
dotnet add package Microsoft.Extensions.Logging --version 8.0.0
dotnet add package Microsoft.Extensions.Logging.Console --version 8.0.0
```

---

### √âtape 2 : Comprendre le partitionnement

#### Sans cl√© (LAB 1.2A)

```csharp
// Message sans cl√© ‚Üí Sticky Partitioner (round-robin am√©lior√©)
await producer.ProduceAsync("orders", new Message<Null, string>
{
    Value = "{...}"  // Partition choisie automatiquement
});
```

**R√©sultat** : Messages distribu√©s uniform√©ment sur toutes les partitions, mais **pas d'ordre garanti** entre les messages.

#### Avec cl√© (LAB 1.2B)

```csharp
// Message avec cl√© ‚Üí Hash-based partitioning
await producer.ProduceAsync("orders", new Message<string, string>
{
    Key = "customer-123",  // Ira TOUJOURS sur la m√™me partition
    Value = "{...}"
});
```

**R√©sultat** : Tous les messages avec `Key = "customer-123"` vont sur la **m√™me partition**, garantissant l'**ordre**.

#### Formule de partitionnement

```mermaid
flowchart LR
    A["üìù Cl√©: 'customer-A'"] --> B["üî¢ Hash Murmur2"]
    B --> C["üíª 1234567890"]
    C --> D["üìä % 6 partitions"]
    D --> E["üì¶ Partition 4"]
    
    style A fill:#bbdefb,stroke:#1976d2
    style B fill:#fff9c4,stroke:#fbc02d
    style E fill:#c8e6c9,stroke:#388e3c
```

**Formule** : `partition = murmur2_hash(key) % nombre_partitions`

**Exemple concret** :
- Topic avec 6 partitions
- Cl√© = "customer-A"
- Hash Murmur2("customer-A") = 1234567890
- Partition = 1234567890 % 6 = 4

‚Üí Tous les messages avec cl√© "customer-A" iront sur **partition 4**.

---

### √âtape 3 : Copier et comprendre le code

Le code fourni dans `Program.cs` simule 5 clients diff√©rents envoyant chacun plusieurs commandes.

#### Points cl√©s du code

**1. Type du Producer**

```csharp
// <string, string> = <Type de la cl√©, Type de la valeur>
using var producer = new ProducerBuilder<string, string>(config)
    .Build();
```

**2. Envoi avec cl√©**

```csharp
var customerId = customers[i % 5];  // customer-A, customer-B, etc.

var deliveryResult = await producer.ProduceAsync(topicName, new Message<string, string>
{
    Key = customerId,  // LA CL√â D√âTERMINE LA PARTITION
    Value = messageValue,
    Timestamp = Timestamp.Default
});

logger.LogInformation(
    "‚úì Delivered ‚Üí Key: {Key}, Partition: {Partition}, Offset: {Offset}",
    customerId,
    deliveryResult.Partition.Value,
    deliveryResult.Offset.Value
);
```

**3. Observation de la distribution**

Le code envoie 30 messages (6 messages par client) et affiche sur quelle partition chaque message atterrit.

---

### √âtape 4 : Ex√©cuter et observer

```bash
dotnet run
```

#### Logs attendus

```
info: Sending order ORD-customer-A-0001 for customer customer-A
info: ‚úì Delivered ‚Üí Key: customer-A, Partition: 3, Offset: 0
info: Sending order ORD-customer-B-0002 for customer customer-B
info: ‚úì Delivered ‚Üí Key: customer-B, Partition: 1, Offset: 0
info: Sending order ORD-customer-C-0003 for customer customer-C
info: ‚úì Delivered ‚Üí Key: customer-C, Partition: 5, Offset: 0
info: Sending order ORD-customer-D-0004 for customer customer-D
info: ‚úì Delivered ‚Üí Key: customer-D, Partition: 2, Offset: 0
info: Sending order ORD-customer-E-0005 for customer customer-E
info: ‚úì Delivered ‚Üí Key: customer-E, Partition: 4, Offset: 0
info: Sending order ORD-customer-A-0006 for customer customer-A
info: ‚úì Delivered ‚Üí Key: customer-A, Partition: 3, Offset: 1  ‚Üê M√™me partition !
info: Sending order ORD-customer-B-0007 for customer customer-B
info: ‚úì Delivered ‚Üí Key: customer-B, Partition: 1, Offset: 1  ‚Üê M√™me partition !
...
```

#### Observations cl√©s

‚úÖ **customer-A** ‚Üí Toujours **partition 3**  
‚úÖ **customer-B** ‚Üí Toujours **partition 1**  
‚úÖ **customer-C** ‚Üí Toujours **partition 5**  
‚úÖ **customer-D** ‚Üí Toujours **partition 2**  
‚úÖ **customer-E** ‚Üí Toujours **partition 4**

**Conclusion** : Le partitionnement par cl√© est **d√©terministe** et **reproductible**.

---

### √âtape 5 : V√©rifier l'ordre dans Kafka

#### Avec CLI Kafka (lire partition sp√©cifique)

**Docker** :
```bash
# Lire uniquement la partition 3 (customer-A)
docker exec kafka /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders.created \
  --partition 3 \
  --from-beginning
```

**OKD/K3s** :
```bash
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-console-consumer.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic orders.created --partition 3 --from-beginning
```

**R√©sultat attendu** : Vous ne verrez que les messages de **customer-A**, dans l'ordre d'envoi.

---

## üß™ Exercices Pratiques

### Exercice 1 : Augmenter le nombre de clients

**Objectif** : Envoyer 60 messages avec 10 clients (customer-A √† customer-J).

**Instructions** :

1. Modifier le code :

```csharp
var customers = Enumerable.Range(0, 10)
    .Select(i => $"customer-{(char)('A' + i)}")
    .ToArray();

for (int i = 1; i <= 60; i++)
{
    var customerId = customers[i % 10];
    // ... reste du code
}
```

2. Ajouter un compteur de distribution :

```csharp
var partitionCounts = new Dictionary<int, int>();

// Dans la boucle, apr√®s ProduceAsync :
partitionCounts[deliveryResult.Partition.Value] = 
    partitionCounts.GetValueOrDefault(deliveryResult.Partition.Value, 0) + 1;

// Apr√®s la boucle :
Console.WriteLine("\n=== Distribution des messages par partition ===");
foreach (var kvp in partitionCounts.OrderBy(x => x.Key))
{
    Console.WriteLine($"Partition {kvp.Key}: {kvp.Value} messages");
}
```

3. Ex√©cuter et observer la distribution.

**Question** : La distribution est-elle uniforme ? Pourquoi ?

<details>
<summary>üí° Solution</summary>

Avec 10 clients et 6 partitions, la distribution d√©pend du hash de chaque cl√©. Elle ne sera pas parfaitement uniforme (certaines partitions peuvent avoir 1 client, d'autres 2), mais sur un grand nombre de cl√©s, la distribution converge vers l'uniformit√©.

</details>

---

### Exercice 2 : Simuler une hot partition

**Objectif** : Observer le probl√®me des hot partitions.

**Instructions** :

1. Modifier le code pour que 80% des messages aient la m√™me cl√© :

```csharp
for (int i = 1; i <= 100; i++)
{
    // 80% des messages avec customer-A, 20% avec les autres
    var customerId = (i % 10 < 8) ? "customer-A" : $"customer-{(char)('B' + (i % 4))}";
    // ... reste du code
}
```

2. Observer la distribution.

**Question** : Quelle partition re√ßoit le plus de messages ? Quel est le probl√®me ?

<details>
<summary>üí° Solution</summary>

La partition de "customer-A" re√ßoit 80 messages, les autres se partagent les 20 restants. C'est une **hot partition** : elle est surcharg√©e, ce qui peut causer :
- Latence accrue pour les consumers de cette partition
- D√©s√©quilibre de charge entre brokers
- Risque de saturation du disque sur le broker h√©bergeant cette partition

**Solution** : Utiliser une cl√© composite ou un hash de la cl√© pour mieux distribuer.

</details>

---

### Exercice 3 : Pr√©dire la partition d'une cl√©

**Objectif** : Calculer sur quelle partition une cl√© atterrira avant de l'envoyer.

**Instructions** :

1. Ajouter cette m√©thode :

```csharp
static int PredictPartition(string key, int numPartitions)
{
    // Simuler le hash Murmur2 (simplifi√©)
    var hash = key.GetHashCode();
    return Math.Abs(hash) % numPartitions;
}
```

2. Avant d'envoyer un message, pr√©dire sa partition :

```csharp
var predictedPartition = PredictPartition(customerId, 6);
Console.WriteLine($"Predicted partition for {customerId}: {predictedPartition}");

var deliveryResult = await producer.ProduceAsync(...);

Console.WriteLine($"Actual partition: {deliveryResult.Partition.Value}");
```

**Note** : La pr√©diction peut ne pas √™tre exacte car `GetHashCode()` n'est pas Murmur2, mais elle donne une id√©e.

---

## ‚úÖ Validation du Lab

Vous avez r√©ussi ce lab si :

- [ ] Vous comprenez que **Key ‚Üí Partition** est d√©terministe
- [ ] M√™me cl√© = m√™me partition = **ordre pr√©serv√©** pour cette cl√©
- [ ] Vous savez observer la distribution des cl√©s sur les partitions
- [ ] Vous comprenez le probl√®me des hot partitions
- [ ] Vous savez quand utiliser une cl√© (ordre, localit√©, compaction)

---

## üéØ Points Cl√©s √† Retenir

### 1. Quand utiliser une cl√© ?

‚úÖ **Utilisez une cl√© si vous avez besoin de** :
- **Ordre garanti** : Tous les √©v√©nements d'une entit√© (client, commande) doivent arriver dans l'ordre
- **Localit√©** : Un consumer doit voir tous les √©v√©nements d'une entit√© ensemble
- **Compaction** : Topic compact√© (derni√®re valeur par cl√© conserv√©e)

‚ùå **N'utilisez pas de cl√© si** :
- Vous voulez une distribution uniforme sans contrainte d'ordre
- Vous avez des cl√©s tr√®s d√©s√©quilibr√©es (risque de hot partition)

### 2. Formule de partitionnement

```
partition = murmur2_hash(key) % nombre_partitions
```

- **D√©terministe** : M√™me cl√© ‚Üí m√™me partition (toujours)
- **Uniforme** : Hash Murmur2 distribue bien les cl√©s
- **Stable** : Ne change pas si nombre de partitions constant

### 3. Hot Partitions

**Probl√®me** : Une partition re√ßoit beaucoup plus de messages que les autres.

**Causes** :
- Cl√©s d√©s√©quilibr√©es (ex: 80% des messages avec m√™me cl√©)
- Cl√©s mal choisies (ex: date du jour ‚Üí tous les messages du jour sur m√™me partition)

**Solutions** :
- Choisir des cl√©s bien distribu√©es
- Utiliser un hash de la cl√© si n√©cessaire
- Augmenter le nombre de partitions
- Utiliser une cl√© composite (ex: `customerId + orderId % 10`)

### 4. Ordre des messages

**Avec cl√©** :
- Ordre garanti **au sein d'une partition**
- Tous les messages avec m√™me cl√© arrivent dans l'ordre

**Sans cl√©** :
- Aucune garantie d'ordre global
- Sticky partitioner groupe les messages par batch

---

## üìñ Concepts Th√©oriques

### Algorithme Murmur2

Kafka utilise l'algorithme de hash **Murmur2** pour calculer la partition √† partir de la cl√© :

```java
// Pseudo-code simplifi√©
int partition = murmur2(key.getBytes()) % numPartitions;
```

**Propri√©t√©s** :
- Rapide (plus rapide que MD5, SHA)
- Bonne distribution (pas de collisions fr√©quentes)
- Non-cryptographique (pas s√©curis√©, mais ce n'est pas le but)

### Sticky Partitioner (sans cl√©)

Depuis Kafka 2.4, le partitionnement sans cl√© utilise le **sticky partitioner** :

1. Choisir une partition al√©atoire
2. Envoyer tous les messages du batch sur cette partition
3. Quand le batch est plein, choisir une nouvelle partition

**Avantage** : Moins de requ√™tes r√©seau, meilleure performance.

### Compaction de Topic

Avec une cl√©, vous pouvez activer la **compaction** :

```bash
--config cleanup.policy=compact
```

Kafka conserve uniquement la **derni√®re valeur** pour chaque cl√©. Utile pour :
- Event Sourcing (√©tat actuel d'une entit√©)
- CDC (Change Data Capture)
- Cache distribu√©

---

## üöÄ Prochaine √âtape

Vous ma√Ætrisez maintenant le partitionnement par cl√© !

üëâ **Passez au [LAB 1.2C : Producer avec Gestion d'Erreurs et DLQ](../lab-1.2c-producer-error-handling/README.md)**

Dans le prochain lab, vous apprendrez :
- Classification des erreurs (retriable vs permanent)
- Pattern Dead Letter Queue (DLQ)
- Retry avec exponential backoff
- Logging et monitoring production-ready
