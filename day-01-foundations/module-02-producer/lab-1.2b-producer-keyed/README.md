# LAB 1.2B : Producer avec ClÃ© (Partitionnement DÃ©terministe)

## â±ï¸ DurÃ©e estimÃ©e : 45 minutes

## ğŸ¯ Objectif

Comprendre comment la clÃ© dÃ©termine la partition et garantit l'ordre des messages pour une mÃªme entitÃ© (client, commande, compte bancaire).

## ğŸ“š Ce que vous allez apprendre

- DiffÃ©rence entre messages avec et sans clÃ©
- Partitionnement hash-based (algorithme Murmur2)
- Garantie d'ordre par clÃ© (mÃªme clÃ© = mÃªme partition)
- Distribution des messages sur les partitions
- Ã‰viter les hot partitions (partitions surchargÃ©es)
- PrÃ©dire sur quelle partition une clÃ© atterrira

---

## ğŸ“‹ PrÃ©requis

### Cluster Kafka et topic crÃ©Ã©s

Si ce n'est pas dÃ©jÃ  fait, crÃ©ez le topic `orders.created` avec 6 partitions (voir LAB 1.2A).

---

## ğŸš€ Instructions Pas Ã  Pas

### Ã‰tape 1 : CrÃ©er le projet

```bash
cd lab-1.2b-producer-keyed
dotnet new console -n KafkaProducerKeyed
cd KafkaProducerKeyed
dotnet add package Confluent.Kafka --version 2.3.0
dotnet add package Microsoft.Extensions.Logging --version 8.0.0
dotnet add package Microsoft.Extensions.Logging.Console --version 8.0.0
```

---

### Ã‰tape 2 : Comprendre le partitionnement

#### Sans clÃ© (LAB 1.2A)

```csharp
// Message sans clÃ© â†’ Sticky Partitioner (round-robin amÃ©liorÃ©)
await producer.ProduceAsync("orders", new Message<Null, string>
{
    Value = "{...}"  // Partition choisie automatiquement
});
```

**RÃ©sultat** : Messages distribuÃ©s uniformÃ©ment sur toutes les partitions, mais **pas d'ordre garanti** entre les messages.

#### Avec clÃ© (LAB 1.2B)

```csharp
// Message avec clÃ© â†’ Hash-based partitioning
await producer.ProduceAsync("orders", new Message<string, string>
{
    Key = "customer-123",  // Ira TOUJOURS sur la mÃªme partition
    Value = "{...}"
});
```

**RÃ©sultat** : Tous les messages avec `Key = "customer-123"` vont sur la **mÃªme partition**, garantissant l'**ordre**.

#### Formule de partitionnement

```
partition = murmur2_hash(key) % nombre_partitions
```

**Exemple** :
- Topic avec 6 partitions
- ClÃ© = "customer-A"
- Hash Murmur2("customer-A") = 1234567890
- Partition = 1234567890 % 6 = 4

â†’ Tous les messages avec clÃ© "customer-A" iront sur **partition 4**.

---

### Ã‰tape 3 : Copier et comprendre le code

Le code fourni dans `Program.cs` simule 5 clients diffÃ©rents envoyant chacun plusieurs commandes.

#### Points clÃ©s du code

**1. Type du Producer**

```csharp
// <string, string> = <Type de la clÃ©, Type de la valeur>
using var producer = new ProducerBuilder<string, string>(config)
    .Build();
```

**2. Envoi avec clÃ©**

```csharp
var customerId = customers[i % 5];  // customer-A, customer-B, etc.

var deliveryResult = await producer.ProduceAsync(topicName, new Message<string, string>
{
    Key = customerId,  // LA CLÃ‰ DÃ‰TERMINE LA PARTITION
    Value = messageValue,
    Timestamp = Timestamp.Default
});

logger.LogInformation(
    "âœ“ Delivered â†’ Key: {Key}, Partition: {Partition}, Offset: {Offset}",
    customerId,
    deliveryResult.Partition.Value,
    deliveryResult.Offset.Value
);
```

**3. Observation de la distribution**

Le code envoie 30 messages (6 messages par client) et affiche sur quelle partition chaque message atterrit.

---

### Ã‰tape 4 : ExÃ©cuter et observer

```bash
dotnet run
```

#### Logs attendus

```
info: Sending order ORD-customer-A-0001 for customer customer-A
info: âœ“ Delivered â†’ Key: customer-A, Partition: 3, Offset: 0
info: Sending order ORD-customer-B-0002 for customer customer-B
info: âœ“ Delivered â†’ Key: customer-B, Partition: 1, Offset: 0
info: Sending order ORD-customer-C-0003 for customer customer-C
info: âœ“ Delivered â†’ Key: customer-C, Partition: 5, Offset: 0
info: Sending order ORD-customer-D-0004 for customer customer-D
info: âœ“ Delivered â†’ Key: customer-D, Partition: 2, Offset: 0
info: Sending order ORD-customer-E-0005 for customer customer-E
info: âœ“ Delivered â†’ Key: customer-E, Partition: 4, Offset: 0
info: Sending order ORD-customer-A-0006 for customer customer-A
info: âœ“ Delivered â†’ Key: customer-A, Partition: 3, Offset: 1  â† MÃªme partition !
info: Sending order ORD-customer-B-0007 for customer customer-B
info: âœ“ Delivered â†’ Key: customer-B, Partition: 1, Offset: 1  â† MÃªme partition !
...
```

#### Observations clÃ©s

âœ… **customer-A** â†’ Toujours **partition 3**  
âœ… **customer-B** â†’ Toujours **partition 1**  
âœ… **customer-C** â†’ Toujours **partition 5**  
âœ… **customer-D** â†’ Toujours **partition 2**  
âœ… **customer-E** â†’ Toujours **partition 4**

**Conclusion** : Le partitionnement par clÃ© est **dÃ©terministe** et **reproductible**.

---

### Ã‰tape 5 : VÃ©rifier l'ordre dans Kafka

#### Avec CLI Kafka (lire partition spÃ©cifique)

**Docker** :
```bash
# Lire uniquement la partition 3 (customer-A)
docker exec kafka /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic orders.created \
  --partition 3 \
  --from-beginning
```

**OKD/K3s** :
```bash
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-console-consumer.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic orders.created --partition 3 --from-beginning
```

**RÃ©sultat attendu** : Vous ne verrez que les messages de **customer-A**, dans l'ordre d'envoi.

---

## ğŸ§ª Exercices Pratiques

### Exercice 1 : Augmenter le nombre de clients

**Objectif** : Envoyer 60 messages avec 10 clients (customer-A Ã  customer-J).

**Instructions** :

1. Modifier le code :

```csharp
var customers = Enumerable.Range(0, 10)
    .Select(i => $"customer-{(char)('A' + i)}")
    .ToArray();

for (int i = 1; i <= 60; i++)
{
    var customerId = customers[i % 10];
    // ... reste du code
}
```

2. Ajouter un compteur de distribution :

```csharp
var partitionCounts = new Dictionary<int, int>();

// Dans la boucle, aprÃ¨s ProduceAsync :
partitionCounts[deliveryResult.Partition.Value] = 
    partitionCounts.GetValueOrDefault(deliveryResult.Partition.Value, 0) + 1;

// AprÃ¨s la boucle :
Console.WriteLine("\n=== Distribution des messages par partition ===");
foreach (var kvp in partitionCounts.OrderBy(x => x.Key))
{
    Console.WriteLine($"Partition {kvp.Key}: {kvp.Value} messages");
}
```

3. ExÃ©cuter et observer la distribution.

**Question** : La distribution est-elle uniforme ? Pourquoi ?

<details>
<summary>ğŸ’¡ Solution</summary>

Avec 10 clients et 6 partitions, la distribution dÃ©pend du hash de chaque clÃ©. Elle ne sera pas parfaitement uniforme (certaines partitions peuvent avoir 1 client, d'autres 2), mais sur un grand nombre de clÃ©s, la distribution converge vers l'uniformitÃ©.

</details>

---

### Exercice 2 : Simuler une hot partition

**Objectif** : Observer le problÃ¨me des hot partitions.

**Instructions** :

1. Modifier le code pour que 80% des messages aient la mÃªme clÃ© :

```csharp
for (int i = 1; i <= 100; i++)
{
    // 80% des messages avec customer-A, 20% avec les autres
    var customerId = (i % 10 < 8) ? "customer-A" : $"customer-{(char)('B' + (i % 4))}";
    // ... reste du code
}
```

2. Observer la distribution.

**Question** : Quelle partition reÃ§oit le plus de messages ? Quel est le problÃ¨me ?

<details>
<summary>ğŸ’¡ Solution</summary>

La partition de "customer-A" reÃ§oit 80 messages, les autres se partagent les 20 restants. C'est une **hot partition** : elle est surchargÃ©e, ce qui peut causer :
- Latence accrue pour les consumers de cette partition
- DÃ©sÃ©quilibre de charge entre brokers
- Risque de saturation du disque sur le broker hÃ©bergeant cette partition

**Solution** : Utiliser une clÃ© composite ou un hash de la clÃ© pour mieux distribuer.

</details>

---

### Exercice 3 : PrÃ©dire la partition d'une clÃ©

**Objectif** : Calculer sur quelle partition une clÃ© atterrira avant de l'envoyer.

**Instructions** :

1. Ajouter cette mÃ©thode :

```csharp
static int PredictPartition(string key, int numPartitions)
{
    // Simuler le hash Murmur2 (simplifiÃ©)
    var hash = key.GetHashCode();
    return Math.Abs(hash) % numPartitions;
}
```

2. Avant d'envoyer un message, prÃ©dire sa partition :

```csharp
var predictedPartition = PredictPartition(customerId, 6);
Console.WriteLine($"Predicted partition for {customerId}: {predictedPartition}");

var deliveryResult = await producer.ProduceAsync(...);

Console.WriteLine($"Actual partition: {deliveryResult.Partition.Value}");
```

**Note** : La prÃ©diction peut ne pas Ãªtre exacte car `GetHashCode()` n'est pas Murmur2, mais elle donne une idÃ©e.

---

## âœ… Validation du Lab

Vous avez rÃ©ussi ce lab si :

- [ ] Vous comprenez que **Key â†’ Partition** est dÃ©terministe
- [ ] MÃªme clÃ© = mÃªme partition = **ordre prÃ©servÃ©** pour cette clÃ©
- [ ] Vous savez observer la distribution des clÃ©s sur les partitions
- [ ] Vous comprenez le problÃ¨me des hot partitions
- [ ] Vous savez quand utiliser une clÃ© (ordre, localitÃ©, compaction)

---

## ğŸ¯ Points ClÃ©s Ã  Retenir

### 1. Quand utiliser une clÃ© ?

âœ… **Utilisez une clÃ© si vous avez besoin de** :
- **Ordre garanti** : Tous les Ã©vÃ©nements d'une entitÃ© (client, commande) doivent arriver dans l'ordre
- **LocalitÃ©** : Un consumer doit voir tous les Ã©vÃ©nements d'une entitÃ© ensemble
- **Compaction** : Topic compactÃ© (derniÃ¨re valeur par clÃ© conservÃ©e)

âŒ **N'utilisez pas de clÃ© si** :
- Vous voulez une distribution uniforme sans contrainte d'ordre
- Vous avez des clÃ©s trÃ¨s dÃ©sÃ©quilibrÃ©es (risque de hot partition)

### 2. Formule de partitionnement

```
partition = murmur2_hash(key) % nombre_partitions
```

- **DÃ©terministe** : MÃªme clÃ© â†’ mÃªme partition (toujours)
- **Uniforme** : Hash Murmur2 distribue bien les clÃ©s
- **Stable** : Ne change pas si nombre de partitions constant

### 3. Hot Partitions

**ProblÃ¨me** : Une partition reÃ§oit beaucoup plus de messages que les autres.

**Causes** :
- ClÃ©s dÃ©sÃ©quilibrÃ©es (ex: 80% des messages avec mÃªme clÃ©)
- ClÃ©s mal choisies (ex: date du jour â†’ tous les messages du jour sur mÃªme partition)

**Solutions** :
- Choisir des clÃ©s bien distribuÃ©es
- Utiliser un hash de la clÃ© si nÃ©cessaire
- Augmenter le nombre de partitions
- Utiliser une clÃ© composite (ex: `customerId + orderId % 10`)

### 4. Ordre des messages

**Avec clÃ©** :
- Ordre garanti **au sein d'une partition**
- Tous les messages avec mÃªme clÃ© arrivent dans l'ordre

**Sans clÃ©** :
- Aucune garantie d'ordre global
- Sticky partitioner groupe les messages par batch

---

## ğŸ“– Concepts ThÃ©oriques

### Algorithme Murmur2

Kafka utilise l'algorithme de hash **Murmur2** pour calculer la partition Ã  partir de la clÃ© :

```java
// Pseudo-code simplifiÃ©
int partition = murmur2(key.getBytes()) % numPartitions;
```

**PropriÃ©tÃ©s** :
- Rapide (plus rapide que MD5, SHA)
- Bonne distribution (pas de collisions frÃ©quentes)
- Non-cryptographique (pas sÃ©curisÃ©, mais ce n'est pas le but)

### Sticky Partitioner (sans clÃ©)

Depuis Kafka 2.4, le partitionnement sans clÃ© utilise le **sticky partitioner** :

1. Choisir une partition alÃ©atoire
2. Envoyer tous les messages du batch sur cette partition
3. Quand le batch est plein, choisir une nouvelle partition

**Avantage** : Moins de requÃªtes rÃ©seau, meilleure performance.

### Compaction de Topic

Avec une clÃ©, vous pouvez activer la **compaction** :

```bash
--config cleanup.policy=compact
```

Kafka conserve uniquement la **derniÃ¨re valeur** pour chaque clÃ©. Utile pour :
- Event Sourcing (Ã©tat actuel d'une entitÃ©)
- CDC (Change Data Capture)
- Cache distribuÃ©

---

## ğŸš€ Prochaine Ã‰tape

Vous maÃ®trisez maintenant le partitionnement par clÃ© !

ğŸ‘‰ **Passez au [LAB 1.2C : Producer avec Gestion d'Erreurs et DLQ](../lab-1.2c-producer-error-handling/README.md)**

Dans le prochain lab, vous apprendrez :
- Classification des erreurs (retriable vs permanent)
- Pattern Dead Letter Queue (DLQ)
- Retry avec exponential backoff
- Logging et monitoring production-ready
