# LAB 1.2B : API Producer avec Cl√© - Partitionnement par Client

## ‚è±Ô∏è Dur√©e estim√©e : 60 minutes

## üè¶ Contexte E-Banking

Dans un syst√®me bancaire, l'**ordre des transactions** est critique. Si un client effectue un d√©p√¥t de 500‚Ç¨ puis un paiement de 200‚Ç¨, ces op√©rations **doivent √™tre trait√©es dans l'ordre** pour √©viter un solde n√©gatif. Kafka garantit l'ordre **uniquement au sein d'une partition**. La solution : utiliser le `CustomerId` comme **cl√© de partition** pour que toutes les transactions d'un m√™me client atterrissent sur la m√™me partition.

Dans ce lab, vous allez √©tendre l'API Web du LAB 1.2A pour utiliser le **partitionnement par cl√©**, garantissant l'**ordre chronologique** des op√©rations bancaires par client.

### Le Probl√®me : D√©sordre Sans Cl√©

```mermaid
sequenceDiagram
    participant Client as üßë‚Äçüíº CUST-001 (Solde: 0‚Ç¨)
    participant API as üöÄ API
    participant K as üî• Kafka (Sans cl√©)
    participant C0 as üì• Consumer P0
    participant C1 as üì• Consumer P3
    participant C2 as üì• Consumer P1
    participant Ledger as üí∞ Grand Livre

    Client->>API: 1. D√©p√¥t +500‚Ç¨
    API->>K: ‚Üí Partition 0 (round-robin)
    Client->>API: 2. Paiement -200‚Ç¨
    API->>K: ‚Üí Partition 3 (round-robin)
    Client->>API: 3. Virement -100‚Ç¨
    API->>K: ‚Üí Partition 1 (round-robin)

    Note over C0,C2: Les consumers traitent en parall√®le, ordre non garanti!

    C1->>Ledger: Paiement -200‚Ç¨ (solde: 0 - 200 = -200‚Ç¨)
    Note over Ledger: ‚ùå SOLDE N√âGATIF! Transaction rejet√©e!
    C0->>Ledger: D√©p√¥t +500‚Ç¨ (arrive trop tard)
    C2->>Ledger: Virement -100‚Ç¨
```

### La Solution : Partitionnement par CustomerId

```mermaid
sequenceDiagram
    participant Client as üßë‚Äçüíº CUST-001 (Solde: 0‚Ç¨)
    participant API as üöÄ API
    participant K as üî• Kafka (Cl√©: CUST-001)
    participant C2 as üì• Consumer P2
    participant Ledger as üí∞ Grand Livre

    Client->>API: 1. D√©p√¥t +500‚Ç¨
    API->>K: Cl√©=CUST-001 ‚Üí Partition 2
    Client->>API: 2. Paiement -200‚Ç¨
    API->>K: Cl√©=CUST-001 ‚Üí Partition 2
    Client->>API: 3. Virement -100‚Ç¨
    API->>K: Cl√©=CUST-001 ‚Üí Partition 2

    Note over K,C2: Un seul consumer traite la partition 2, dans l'ordre!

    C2->>Ledger: 1. D√©p√¥t +500‚Ç¨ (solde: 500‚Ç¨) ‚úÖ
    C2->>Ledger: 2. Paiement -200‚Ç¨ (solde: 300‚Ç¨) ‚úÖ
    C2->>Ledger: 3. Virement -100‚Ç¨ (solde: 200‚Ç¨) ‚úÖ
    Ledger-->>Client: üìß Solde final: 200‚Ç¨ - Correct!
```

### Distribution Multi-Clients

```mermaid
flowchart TB
    subgraph "Sans Cl√© (LAB 1.2A) - Ordre al√©atoire"
        A1["Tx CUST-001: +500‚Ç¨"] --> P1["Partition 0"]
        A2["Tx CUST-001: -200‚Ç¨"] --> P2["Partition 3"]
        A3["Tx CUST-001: -100‚Ç¨"] --> P3["Partition 1"]
    end

    subgraph "Avec Cl√© CustomerId (LAB 1.2B) - Ordre garanti"
        B1["Tx CUST-001: +500‚Ç¨"] --> Q1["Partition 2"]
        B2["Tx CUST-001: -200‚Ç¨"] --> Q1
        B3["Tx CUST-001: -100‚Ç¨"] --> Q1
        B4["Tx CUST-002: +1000‚Ç¨"] --> Q2["Partition 5"]
        B5["Tx CUST-003: +750‚Ç¨"] --> Q3["Partition 0"]
    end

    style A1 fill:#ffcc80
    style A2 fill:#ffcc80
    style A3 fill:#ffcc80
    style B1 fill:#81d4fa
    style B2 fill:#81d4fa
    style B3 fill:#81d4fa
    style B4 fill:#a5d6a7
    style B5 fill:#ce93d8
```

### Sc√©narios E-Banking : Pourquoi l'Ordre est Critique

| Sc√©nario | S√©quence | Sans cl√© (risque) | Avec cl√© (garanti) |
| -------- | -------- | ------------------ | ------------------- |
| **D√©p√¥t puis paiement** | +500‚Ç¨, -200‚Ç¨ | Paiement avant d√©p√¥t ‚Üí rejet | Ordre respect√© ‚Üí OK |
| **Cr√©dit puis d√©bit carte** | +1000‚Ç¨, -950‚Ç¨ | D√©bit d'abord ‚Üí solde n√©gatif | Ordre respect√© ‚Üí OK |
| **Virement + frais** | -500‚Ç¨, -2.50‚Ç¨ frais | Frais avant virement ‚Üí incoh√©rence | Ordre respect√© ‚Üí OK |
| **Annulation** | -100‚Ç¨, +100‚Ç¨ annul√© | Annulation avant d√©bit ‚Üí double cr√©dit | Ordre respect√© ‚Üí OK |
| **D√©tection fraude** | Tx1, Tx2, Tx3 | Analyse d√©sordonn√©e ‚Üí faux positifs | S√©quence compl√®te ‚Üí pr√©cis |

---

## üéØ Objectifs

√Ä la fin de ce lab, vous serez capable de :

1. Utiliser le **partitionnement par cl√©** (`CustomerId`) dans une API Web
2. Comprendre l'algorithme **Murmur2** et la formule `partition = hash(key) % N`
3. Garantir l'**ordre des transactions** par client
4. D√©tecter et pr√©venir les **hot partitions**
5. Tester le partitionnement via **Swagger/OpenAPI**
6. Observer la **distribution des messages** par partition

---

## üìã Pr√©requis

### LAB 1.2A compl√©t√©

Vous devez avoir compl√©t√© le LAB 1.2A (API Producer Basique).

### Topic Kafka cr√©√©

<details>
<summary>üê≥ Mode: Docker</summary>

```bash
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create --if-not-exists \
  --topic banking.transactions \
  --partitions 6 \
  --replication-factor 1
```
</details>

<details>
<summary>‚ò∏Ô∏è Mode: OKD / K3s</summary>

```bash
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-topics.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --create --if-not-exists --topic banking.transactions --partitions 6 --replication-factor 3
```
</details>

<details>
<summary>‚òÅÔ∏è Mode: OpenShift Sandbox</summary>

```bash
# Via le terminal OpenShift ou localement avec 'oc'
oc exec kafka-0 -- /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create --if-not-exists --topic banking.transactions --partitions 6 --replication-factor 3
```
</details>

---

## üöÄ Instructions Pas √† Pas

### √âtape 0 : Auto-V√©rification de l'Environnement

Avant de commencer, v√©rifiez que votre cluster est pr√™t :

| Composant | Status | Commande de v√©rification |
| :--- | :--- | :--- |
| **Kafka Broker** | En ligne | `oc get pods \| grep kafka` ou `docker ps` |
| **Topic** | Cr√©√© (6 part.) | `oc exec kafka-0 -- bin/kafka-topics.sh --list --bootstrap-server localhost:9092` |
| **Connectivit√©** | OK | `oc exec kafka-0 -- bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test --request-required-acks 1` |

---

### √âtape 1 : Cr√©er le projet API Web

#### üíª Option A : Visual Studio Code

```bash
cd lab-1.2b-producer-keyed
dotnet new webapi -n EBankingKeyedProducerAPI
cd EBankingKeyedProducerAPI
dotnet add package Confluent.Kafka --version 2.3.0
dotnet add package Swashbuckle.AspNetCore --version 6.5.0
```

#### üé® Option B : Visual Studio 2022

1. **Fichier** ‚Üí **Nouveau** ‚Üí **Projet** (`Ctrl+Shift+N`)
2. S√©lectionner **API Web ASP.NET Core**
3. Nom : `EBankingKeyedProducerAPI`, Framework : **.NET 8.0**
4. Clic droit projet ‚Üí **G√©rer les packages NuGet** :
   - `Confluent.Kafka` version **2.3.0**
   - `Swashbuckle.AspNetCore` version **6.5.0**

---

### √âtape 2 : Comprendre le partitionnement par cl√©

#### Algorithme Murmur2 : Comment Kafka Choisit la Partition

```mermaid
flowchart LR
    A["üìù Cl√©: CUST-001"] --> B["üîÑ UTF-8 Bytes"]
    B --> C["üî¢ Murmur2 Hash"]
    C --> D["hash = 0x7A3F..."]
    D --> E["üìä hash % 6"]
    E --> F["üì¶ Partition 2"]

    G["üìù Cl√©: CUST-002"] --> H["üîÑ UTF-8 Bytes"]
    H --> I["üî¢ Murmur2 Hash"]
    I --> J["hash = 0xB2E1..."]
    J --> K["üìä hash % 6"]
    K --> L["üì¶ Partition 5"]

    style A fill:#bbdefb,stroke:#1976d2
    style F fill:#c8e6c9,stroke:#388e3c
    style G fill:#ffe0b2,stroke:#f57c00
    style L fill:#c8e6c9,stroke:#388e3c
```

**Formule** : `partition = murmur2_hash(key_bytes) % nombre_partitions`

**Propri√©t√©s cl√©s** :

- **D√©terministe** : `CUST-001` ‚Üí toujours la m√™me partition (tant que le nombre de partitions ne change pas)
- **Rapide** : Murmur2 est un hash non-cryptographique optimis√© pour la performance
- **Bien distribu√©** : R√©partition uniforme des cl√©s sur les partitions

#### S√©quence Code : Partitionnement dans le Service

```mermaid
sequenceDiagram
    participant Ctrl as üìã Controller
    participant Svc as ‚öôÔ∏è KeyedKafkaProducerService
    participant Kafka as üî• Kafka Client Library
    participant Part as üìä Partitioner (Murmur2)
    participant Broker as üì¶ Kafka Broker

    Ctrl->>Svc: SendTransactionAsync(tx)
    Svc->>Svc: Key = transaction.CustomerId ("CUST-001")
    Svc->>Svc: Value = JsonSerializer.Serialize(transaction)
    Svc->>Svc: Headers: correlation-id, partition-key, event-type

    Svc->>Kafka: ProduceAsync(topic, message)
    Kafka->>Part: Calculer partition pour cl√© "CUST-001"
    Part->>Part: bytes = UTF8.GetBytes("CUST-001")
    Part->>Part: hash = Murmur2(bytes) = 0x7A3F...
    Part->>Part: partition = hash % 6 = 2
    Part-->>Kafka: Partition 2

    Kafka->>Broker: ProduceRequest ‚Üí Partition 2
    Broker-->>Kafka: ACK (Partition: 2, Offset: 42)
    Kafka-->>Svc: DeliveryResult

    Svc->>Svc: _partitionStats[2]++ (tracking)
    Svc->>Svc: _customerPartitionMap["CUST-001"] = 2
    Svc-->>Ctrl: DeliveryResult {Partition: 2, Offset: 42}
```

---

### √âtape 3 : Cr√©er le mod√®le Transaction

Cr√©er le dossier `Models` et le fichier `Models/Transaction.cs` :

```csharp
using System.ComponentModel.DataAnnotations;

namespace EBankingKeyedProducerAPI.Models;

public enum TransactionType
{
    Transfer = 1, Payment = 2, Deposit = 3, 
    Withdrawal = 4, Refund = 5, International = 6
}

public enum TransactionStatus
{
    Pending, Processing, Completed, Failed, Rejected
}

public class Transaction
{
    [Required]
    public string TransactionId { get; set; } = Guid.NewGuid().ToString();

    [Required]
    [StringLength(20, MinimumLength = 10)]
    public string FromAccount { get; set; } = string.Empty;

    [Required]
    [StringLength(20, MinimumLength = 10)]
    public string ToAccount { get; set; } = string.Empty;

    [Required]
    [Range(0.01, 1_000_000.00)]
    public decimal Amount { get; set; }

    [Required]
    [StringLength(3, MinimumLength = 3)]
    public string Currency { get; set; } = "EUR";

    [Required]
    public TransactionType Type { get; set; }

    [StringLength(500)]
    public string? Description { get; set; }

    [Required]
    public string CustomerId { get; set; } = string.Empty;

    public DateTime Timestamp { get; set; } = DateTime.UtcNow;

    [Range(0, 100)]
    public int RiskScore { get; set; } = 0;

    public TransactionStatus Status { get; set; } = TransactionStatus.Pending;
}
```

---

### √âtape 4 : Cr√©er le service Kafka Producer avec cl√©

Cr√©er le fichier `Services/KeyedKafkaProducerService.cs` :

```csharp
using Confluent.Kafka;
using System.Text.Json;
using EBankingKeyedProducerAPI.Models;

namespace EBankingKeyedProducerAPI.Services;

public class KeyedKafkaProducerService : IDisposable
{
    private readonly IProducer<string, string> _producer;
    private readonly ILogger<KeyedKafkaProducerService> _logger;
    private readonly string _topic;

    // Statistiques de distribution
    private readonly Dictionary<int, int> _partitionStats = new();
    private readonly Dictionary<string, int> _customerPartitionMap = new();

    public KeyedKafkaProducerService(IConfiguration config, ILogger<KeyedKafkaProducerService> logger)
    {
        _logger = logger;
        _topic = config["Kafka:Topic"] ?? "banking.transactions";

        var producerConfig = new ProducerConfig
        {
            BootstrapServers = config["Kafka:BootstrapServers"] ?? "localhost:9092",
            ClientId = config["Kafka:ClientId"] ?? "ebanking-keyed-producer-api",
            Acks = Acks.All,
            EnableIdempotence = true,
            MessageSendMaxRetries = 3,
            RetryBackoffMs = 1000,
            LingerMs = 10,
            BatchSize = 16384,
            CompressionType = CompressionType.Snappy
        };

        _producer = new ProducerBuilder<string, string>(producerConfig)
            .SetErrorHandler((_, error) =>
                _logger.LogError("Kafka Error: {Reason} (Code: {Code})", error.Reason, error.Code))
            .SetLogHandler((_, msg) =>
            {
                if (msg.Level >= SyslogLevel.Warning)
                    _logger.LogWarning("Kafka Log: {Message}", msg.Message);
            })
            .Build();
    }

    /// <summary>
    /// Send transaction with CustomerId as partition key
    /// </summary>
    public async Task<DeliveryResult<string, string>> SendTransactionAsync(
        Transaction transaction, CancellationToken ct = default)
    {
        var json = JsonSerializer.Serialize(transaction, new JsonSerializerOptions
        {
            PropertyNamingPolicy = JsonNamingPolicy.CamelCase
        });

        // KEY = CustomerId ‚Üí garantit l'ordre par client
        var message = new Message<string, string>
        {
            Key = transaction.CustomerId,  // PARTITION KEY
            Value = json,
            Headers = new Headers
            {
                { "correlation-id", System.Text.Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) },
                { "event-type", System.Text.Encoding.UTF8.GetBytes("transaction.created") },
                { "source", System.Text.Encoding.UTF8.GetBytes("ebanking-keyed-api") },
                { "partition-key", System.Text.Encoding.UTF8.GetBytes(transaction.CustomerId) },
                { "transaction-type", System.Text.Encoding.UTF8.GetBytes(transaction.Type.ToString()) }
            },
            Timestamp = new Timestamp(transaction.Timestamp)
        };

        var result = await _producer.ProduceAsync(_topic, message, ct);

        // Track partition statistics
        var partition = result.Partition.Value;
        _partitionStats[partition] = _partitionStats.GetValueOrDefault(partition, 0) + 1;
        _customerPartitionMap[transaction.CustomerId] = partition;

        _logger.LogInformation(
            "‚úÖ Transaction {Id} ‚Üí Key: {Key}, Partition: {P}, Offset: {O}, Amount: {Amt} {Cur}",
            transaction.TransactionId, transaction.CustomerId,
            partition, result.Offset.Value,
            transaction.Amount, transaction.Currency);

        return result;
    }

    /// <summary>
    /// Get current partition distribution statistics
    /// </summary>
    public Dictionary<int, int> GetPartitionStats() => new(_partitionStats);

    /// <summary>
    /// Get customer-to-partition mapping
    /// </summary>
    public Dictionary<string, int> GetCustomerPartitionMap() => new(_customerPartitionMap);

    public void Dispose()
    {
        _producer?.Flush(TimeSpan.FromSeconds(10));
        _producer?.Dispose();
        _logger.LogInformation("Kafka Producer disposed");
    }
}
```

---

### √âtape 5 : Cr√©er le contr√¥leur API

Cr√©er le fichier `Controllers/TransactionsController.cs` :

```csharp
using Microsoft.AspNetCore.Mvc;
using EBankingKeyedProducerAPI.Models;
using EBankingKeyedProducerAPI.Services;

namespace EBankingKeyedProducerAPI.Controllers;

[ApiController]
[Route("api/[controller]")]
[Produces("application/json")]
public class TransactionsController : ControllerBase
{
    private readonly KeyedKafkaProducerService _kafka;
    private readonly ILogger<TransactionsController> _logger;

    public TransactionsController(KeyedKafkaProducerService kafka, ILogger<TransactionsController> logger)
    {
        _kafka = kafka;
        _logger = logger;
    }

    /// <summary>
    /// Cr√©er une transaction avec partitionnement par CustomerId
    /// </summary>
    /// <remarks>
    /// La cl√© de partition est le CustomerId.
    /// Toutes les transactions d'un m√™me client vont sur la m√™me partition Kafka.
    ///
    ///     POST /api/transactions
    ///     {
    ///         "fromAccount": "FR7630001000123456789",
    ///         "toAccount":   "FR7630001000987654321",
    ///         "amount": 250.00,
    ///         "currency": "EUR",
    ///         "type": 1,
    ///         "description": "Virement mensuel loyer",
    ///         "customerId": "CUST-001"
    ///     }
    ///
    /// </remarks>
    [HttpPost]
    [ProducesResponseType(typeof(KeyedTransactionResponse), StatusCodes.Status201Created)]
    [ProducesResponseType(typeof(ProblemDetails), StatusCodes.Status400BadRequest)]
    public async Task<ActionResult<KeyedTransactionResponse>> CreateTransaction(
        [FromBody] Transaction transaction, CancellationToken ct)
    {
        if (string.IsNullOrEmpty(transaction.TransactionId))
            transaction.TransactionId = Guid.NewGuid().ToString();

        var result = await _kafka.SendTransactionAsync(transaction, ct);

        return CreatedAtAction(nameof(GetTransaction),
            new { transactionId = transaction.TransactionId },
            new KeyedTransactionResponse
            {
                TransactionId = transaction.TransactionId,
                CustomerId = transaction.CustomerId,
                PartitionKey = transaction.CustomerId,
                KafkaPartition = result.Partition.Value,
                KafkaOffset = result.Offset.Value,
                Timestamp = result.Timestamp.UtcDateTime,
                Status = "Processing"
            });
    }

    /// <summary>
    /// Envoyer un lot de transactions (partitionn√©es par client)
    /// </summary>
    [HttpPost("batch")]
    [ProducesResponseType(typeof(BatchKeyedResponse), StatusCodes.Status201Created)]
    public async Task<ActionResult<BatchKeyedResponse>> CreateBatch(
        [FromBody] List<Transaction> transactions, CancellationToken ct)
    {
        var results = new List<KeyedTransactionResponse>();

        foreach (var tx in transactions)
        {
            if (string.IsNullOrEmpty(tx.TransactionId))
                tx.TransactionId = Guid.NewGuid().ToString();

            var dr = await _kafka.SendTransactionAsync(tx, ct);
            results.Add(new KeyedTransactionResponse
            {
                TransactionId = tx.TransactionId,
                CustomerId = tx.CustomerId,
                PartitionKey = tx.CustomerId,
                KafkaPartition = dr.Partition.Value,
                KafkaOffset = dr.Offset.Value,
                Timestamp = dr.Timestamp.UtcDateTime,
                Status = "Processing"
            });
        }

        return Created("", new BatchKeyedResponse
        {
            ProcessedCount = results.Count,
            Transactions = results
        });
    }

    /// <summary>
    /// Obtenir les statistiques de distribution par partition
    /// </summary>
    [HttpGet("stats/partitions")]
    [ProducesResponseType(typeof(PartitionStatsResponse), StatusCodes.Status200OK)]
    public ActionResult<PartitionStatsResponse> GetPartitionStats()
    {
        return Ok(new PartitionStatsResponse
        {
            PartitionDistribution = _kafka.GetPartitionStats(),
            CustomerPartitionMap = _kafka.GetCustomerPartitionMap(),
            TotalMessages = _kafka.GetPartitionStats().Values.Sum()
        });
    }

    /// <summary>
    /// Obtenir le statut d'une transaction (placeholder)
    /// </summary>
    [HttpGet("{transactionId}")]
    [ProducesResponseType(typeof(KeyedTransactionResponse), StatusCodes.Status200OK)]
    public ActionResult<KeyedTransactionResponse> GetTransaction(string transactionId)
    {
        return Ok(new KeyedTransactionResponse
        {
            TransactionId = transactionId,
            Status = "Processing",
            Timestamp = DateTime.UtcNow
        });
    }

    /// <summary>
    /// Health check
    /// </summary>
    [HttpGet("health")]
    [ProducesResponseType(typeof(object), StatusCodes.Status200OK)]
    public ActionResult GetHealth()
    {
        return Ok(new
        {
            Status = "Healthy",
            Service = "EBanking Keyed Producer API",
            PartitioningStrategy = "CustomerId (Murmur2 Hash)",
            Timestamp = DateTime.UtcNow
        });
    }
}

// --- Response DTOs ---

public class KeyedTransactionResponse
{
    public string TransactionId { get; set; } = string.Empty;
    public string CustomerId { get; set; } = string.Empty;
    public string PartitionKey { get; set; } = string.Empty;
    public string Status { get; set; } = string.Empty;
    public int KafkaPartition { get; set; }
    public long KafkaOffset { get; set; }
    public DateTime Timestamp { get; set; }
}

public class BatchKeyedResponse
{
    public int ProcessedCount { get; set; }
    public List<KeyedTransactionResponse> Transactions { get; set; } = new();
}

public class PartitionStatsResponse
{
    public Dictionary<int, int> PartitionDistribution { get; set; } = new();
    public Dictionary<string, int> CustomerPartitionMap { get; set; } = new();
    public int TotalMessages { get; set; }
}
```

---

### √âtape 6 : Configurer Program.cs

```csharp
using EBankingKeyedProducerAPI.Services;
using Microsoft.OpenApi.Models;
using System.Reflection;

var builder = WebApplication.CreateBuilder(args);

builder.Services.AddControllers();
builder.Services.AddSingleton<KeyedKafkaProducerService>();

builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen(options =>
{
    options.SwaggerDoc("v1", new OpenApiInfo
    {
        Title = "E-Banking Keyed Producer API",
        Version = "v1",
        Description = "API de transactions bancaires avec partitionnement par CustomerId.\n\n"
            + "**Partitionnement** : `partition = murmur2(CustomerId) % 6`\n\n"
            + "**Endpoints :**\n"
            + "- `POST /api/transactions` ‚Äî Transaction avec cl√© CustomerId\n"
            + "- `POST /api/transactions/batch` ‚Äî Lot de transactions\n"
            + "- `GET /api/transactions/stats/partitions` ‚Äî Distribution par partition\n"
            + "- `GET /api/transactions/health` ‚Äî Health check"
    });

    var xmlFile = $"{Assembly.GetExecutingAssembly().GetName().Name}.xml";
    var xmlPath = Path.Combine(AppContext.BaseDirectory, xmlFile);
    if (File.Exists(xmlPath))
        options.IncludeXmlComments(xmlPath);
});

var app = builder.Build();

app.UseSwagger();
app.UseSwaggerUI(c =>
{
    c.SwaggerEndpoint("/swagger/v1/swagger.json", "E-Banking Keyed Producer API v1");
    c.RoutePrefix = "swagger";
});

app.MapControllers();

var logger = app.Services.GetRequiredService<ILogger<Program>>();
logger.LogInformation("========================================");
logger.LogInformation("  E-Banking Keyed Producer API");
logger.LogInformation("  Partitioning: CustomerId (Murmur2)");
logger.LogInformation("  Swagger UI : https://localhost:5001/swagger");
logger.LogInformation("========================================");

app.Run();
```

---

### √âtape 7 : Configurer appsettings.json

```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "Kafka": {
    "BootstrapServers": "localhost:9092",
    "Topic": "banking.transactions",
    "ClientId": "ebanking-keyed-producer-api"
  }
}
```

---

### √âtape 8 : Ex√©cuter et tester (Local)

```bash
dotnet run
```

Ouvrir Swagger UI : **<https://localhost:5001/swagger>**

---

## ‚òÅÔ∏è D√©ploiement sur OpenShift Sandbox

> **üéØ Objectif** : Ce d√©ploiement valide les concepts de **partitionnement par cl√© (Keyed Producer)** dans un environnement cloud :
> - **Cl√© de partitionnement** : le `customerId` d√©termine la partition cible via l'algorithme **Murmur2**
> - **Garantie d'ordre** : toutes les transactions d'un m√™me client arrivent dans la **m√™me partition**, dans l'ordre
> - **Distribution √©quitable** : des clients diff√©rents se r√©partissent sur des partitions diff√©rentes
> - **V√©rification via Kafka CLI** : lire une partition sp√©cifique pour prouver l'isolation par client

Si vous utilisez l'OpenShift Developer Sandbox, suivez ces √©tapes pour d√©ployer l'API :

### 1. Cr√©er le Build et l'Application
Depuis le dossier `EBankingKeyedProducerAPI` :

```bash
# 1. Cr√©er la config de build binaire
oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-keyed-producer-api

# 2. Lancer le build en envoyant le code source
oc start-build ebanking-keyed-producer-api --from-dir=. --follow

# 3. Cr√©er l'application √† partir de l'image g√©n√©r√©e
oc new-app ebanking-keyed-producer-api
```

### 2. Configurer les variables d'environnement
Il est crucial de pointer vers le service Kafka interne et de configurer l'√©coute sur le port 8080 :

```bash
oc set env deployment/ebanking-keyed-producer-api \
  Kafka__BootstrapServers=kafka-svc:9092 \
  ASPNETCORE_URLS=http://0.0.0.0:8080 \
  ASPNETCORE_ENVIRONMENT=Development
```

### 3. Exposer publiquement (Secure Edge Route)

> [!IMPORTANT]
> Standard routes may hang on the Sandbox. Use an **edge route** for reliable public access.

```bash
oc create route edge ebanking-keyed-api --service=ebanking-keyed-producer-api --port=8080-tcp
```

### 4. Tester l'API d√©ploy√©e

```bash
HOST=$(oc get route ebanking-keyed-api -o jsonpath='{.spec.host}')
echo "Swagger UI : https://$HOST/swagger"
```

### 5. Stability Warning

For Sandbox environments, use `Acks = Acks.Leader` and `EnableIdempotence = false` in `ProducerConfig` to avoid `Coordinator load in progress` hangs.

### 6. üß™ Sc√©narios de Test et Validation des Concepts (Sandbox)

> Les sc√©narios Swagger d√©taill√©s ci-dessous (Tests OpenAPI) fonctionnent aussi bien en local qu'en Sandbox.
> Pour le Sandbox, utilisez `https://$HOST` au lieu de `https://localhost:5001`.

Apr√®s avoir ex√©cut√© les Tests OpenAPI (section suivante), v√©rifiez la distribution dans Kafka **via le CLI du Sandbox** :

```bash
HOST=$(oc get route ebanking-keyed-api -o jsonpath='{.spec.host}')

# Envoyer des transactions pour diff√©rents clients
curl -k -s -X POST "https://$HOST/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000111111111","toAccount":"FR7630001000222222222","amount":500.00,"currency":"EUR","type":3,"description":"Depot CUST-001","customerId":"CUST-001"}' | jq .kafkaPartition

curl -k -s -X POST "https://$HOST/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000333333333","toAccount":"FR7630001000444444444","amount":1500.00,"currency":"EUR","type":1,"description":"Virement CUST-002","customerId":"CUST-002"}' | jq .kafkaPartition
```

**üìñ V√©rification** : Les deux `kafkaPartition` doivent √™tre **diff√©rents** (clients diff√©rents ‚Üí partitions diff√©rentes).

```bash
# Consulter les statistiques de distribution
curl -k -s "https://$HOST/api/Transactions/stats/partitions" | jq .

# V√©rifier dans Kafka que les partitions contiennent les bons messages
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --partition 2 \
  --from-beginning --max-messages 5

# V√©rifier la distribution des offsets
oc exec kafka-0 -- /opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic banking.transactions
```

**üìñ Concepts valid√©s** :

| Concept | Comment le v√©rifier |
| ------- | ------------------- |
| M√™me cl√© ‚Üí m√™me partition | Envoyer 3 tx pour CUST-001, v√©rifier que `kafkaPartition` est identique |
| Cl√©s diff√©rentes ‚Üí partitions diff√©rentes | Comparer `kafkaPartition` de CUST-001 vs CUST-002 |
| Ordre garanti par partition | Lire une partition sp√©cifique, l'ordre chronologique est respect√© |
| Distribution Murmur2 | `GET /stats/partitions` montre la map `customerId ‚Üí partition` |

#### R√©capitulatif des Endpoints

| M√©thode | Endpoint | Objectif p√©dagogique |
| ------- | -------- | -------------------- |
| `POST` | `/api/Transactions` | Produire un message avec cl√© (`customerId`) |
| `POST` | `/api/Transactions/batch` | Produire un lot ‚Äî observer la distribution par cl√© |
| `GET` | `/api/Transactions/stats/partitions` | Voir la map `customerId ‚Üí partition` et la distribution |
| `GET` | `/api/Transactions/{id}` | Obtenir le statut d'une transaction |
| `GET` | `/api/Transactions/health` | V√©rifier la disponibilit√© du service |

---

## üß™ Tests OpenAPI (Swagger)

### Test 1 : Transactions du m√™me client (v√©rifier m√™me partition)

Envoyer 3 transactions pour **CUST-001** via **POST /api/transactions** :

**Transaction 1** ‚Äî D√©p√¥t :

```json
{
  "fromAccount": "BANK-DEPOSIT-001",
  "toAccount": "FR7630001000123456789",
  "amount": 500.00,
  "currency": "EUR",
  "type": 3,
  "description": "D√©p√¥t salaire janvier",
  "customerId": "CUST-001"
}
```

**Transaction 2** ‚Äî Paiement :

```json
{
  "fromAccount": "FR7630001000123456789",
  "toAccount": "FR7630001000111222333",
  "amount": 200.00,
  "currency": "EUR",
  "type": 2,
  "description": "Paiement loyer",
  "customerId": "CUST-001"
}
```

**Transaction 3** ‚Äî Virement :

```json
{
  "fromAccount": "FR7630001000123456789",
  "toAccount": "FR7630001000444555666",
  "amount": 100.00,
  "currency": "EUR",
  "type": 1,
  "description": "Virement √©pargne",
  "customerId": "CUST-001"
}
```

**V√©rification** : Les 3 r√©ponses doivent montrer le **m√™me `kafkaPartition`**.

### Test 2 : Transactions de clients diff√©rents

Envoyer via **POST /api/transactions/batch** :

```json
[
  {
    "fromAccount": "FR7630001000111111111",
    "toAccount": "FR7630001000999999999",
    "amount": 75.00,
    "currency": "EUR",
    "type": 2,
    "description": "Paiement Netflix",
    "customerId": "CUST-001"
  },
  {
    "fromAccount": "FR7630001000222222222",
    "toAccount": "FR7630001000888888888",
    "amount": 1500.00,
    "currency": "EUR",
    "type": 1,
    "description": "Virement investissement",
    "customerId": "CUST-002"
  },
  {
    "fromAccount": "FR7630001000333333333",
    "toAccount": "GB29NWBK60161331926819",
    "amount": 5000.00,
    "currency": "EUR",
    "type": 6,
    "description": "International transfer",
    "customerId": "CUST-003"
  }
]
```

**V√©rification** : Chaque `customerId` a un `kafkaPartition` potentiellement diff√©rent.

### Test 3 : Consulter les statistiques de distribution

Appeler **GET /api/transactions/stats/partitions** :

**R√©ponse attendue** :

```json
{
  "partitionDistribution": {
    "2": 4,
    "5": 1,
    "0": 1
  },
  "customerPartitionMap": {
    "CUST-001": 2,
    "CUST-002": 5,
    "CUST-003": 0
  },
  "totalMessages": 6
}
```

**Observation cl√©** : CUST-001 a 4 messages, tous sur la **m√™me partition** (2).

---

## üìä V√©rifier dans Kafka

### Lire une partition sp√©cifique

```bash
# Lire uniquement la partition 2 (CUST-001)
docker exec kafka /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --partition 2 \
  --from-beginning
```

Vous ne verrez que les transactions de **CUST-001**, dans l'ordre chronologique.

### V√©rifier la distribution

```bash
docker exec kafka /opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic banking.transactions
```

---

## üñ•Ô∏è D√©ploiement Local OpenShift (CRC / OpenShift Local)

Si vous disposez d'un cluster **OpenShift Local** (anciennement CRC ‚Äî CodeReady Containers), vous pouvez d√©ployer l'API directement depuis votre machine.

### 1. Pr√©requis

```bash
# V√©rifier que le cluster est d√©marr√©
crc status

# Se connecter au cluster
oc login -u developer https://api.crc.testing:6443
oc project ebanking-labs
```

### 2. Build et D√©ploiement (Binary Build)

```bash
cd EBankingKeyedProducerAPI

# Cr√©er la build config et lancer le build
oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-keyed-producer-api
oc start-build ebanking-keyed-producer-api --from-dir=. --follow

# Cr√©er l'application
oc new-app ebanking-keyed-producer-api
```

### 3. Configurer les variables d'environnement

```bash
oc set env deployment/ebanking-keyed-producer-api \
  Kafka__BootstrapServers=kafka-svc:9092 \
  Kafka__Topic=banking.transactions \
  ASPNETCORE_URLS=http://0.0.0.0:8080 \
  ASPNETCORE_ENVIRONMENT=Development
```

### 4. Exposer et tester

```bash
# Cr√©er une route edge
oc create route edge ebanking-keyed-producer-api-secure --service=ebanking-keyed-producer-api --port=8080-tcp

# Obtenir l'URL
URL=$(oc get route ebanking-keyed-producer-api-secure -o jsonpath='{.spec.host}')
echo "https://$URL/swagger"

# Tester
curl -k -i "https://$URL/api/Transactions/health"
```

### 5. üß™ Validation des concepts (CRC)

```bash
URL=$(oc get route ebanking-keyed-producer-api-secure -o jsonpath='{.spec.host}')

# Envoyer 2 transactions pour le m√™me client
curl -k -s -X POST "https://$URL/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000111111111","toAccount":"FR7630001000222222222","amount":500.00,"currency":"EUR","type":3,"description":"Tx1 CUST-001","customerId":"CUST-001"}' | jq .kafkaPartition

curl -k -s -X POST "https://$URL/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000333333333","toAccount":"FR7630001000444444444","amount":100.00,"currency":"EUR","type":2,"description":"Tx2 CUST-001","customerId":"CUST-001"}' | jq .kafkaPartition

# Les deux kafkaPartition doivent √™tre IDENTIQUES (m√™me cl√© ‚Üí m√™me partition)

# Consulter les stats de distribution
curl -k -s "https://$URL/api/Transactions/stats/partitions" | jq .

# V√©rifier dans Kafka ‚Äî lire une partition sp√©cifique
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --partition 2 --from-beginning --max-messages 5
```

### 6. Alternative : D√©ploiement par manifeste YAML

```bash
sed "s/\${NAMESPACE}/ebanking-labs/g" deployment/openshift-deployment.yaml | oc apply -f -
```

---

## ‚ò∏Ô∏è D√©ploiement Kubernetes / OKD (K3s, K8s, OKD)

Pour un cluster **Kubernetes standard** (K3s, K8s, Minikube) ou **OKD**, utilisez les manifestes YAML fournis dans le dossier `deployment/`.

### 1. Construire l'image Docker

```bash
cd EBankingKeyedProducerAPI

# Build de l'image
docker build -t ebanking-keyed-producer-api:latest .

# Pour un registry distant (adapter l'URL du registry)
docker tag ebanking-keyed-producer-api:latest <registry>/ebanking-keyed-producer-api:latest
docker push <registry>/ebanking-keyed-producer-api:latest
```

> **K3s / Minikube** : Si vous utilisez un cluster local, l'image locale suffit avec `imagePullPolicy: IfNotPresent`.

### 2. D√©ployer les manifestes

```bash
# Appliquer le Deployment + Service + Ingress
kubectl apply -f deployment/k8s-deployment.yaml

# V√©rifier le d√©ploiement
kubectl get pods -l app=ebanking-keyed-producer-api
kubectl get svc ebanking-keyed-producer-api
```

### 3. Configurer le Kafka Bootstrap (si diff√©rent)

```bash
kubectl set env deployment/ebanking-keyed-producer-api \
  Kafka__BootstrapServers=<kafka-bootstrap>:9092
```

### 4. Acc√©der √† l'API

```bash
# Port-forward pour acc√®s local
kubectl port-forward svc/ebanking-keyed-producer-api 8080:8080

# Tester
curl http://localhost:8080/api/Transactions/health
curl http://localhost:8080/swagger/index.html
```

> **Ingress** : Si vous avez un Ingress Controller (nginx, traefik), ajoutez `ebanking-keyed-producer-api.local` √† votre fichier `/etc/hosts` pointant vers l'IP du cluster.

### 5. üß™ Validation des concepts (K8s)

```bash
# Envoyer des transactions pour 2 clients diff√©rents (port-forward actif sur 8080)
curl -s -X POST "http://localhost:8080/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000111111111","toAccount":"FR7630001000222222222","amount":500.00,"currency":"EUR","type":3,"description":"Test K8s CUST-001","customerId":"CUST-001"}' | jq .kafkaPartition

curl -s -X POST "http://localhost:8080/api/Transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000333333333","toAccount":"FR7630001000444444444","amount":1500.00,"currency":"EUR","type":1,"description":"Test K8s CUST-002","customerId":"CUST-002"}' | jq .kafkaPartition

# Consulter les stats de distribution
curl -s "http://localhost:8080/api/Transactions/stats/partitions" | jq .

# V√©rifier dans Kafka
kubectl exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --partition 2 --from-beginning --max-messages 5

# V√©rifier la distribution des offsets
kubectl exec kafka-0 -- /opt/kafka/bin/kafka-run-class.sh kafka.tools.GetOffsetShell \
  --broker-list localhost:9092 --topic banking.transactions
```

> **Docker Compose** : Si Kafka tourne via Docker Compose, utilisez `docker exec kafka ...` au lieu de `kubectl exec kafka-0 ...`.

### 6. OKD : Utiliser les manifestes OpenShift

```bash
sed "s/\${NAMESPACE}/$(oc project -q)/g" deployment/openshift-deployment.yaml | oc apply -f -
```

---

## üéØ Concepts Cl√©s Expliqu√©s

### Partitionnement Murmur2

| √âl√©ment | Description |
| ------- | ----------- |
| **Algorithme** | Murmur2 (non-cryptographique, rapide) |
| **Formule** | `partition = murmur2(key_bytes) % num_partitions` |
| **D√©terministe** | M√™me cl√© ‚Üí m√™me partition (toujours) |
| **Stable** | Ne change pas tant que le nombre de partitions est constant |

### Quand utiliser une cl√© en e-banking ?

| Sc√©nario | Cl√© recommand√©e | Raison |
| -------- | --------------- | ------ |
| **Transactions client** | `CustomerId` | Ordre des op√©rations par client |
| **Op√©rations compte** | `AccountId` | Coh√©rence du solde |
| **Paiements carte** | `CardNumber` | D√©tection fraude s√©quentielle |
| **Virements internationaux** | `CustomerId` | Conformit√© et audit |
| **Pr√©l√®vements** | `MandateId` | Suivi du mandat SEPA |
| **Notifications** | `CustomerId` | Ordre des alertes par client |

### S√©quence : V√©rification de la Distribution (Endpoint Stats)

```mermaid
sequenceDiagram
    participant C as üåê Client (Swagger)
    participant Ctrl as üìã Controller
    participant Svc as ‚öôÔ∏è KeyedKafkaProducer

    Note over C: Apr√®s avoir envoy√© 6 transactions
    C->>Ctrl: GET /api/transactions/stats/partitions
    Ctrl->>Svc: GetPartitionStats()
    Svc-->>Ctrl: {2: 4, 5: 1, 0: 1}
    Ctrl->>Svc: GetCustomerPartitionMap()
    Svc-->>Ctrl: {CUST-001: 2, CUST-002: 5, CUST-003: 0}
    Ctrl-->>C: 200 OK

    Note over C: V√©rification:
    Note over C: CUST-001 ‚Üí 4 messages, tous Partition 2 ‚úÖ
    Note over C: CUST-002 ‚Üí 1 message, Partition 5 ‚úÖ
    Note over C: CUST-003 ‚Üí 1 message, Partition 0 ‚úÖ
```

### Hot Partition : D√©tection et Pr√©vention

```mermaid
flowchart TB
    subgraph "Hot Partition (Anti-Pattern)"
        direction TB
        HP_A["CUST-VIP-001: 10000 tx/jour"] --> HP_P2["Partition 2: 10000 msg"]
        HP_B["CUST-002: 5 tx/jour"] --> HP_P5["Partition 5: 5 msg"]
        HP_C["CUST-003: 3 tx/jour"] --> HP_P0["Partition 0: 3 msg"]
    end

    subgraph "Solution : Cl√© Composite"
        direction TB
        S_A["CUST-VIP-001-ACC-A: 5000 tx"] --> S_P2["Partition 2: 5000 msg"]
        S_B["CUST-VIP-001-ACC-B: 5000 tx"] --> S_P4["Partition 4: 5000 msg"]
        S_C["CUST-002: 5 tx"] --> S_P5["Partition 5: 5 msg"]
    end

    style HP_P2 fill:#ffcdd2,stroke:#d32f2f
    style S_P2 fill:#c8e6c9,stroke:#388e3c
    style S_P4 fill:#c8e6c9,stroke:#388e3c
```

> **Astuce** : Si un client VIP g√©n√®re un volume disproportionn√©, utilisez une cl√© composite `CustomerId + AccountId` pour r√©partir la charge.

### Anti-patterns en e-banking

| Anti-pattern | Probl√®me | Solution |
| ------------ | -------- | -------- |
| Cl√© = date du jour | Hot partition (toutes les tx du jour) | Utiliser `CustomerId` |
| Cl√© = type de transaction | 80% des tx sont des paiements | Utiliser `CustomerId` |
| Pas de cl√© | Ordre des tx non garanti | Toujours utiliser `CustomerId` |
| Cl√© = agence | D√©s√©quilibre si une agence est plus active | Utiliser `CustomerId` |
| Ajout de partitions | Redistribution des cl√©s ‚Üí ordre cass√© | Planifier les partitions d√®s le d√©part |

---

## üîß Troubleshooting

| Sympt√¥me | Cause probable | Solution |
| -------- | -------------- | -------- |
| M√™me client sur partitions diff√©rentes | Cl√© inconsistante | V√©rifier que `CustomerId` est identique (casse) |
| Hot partition (80%+ sur une partition) | Un client domine le trafic | Cl√© composite `CustomerId + AccountId` |
| Distribution in√©gale | Peu de clients uniques | Normal avec peu de cl√©s, augmenter les clients |
| Ordre non respect√© | `EnableIdempotence = false` | Activer l'idempotence |

---

## ‚úÖ Validation du Lab

- [ ] L'API d√©marre et Swagger UI est accessible
- [ ] `POST /api/transactions` retourne la partition dans la r√©ponse
- [ ] 3 transactions du m√™me client vont sur la **m√™me partition**
- [ ] 3 transactions de clients diff√©rents montrent des partitions (potentiellement) diff√©rentes
- [ ] `GET /api/transactions/stats/partitions` affiche la distribution correcte
- [ ] Le `customerPartitionMap` montre un mapping client ‚Üí partition stable
- [ ] Vous comprenez la formule `partition = murmur2(key) % N`

---

## üöÄ Prochaine √âtape

üëâ **[LAB 1.2C : API Producer avec Gestion d'Erreurs et DLQ](../lab-1.2c-producer-error-handling/README.md)**

Dans le prochain lab :

- Classification des erreurs (retriable vs permanent)
- Pattern Dead Letter Queue (DLQ) pour les transactions √©chou√©es
- Retry avec exponential backoff
- Fallback fichier local si DLQ √©choue
