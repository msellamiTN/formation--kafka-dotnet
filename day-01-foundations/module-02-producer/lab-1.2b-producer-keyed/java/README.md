# LAB 1.2B (Java) : Producer avec Cl√© ‚Äî Partitionnement D√©terministe

## ‚è±Ô∏è Dur√©e estim√©e : 45 minutes

## üè¶ Contexte E-Banking

Dans une banque, il est crucial que **toutes les transactions d'un m√™me client arrivent en ordre** sur Kafka. Ce lab impl√©mente un producer qui utilise la cl√© `customerId` pour garantir que toutes les transactions d'un client sont envoy√©es vers la m√™me partition, pr√©servant ainsi l'ordre chronologique.

## üéØ Objectifs

√Ä la fin de ce lab, vous serez capable de :

1. Comprendre comment la **cl√© d√©termine la partition** dans Kafka
2. Impl√©menter un **producer avec cl√©** pour garantir l'ordre
3. Visualiser le **partitionnement hash-based** (Murmur2)
4. Comparer les messages avec et sans cl√©
5. √âviter les **hot partitions** avec une bonne strat√©gie de cl√©s
6. Comprendre la **garantie d'ordre par cl√©**

---

## üìä Architecture

### Producer avec Cl√© ‚Üí Partition D√©terministe

```mermaid
flowchart LR
    subgraph API["üöÄ Spring Boot API"]
        C["‚öôÔ∏è TransactionController"]
        S["üì¶ KafkaTemplate"]
        P["üìã Keyed Producer"]
    end

    subgraph Kafka["üî• Kafka"]
        P0["üìã Partition 0"]
        P1["üìã Partition 1"]
        P2["üìã Partition 2"]
    end

    subgraph Hash["üîê Murmur2 Hash"]
        H1["hash(CUST-001) % 3 = 1"]
        H2["hash(CUST-002) % 3 = 2"]
        H3["hash(CUST-003) % 3 = 0"]
    end

    C --> S
    S --> P
    P --> H
    H --> P1
    H --> P2
    H --> P0

    style API fill:#e8f5e8,stroke:#388e3c
    style Kafka fill:#fff3e0,stroke:#f57c00
    style Hash fill:#f3e5f5,stroke:#9e9e9e
```

---

## üèóÔ∏è Structure du Projet

```
java/
‚îú‚îÄ‚îÄ src/main/java/com/data2ai/kafka/producer/keyed/
‚îÇ   ‚îú‚îÄ‚îÄ EBankingProducerApplication.java
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ KafkaConfig.java
‚îÇ   ‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Transaction.java
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TransactionType.java
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TransactionStatus.java
‚îÇ   ‚îú‚îÄ‚îÄ producer/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ KeyedTransactionProducer.java
‚îÇ   ‚îú‚îÄ‚îÄ controller/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ TransactionController.java
‚îÇ   ‚îú‚îÄ‚îÄ dto/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ CreateTransactionRequest.java
‚îÇ   ‚îî‚îÄ‚îÄ service/
‚îÇ       ‚îî‚îÄ‚îÄ PartitionAnalyzer.java
‚îú‚îÄ‚îÄ src/main/resources/
‚îÇ   ‚îî‚îÄ‚îÄ application.yml
‚îî‚îÄ‚îÄ pom.xml
```

---

## üìã √âtapes de R√©alisation

### √âtape 1 : Configuration Maven (`pom.xml`)

> **‚ö†Ô∏è Important** : Assurez-vous que le plugin Spring Boot Maven est correctement configur√© pour cr√©er un JAR ex√©cutable :

```xml
<plugin>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-maven-plugin</artifactId>
    <version>${spring-boot.version}</version>
    <executions>
        <execution>
            <goals>
                <goal>repackage</goal>
            </goals>
        </execution>
    </executions>
</plugin>
```

### √âtape 2 : Producer avec Cl√© (`producer/KeyedTransactionProducer.java`)

```java
@Service
@Slf4j
public class KeyedTransactionProducer {

    private final KafkaTemplate<String, String> kafkaTemplate;
    private final ObjectMapper objectMapper;

    @Value("${app.kafka.topic:banking.transactions}")
    private String topic;

    public KeyedTransactionProducer(KafkaTemplate<String, String> kafkaTemplate, ObjectMapper objectMapper) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
    }

    public CompletableFuture<SendResult<String, String>> sendTransaction(Transaction transaction) {
        try {
            transaction.setTransactionId(UUID.randomUUID().toString());
            transaction.setTimestamp(Instant.now());
            transaction.setStatus(TransactionStatus.PENDING);

            String jsonValue = objectMapper.writeValueAsString(transaction);
            
            // La cl√© est le customerId pour garantir l'ordre par client
            String key = transaction.getCustomerId();
            
            log.info("Sending keyed transaction: {} | Key: {} | Amount: {} {} | Customer: {}",
                    transaction.getTransactionId(),
                    key,
                    transaction.getAmount(),
                    transaction.getCurrency(),
                    transaction.getCustomerId());

            ListenableFuture<SendResult<String, String>> future = kafkaTemplate.send(topic, key, jsonValue);
            
            // Add callback for async handling
            future.addCallback(
                result -> {
                    transaction.setStatus(TransactionStatus.COMPLETED);
                    transaction.setKafkaPartition(result.getRecordMetadata().partition());
                    transaction.setKafkaOffset(result.getRecordMetadata().offset());
                    log.info("Keyed transaction sent successfully: {} | Key: {} | Partition: {} | Offset: {}",
                            transaction.getTransactionId(),
                            key,
                            result.getRecordMetadata().partition(),
                            result.getRecordMetadata().offset());
                },
                failure -> {
                    transaction.setStatus(TransactionStatus.FAILED);
                    log.error("Failed to send keyed transaction: {} | Key: {} | Error: {}",
                            transaction.getTransactionId(),
                            key,
                            failure.getMessage());
                }
            );

            return future.toCompletableFuture();
        } catch (Exception e) {
            log.error("Error preparing keyed transaction for Kafka", e);
            throw new RuntimeException("Failed to prepare transaction", e);
        }
    }

    public Transaction sendTransactionSync(Transaction transaction) {
        try {
            transaction.setTransactionId(UUID.randomUUID().toString());
            transaction.setTimestamp(Instant.now());
            transaction.setStatus(TransactionStatus.PENDING);

            String jsonValue = objectMapper.writeValueAsString(transaction);
            String key = transaction.getCustomerId();
            
            log.info("Sending keyed transaction synchronously: {} | Key: {} | Amount: {} {} | Customer: {}",
                    transaction.getTransactionId(),
                    key,
                    transaction.getAmount(),
                    transaction.getCurrency(),
                    transaction.getCustomerId());

            RecordMetadata metadata = kafkaTemplate.send(topic, key, jsonValue).get(5, TimeUnit.SECONDS);
            
            transaction.setStatus(TransactionStatus.COMPLETED);
            transaction.setKafkaPartition(metadata.partition());
            transaction.setKafkaOffset(metadata.offset());
            
            log.info("Keyed transaction sent successfully: {} | Key: {} | Partition: {} | Offset: {}",
                    transaction.getTransactionId(),
                    key,
                    metadata.partition(),
                    metadata.offset());
            
            return transaction;
        } catch (Exception e) {
            transaction.setStatus(TransactionStatus.FAILED);
            log.error("Failed to send keyed transaction synchronously: {} | Key: {} | Error: {}",
                    transaction.getTransactionId(),
                    transaction.getCustomerId(),
                    e.getMessage());
            throw new RuntimeException("Failed to send transaction", e);
        }
    }

    public void sendTransactionWithoutKey(Transaction transaction) {
        try {
            transaction.setTransactionId(UUID.randomUUID().toString());
            transaction.setTimestamp(Instant.now());
            transaction.setStatus(TransactionStatus.PENDING);

            String jsonValue = objectMapper.writeValueAsString(transaction);
            
            // SANS cl√© - partitionnement round-robin
            log.info("Sending transaction WITHOUT key: {} | Amount: {} {} | Customer: {}",
                    transaction.getTransactionId(),
                    transaction.getAmount(),
                    transaction.getCurrency(),
                    transaction.getCustomerId());

            RecordMetadata metadata = kafkaTemplate.send(topic, null, jsonValue).get(5, TimeUnit.SECONDS);
            
            transaction.setStatus(TransactionStatus.COMPLETED);
            transaction.setKafkaPartition(metadata.partition());
            transaction.setKafkaOffset(metadata.offset());
            
            log.info("Transaction without key sent: {} | Partition: {} | Offset: {}",
                    transaction.getTransactionId(),
                    metadata.partition(),
                    metadata.offset());
        } catch (Exception e) {
            transaction.setStatus(TransactionStatus.FAILED);
            log.error("Failed to send transaction without key", e);
            throw new RuntimeException("Failed to send transaction", e);
        }
    }
}
```

### √âtape 3 : Service d'Analyse de Partitions (`service/PartitionAnalyzer.java`)

```java
@Service
@Slf4j
public class PartitionAnalyzer {

    public static int calculatePartition(String key, int numPartitions) {
        // Simulation simplifi√©e du hash Murmur2 utilis√© par Kafka
        return Math.abs(key.hashCode()) % numPartitions;
    }

    public Map<String, Object> analyzeKeyDistribution(List<String> customerIds, int numPartitions) {
        Map<Integer, List<String>> partitionMap = new HashMap<>();
        for (int i = 0; i < numPartitions; i++) {
            partitionMap.put(i, new ArrayList<>());
        }

        for (String customerId : customerIds) {
            int partition = calculatePartition(customerId, numPartitions);
            partitionMap.get(partition).add(customerId);
        }

        Map<String, Object> analysis = new HashMap<>();
        analysis.put("totalCustomers", customerIds.size());
        analysis.put("numPartitions", numPartitions);
        analysis.put("distribution", partitionMap);
        
        // Calculer la distribution
        Map<String, Integer> distributionCounts = new HashMap<>();
        partitionMap.forEach((partition, customers) -> {
            distributionCounts.put("partition-" + partition, customers.size());
        });
        analysis.put("counts", distributionCounts);

        return analysis;
    }
}
```

### √âtape 4 : Contr√¥leur REST avec Tests de Partitionnement (`controller/TransactionController.java`)

```java
@RestController
@RequestMapping("/api/v1")
@Slf4j
public class TransactionController {

    private final KeyedTransactionProducer transactionProducer;
    private final PartitionAnalyzer partitionAnalyzer;

    public TransactionController(KeyedTransactionProducer transactionProducer, 
                                PartitionAnalyzer partitionAnalyzer) {
        this.transactionProducer = transactionProducer;
        this.partitionAnalyzer = partitionAnalyzer;
    }

    @PostMapping("/transactions")
    public ResponseEntity<Transaction> createTransaction(@RequestBody CreateTransactionRequest request) {
        Transaction transaction = new Transaction();
        transaction.setFromAccount(request.getFromAccount());
        transaction.setToAccount(request.getToAccount());
        transaction.setAmount(request.getAmount());
        transaction.setCurrency(request.getCurrency());
        transaction.setType(request.getType());
        transaction.setDescription(request.getDescription());
        transaction.setCustomerId(request.getCustomerId());

        try {
            Transaction result = transactionProducer.sendTransactionSync(transaction);
            return ResponseEntity.ok(result);
        } catch (Exception e) {
            log.error("Failed to create transaction", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    @PostMapping("/transactions/without-key")
    public ResponseEntity<Transaction> createTransactionWithoutKey(@RequestBody CreateTransactionRequest request) {
        Transaction transaction = new Transaction();
        transaction.setFromAccount(request.getFromAccount());
        transaction.setToAccount(request.getToAccount());
        transaction.setAmount(request.getAmount());
        transaction.setCurrency(request.getCurrency());
        transaction.setType(request.getType());
        transaction.setDescription(request.getDescription());
        transaction.setCustomerId(request.getCustomerId());

        try {
            transactionProducer.sendTransactionWithoutKey(transaction);
            return ResponseEntity.ok(transaction);
        } catch (Exception e) {
            log.error("Failed to create transaction without key", e);
            return ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR).build();
        }
    }

    @PostMapping("/transactions/batch")
    public ResponseEntity<Map<String, Object>> createTransactionsBatch(@RequestBody List<CreateTransactionRequest> requests) {
        List<CompletableFuture<SendResult<String, String>>> futures = new ArrayList<>();
        AtomicInteger successCount = new AtomicInteger(0);
        AtomicInteger failureCount = new AtomicInteger(0);

        for (CreateTransactionRequest request : requests) {
            Transaction transaction = new Transaction();
            transaction.setFromAccount(request.getFromAccount());
            transaction.setToAccount(request.getToAccount());
            transaction.setAmount(request.getAmount());
            transaction.setCurrency(request.getCurrency());
            transaction.setType(request.getType());
            transaction.setDescription(request.getDescription());
            transaction.setCustomerId(request.getCustomerId());

            CompletableFuture<SendResult<String, String>> future = transactionProducer.sendTransaction(transaction);
            futures.add(future);
            
            future.thenAccept(result -> successCount.incrementAndGet())
                    .exceptionally(throwable -> {
                        failureCount.incrementAndGet();
                        return null;
                    });
        }

        // Wait for all to complete
        CompletableFuture.allOf(futures.toArray(new CompletableFuture[0])).join();

        return ResponseEntity.ok(Map.of(
            "total", requests.size(),
            "success", successCount.get(),
            "failed", failureCount.get(),
            "message", "Batch processing completed"
        ));
    }

    @GetMapping("/partitions/analyze")
    public ResponseEntity<Map<String, Object>> analyzePartitions(@RequestParam List<String> customerIds) {
        Map<String, Object> analysis = partitionAnalyzer.analyzeKeyDistribution(customerIds, 3);
        return ResponseEntity.ok(analysis);
    }

    @GetMapping("/health")
    public ResponseEntity<Map<String, String>> health() {
        return ResponseEntity.ok(Map.of(
            "status", "UP",
            "service", "EBanking Keyed Producer API",
            "timestamp", Instant.now().toString()
        ));
    }
}
```

### √âtape 5 : Configuration Application (`application.yml`)

```yaml
server:
  port: 8080

spring:
  application:
    name: ebanking-producer-keyed-java
  
  kafka:
    bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      acks: all
      retries: 3
      linger-ms: 10
      batch-size: 16384

app:
  kafka:
    topic: ${KAFKA_TOPIC:banking.transactions}

logging:
  level:
    com.data2ai.kafka.producer: INFO
    org.apache.kafka: WARN
    org.springframework.kafka: WARN
```

---

## üöÄ D√©ploiement

### D√©veloppement Local

#### 1. D√©marrer l'application

```bash
cd java
mvn spring-boot:run
```

#### 2. Tester le partitionnement

```bash
# Cr√©er des transactions pour le m√™me client (doivent aller sur la m√™me partition)
curl -X POST http://localhost:8080/api/v1/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 100.00,
    "currency": "EUR",
    "type": "VIREMENT",
    "description": "Premi√®re transaction",
    "customerId": "CUST-001"
  }'

curl -X POST http://localhost:8080/api/v1/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 200.00,
    "currency": "EUR",
    "type": "VIREMENT",
    "description": "Deuxi√®me transaction",
    "customerId": "CUST-001"
  }'

# Cr√©er une transaction sans cl√© (partition round-robin)
curl -X POST http://localhost:8080/api/v1/transactions/without-key \
  -H "Content-Type: application/json" \
  -d '{
    "fromAccount": "FR7630001000222222222",
    "toAccount": "FR7630001000333333333",
    "amount": 300.00,
    "currency": "EUR",
    "type": "PAIEMENT",
    "description": "Transaction sans cl√©",
    "customerId": "CUST-002"
  }'

# Analyser la distribution des partitions
curl "http://localhost:8080/api/v1/partitions/analyze?customerIds=CUST-001,CUST-002,CUST-003"
```

### OpenShift Sandbox ‚Äî Option A : Build S2I Binaire

> **üéØ Objectif** : Ce d√©ploiement valide les concepts de **partitionnement par cl√©** dans un environnement cloud :
> - **Partitionnement d√©terministe** avec cl√© = customerId
> - **Garantie d'ordre** pour les transactions d'un m√™me client
> - **Distribution √©quilibr√©e** des messages sur les partitions
> - **Comparaison** avec le partitionnement round-robin

#### 1. Build et D√©ploiement

```bash
cd module-02-producer/lab-1.2b-producer-keyed/java

# Cr√©er le BuildConfig (avec image stream explicite)
oc new-build --image-stream="openshift/java:openjdk-17-ubi8" --binary=true --name=ebanking-producer-keyed-java

# Build depuis le source local
oc start-build ebanking-producer-keyed-java --from-dir=. --follow

# D√©ployer
oc new-app ebanking-producer-keyed-java
```

#### 2. Configurer les variables d'environnement

```bash
oc set env deployment/ebanking-producer-keyed-java \
  SERVER_PORT=8080 \
  KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092 \
  KAFKA_TOPIC=banking.transactions
```

#### 3. Cr√©er la route Edge

```bash
oc create route edge ebanking-producer-keyed-java-secure \
  --service=ebanking-producer-keyed-java --port=8080-tcp
```

#### 4. V√©rifier le d√©ploiement

```bash
# Obtenir l'URL publique
URL=$(oc get route ebanking-producer-keyed-java-secure -o jsonpath='{.spec.host}')

# Health check
curl -k "https://$URL/api/v1/health"

# Test de partitionnement
curl -k -X POST "https://$URL/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 100.00,
    "currency": "EUR",
    "type": "VIREMENT",
    "description": "Test partitionnement",
    "customerId": "CUST-001"
  }'
```

#### 5. ‚úÖ Crit√®res de succ√®s

```bash
# Pod en cours d'ex√©cution ?
oc get pod -l deployment=ebanking-producer-keyed-java
# Attendu : STATUS=Running, READY=1/1

# API accessible ?
curl -k -s "https://$URL/api/v1/health"
# Attendu : {"status":"UP",...}
```

#### 6. Script automatis√©

```bash
# Bash
./scripts/bash/deploy-and-test-1.2b-java.sh

# PowerShell
.\scripts\powershell\deploy-and-test-1.2b-java.ps1
```

---

## üß™ Tests

### Sc√©narios de test

```bash
URL=$(oc get route ebanking-producer-keyed-java-secure -o jsonpath='{.spec.host}')

# 1. Health check
curl -k -s "https://$URL/api/v1/health"

# 2. Transactions du m√™me client (m√™me partition)
curl -k -X POST "https://$URL/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":100.00,"currency":"EUR","type":"VIREMENT","description":"Tx1","customerId":"CUST-001"}'

curl -k -X POST "https://$URL/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":200.00,"currency":"EUR","type":"VIREMENT","description":"Tx2","customerId":"CUST-001"}'

curl -k -X POST "https://$URL/api/v1/transactions" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":300.00,"currency":"EUR","type":"VIREMENT","description":"Tx3","customerId":"CUST-001"}'

# 3. Transaction sans cl√© (partition round-robin)
curl -k -X POST "https://$URL/api/v1/transactions/without-key" \
  -H "Content-Type: application/json" \
  -d '{"fromAccount":"FR7630001000222222222","toAccount":"FR7630001000333333333","amount":150.00,"currency":"EUR","type":"PAIEMENT","description":"Tx sans cl√©","customerId":"CUST-002"}'

# 4. Analyse de distribution
curl -k "https://$URL/api/v1/partitions/analyze?customerIds=CUST-001,CUST-002,CUST-003,CUST-004,CUST-005"

# 5. Batch avec diff√©rentes cl√©s
curl -k -X POST "https://$URL/api/v1/transactions/batch" \
  -H "Content-Type: application/json" \
  -d '[
    {"fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":100.00,"currency":"EUR","type":"VIREMENT","description":"Batch 1","customerId":"CUST-001"},
    {"fromAccount":"FR7630001000222222222","toAccount":"FR7630001000333333333","amount":200.00,"currency":"EUR","type":"VIREMENT","description":"Batch 2","customerId":"CUST-002"},
    {"fromAccount":"FR7630001000444444444","toAccount":"FR7630001000555555555","amount":300.00,"currency":"EUR","type":"PAIEMENT","description":"Batch 3","customerId":"CUST-003"}
  ]'
```

### V√©rification dans Kafka

```bash
# V√©rifier les messages et leurs partitions
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --from-beginning \
  --max-messages 10 \
  --property print.partition \
  --property print.key
```

---

## üìã Endpoints API

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| `POST` | `/api/v1/transactions` | Cr√©er une transaction (avec cl√©) |
| `POST` | `/api/v1/transactions/without-key` | Cr√©er une transaction (sans cl√©) |
| `POST` | `/api/v1/transactions/batch` | Cr√©er plusieurs transactions |
| `GET` | `/api/v1/partitions/analyze` | Analyser la distribution des partitions |
| `GET` | `/api/v1/health` | Health check |

---

## üéØ Concepts Cl√©s Expliqu√©s

### Partitionnement avec Cl√©

**Formule** :
```
partition = murmur2_hash(key) % nombre_partitions
```

**Propri√©t√©s** :
- **D√©terministe** : m√™me cl√© = m√™me partition
- **Ordre garanti** : pour les messages d'une m√™me cl√©
- **Distribution** : d√©pend de la distribution des cl√©s

### Sans Cl√© (Round-Robin)

- **Sticky Partitioner** (Kafka 2.4+) : minimise les changements de partition
- **Distribution** : plus √©quilibr√©e mais pas d'ordre garanti

### Impact sur l'Ordre

| Sc√©nario | Cl√© | Ordre Garanti | Partition |
|----------|-----|----------------|----------|
| M√™me client | ‚úÖ | ‚úÖ | Fixe |
| Clients diff√©rents | ‚ùå | ‚ùå | Variable |
| Sans cl√© | ‚ùå | ‚ùå | Round-robin |

---

## üîß D√©pannage

### Probl√®mes courants

1. **Hot Partitions**
   - V√©rifier la distribution des cl√©s avec `/partitions/analyze`
   - Consid√©rer des cl√©s plus distribu√©es si n√©cessaire

2. **Messages d√©sordonn√©s**
   - V√©rifiez que vous utilisez la m√™me cl√©
   - Le partitionnement peut changer si le nombre de partitions change

3. **Performance**
   - Les cl√©s longues peuvent impacter la performance de hash
   - Utilisez des cl√©s courtes et stables

---

## ‚úÖ Validation du Lab

√Ä la fin de ce lab, vous devez √™tre capable de :

- [ ] Impl√©menter un producer avec cl√©
- [ ] Comprendre comment la cl√© d√©termine la partition
- [] Garantir l'ordre des messages par cl√©
- [] Comparer avec et sans cl√©
- [] Analyser la distribution des partitions
- [] D√©ployer sur OpenShift avec S2I
- [] V√©rifier le partitionnement dans Kafka

---

## üìö Ressources

- [Kafka Partitioning](https://kafka.apache.org/documentation/#design_partitioning)
- [Producer Configuration](https://kafka.apache.org/documentation/#producerconfigs)
- [Spring Kafka with Keys](https://spring.io/projects/spring-kafka/reference/html/#kafkatemplate-operations)
