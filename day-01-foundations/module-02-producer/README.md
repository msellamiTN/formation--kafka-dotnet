# Module 02 - Premier Producer C# - Formation Auto-rythmÃ©e

## DurÃ©e estimÃ©e

â±ï¸ **2 heures**

## Objectifs pÃ©dagogiques

Ã€ la fin de ce module, vous serez capable de :

1. âœ… DÃ©velopper un Producer .NET minimaliste
2. âœ… Comprendre la configuration de base et les trade-offs
3. âœ… ImplÃ©menter le partitionnement par clÃ©
4. âœ… GÃ©rer les erreurs et confirmations de livraison
5. âœ… Optimiser les performances (batching, compression)

---

## ğŸ§­ Parcours d'Apprentissage

```mermaid
flowchart LR
    A["ğŸ“˜ LAB 1.2A\nProducer Basique\n30 min"] --> B["ğŸ“— LAB 1.2B\nPartitionnement\n45 min"]
    B --> C["ğŸ“™ LAB 1.2C\nGestion d'Erreurs\n45 min"]
    
    style A fill:#bbdefb,stroke:#1976d2
    style B fill:#c8e6c9,stroke:#388e3c
    style C fill:#fff9c4,stroke:#fbc02d
```

**Progression** : Basique â†’ IntermÃ©diaire â†’ AvancÃ©

---

## ğŸ“– Structure du Module

Ce module contient 3 labs progressifs :

### LAB 1.2A : Producer Synchrone Basique
**DurÃ©e** : 30 minutes  
**Objectif** : CrÃ©er un producer simple qui envoie des messages string Ã  Kafka avec gestion d'erreurs de base.

ğŸ“ [`lab-1.2a-producer-basic/`](./lab-1.2a-producer-basic/)
 
**Java (Spring Boot)** : `lab-1.2a-producer-basic/java/`

**Ce que vous allez apprendre** :
- Configuration minimale d'un Producer
- Envoi de messages avec `ProduceAsync()`
- Gestion des `DeliveryResult`
- Error handlers et log handlers
- Importance du `Flush()` avant fermeture

---

### LAB 1.2B : Producer avec ClÃ© (Partitionnement DÃ©terministe)
**DurÃ©e** : 45 minutes  
**Objectif** : Comprendre comment la clÃ© dÃ©termine la partition et garantit l'ordre des messages.

ğŸ“ [`lab-1.2b-producer-keyed/`](./lab-1.2b-producer-keyed/)
 
**Java (Spring Boot)** : `lab-1.2b-producer-keyed/java/`

**Ce que vous allez apprendre** :
- DiffÃ©rence entre messages avec et sans clÃ©
- Partitionnement hash-based (Murmur2)
- Garantie d'ordre par clÃ©
- Distribution des messages sur les partitions
- Ã‰viter les hot partitions

---

### LAB 1.2C : Producer avec Gestion d'Erreurs et DLQ
**DurÃ©e** : 45 minutes  
**Objectif** : ImplÃ©menter un pattern production-ready avec retry et Dead Letter Queue.

ğŸ“ [`lab-1.2c-producer-error-handling/`](./lab-1.2c-producer-error-handling/)
 
**Java (Spring Boot)** : `lab-1.2c-producer-error-handling/java/`

**Ce que vous allez apprendre** :
- Classification des erreurs (retriable vs permanent)
- Pattern Dead Letter Queue (DLQ)
- Retry avec exponential backoff
- MÃ©tadonnÃ©es d'erreur dans headers
- Logging et monitoring des Ã©checs

---

## ğŸš€ PrÃ©requis

### Environnement

Vous devez avoir un cluster Kafka en fonctionnement. Deux options :

#### Option A : Docker (DÃ©veloppement local)
```bash
cd ../module-01-cluster
./scripts/up.sh
```

VÃ©rifiez que Kafka est accessible :
```bash
docker ps
# Vous devez voir : kafka (healthy) et kafka-ui (healthy)
```

#### Option B : OKD/K3s/OpenShift (Production-like)

> â„¹ï¸ Sur OpenShift/OKD, remplacez `kubectl` par `oc`.
```bash
kubectl get kafka -n kafka
# Attendu : bhf-kafka avec status Ready
```

#### Option C : OpenShift Developer Sandbox

Pour ce lab, nous devons exposer les brokers localement via `port-forward`.

1. **Ouvrez 3 terminaux sÃ©parÃ©s** et lancez ces commandes pour crÃ©er les tunnels :

   **Terminal A (Broker 0)** :
   ```bash
   oc port-forward kafka-0 9094:9094
   ```

   **Terminal B (Broker 1)** :
   ```bash
   oc port-forward kafka-1 9095:9094
   ```

   **Terminal C (Broker 2)** :
   ```bash
   oc port-forward kafka-2 9096:9094
   ```

2. **Configuration** :
   Utilisez `localhost:9094` comme `BootstrapServers`.

### Outils de dÃ©veloppement

**Visual Studio Code** :
- Extension C# Dev Kit
- Extension Docker (optionnel)

**Visual Studio 2022** :
- Workload ".NET Desktop Development"
- Workload "ASP.NET and web development"

### SDK .NET
```bash
dotnet --version
# Attendu : 8.0.x ou supÃ©rieur
```

### Java (Spring Boot)

#### JDK
```bash
java -version
# Attendu : 17.x
```

#### Maven
```bash
mvn -version
```

---

## â˜• Java Track (Spring Boot) â€” Run & Test

Chaque lab contient une version Java Spring Boot dans le dossier `java/`.

### Variables d'environnement

Les applications Java utilisent les variables suivantes (avec valeurs par dÃ©faut) :

- **`KAFKA_BOOTSTRAP_SERVERS`** (default: `localhost:9092`)
- **`KAFKA_TOPIC`** (default: `banking.transactions`)
- **`KAFKA_DLQ_TOPIC`** (Lab 1.2C only, default: `banking.transactions.dlq`)
- **`SERVER_PORT`** (default: `8080`)

Lab 1.2C (rÃ©silience) ajoute :

- **`MAX_RETRIES`** (default: `3`)
- **`RETRY_BACKOFF_MS`** (default: `1000`)
- **`CIRCUIT_BREAKER_THRESHOLD`** (default: `5`)
- **`CIRCUIT_BREAKER_OPEN_MS`** (default: `60000`)

### DÃ©marrer une application Java

#### Lab 1.2A â€” Producer Basic
```bash
cd lab-1.2a-producer-basic/java
mvn spring-boot:run
```

#### Lab 1.2B â€” Producer Keyed (key = customerId)
```bash
cd lab-1.2b-producer-keyed/java
mvn spring-boot:run
```

#### Lab 1.2C â€” Producer Resilient (retry + DLQ)
```bash
cd lab-1.2c-producer-error-handling/java
mvn spring-boot:run
```

### Endpoints

Labs 1.2A / 1.2B / 1.2C exposent :

- **POST** `http://localhost:8080/api/v1/transactions`
- **POST** `http://localhost:8080/api/v1/transactions/batch`

Lab 1.2C expose aussi :

- **GET** `http://localhost:8080/api/v1/transactions/metrics`

### Health

```bash
curl http://localhost:8080/actuator/health
```

### Docker (Java)

Each Java lab includes a `java/Dockerfile`.

#### Build

```bash
# Lab 1.2A
cd lab-1.2a-producer-basic/java
docker build -t ebanking-producer-basic-java:latest .

# Lab 1.2B
cd ../../lab-1.2b-producer-keyed/java
docker build -t ebanking-producer-keyed-java:latest .

# Lab 1.2C
cd ../../lab-1.2c-producer-error-handling/java
docker build -t ebanking-producer-resilient-java:latest .
```

#### Run

```bash
docker run --rm -p 8080:8080 \
  -e KAFKA_BOOTSTRAP_SERVERS=localhost:9092 \
  -e KAFKA_TOPIC=banking.transactions \
  ebanking-producer-basic-java:latest
```

For Lab 1.2C, add DLQ + retry env vars:

```bash
docker run --rm -p 8080:8080 \
  -e KAFKA_BOOTSTRAP_SERVERS=localhost:9092 \
  -e KAFKA_TOPIC=banking.transactions \
  -e KAFKA_DLQ_TOPIC=banking.transactions.dlq \
  -e MAX_RETRIES=3 \
  -e RETRY_BACKOFF_MS=1000 \
  ebanking-producer-resilient-java:latest
```

### OpenShift (Java)

Each Java lab includes an OpenShift manifest:

- Lab 1.2A: `lab-1.2a-producer-basic/java/deployment/openshift-deployment.yaml`
- Lab 1.2B: `lab-1.2b-producer-keyed/java/deployment/openshift-deployment.yaml`
- Lab 1.2C: `lab-1.2c-producer-error-handling/java/deployment/openshift-deployment.yaml`

#### Deploy (example: Lab 1.2A)

```bash
cd lab-1.2a-producer-basic/java

# Build image in your registry/workflow (Docker/CI), then update image field if needed.

oc apply -f deployment/openshift-deployment.yaml

# Get route
oc get route ebanking-producer-basic-java -o jsonpath='{.spec.host}'
```

#### Health check

```bash
HOST=$(oc get route ebanking-producer-basic-java -o jsonpath='{.spec.host}')
curl -k "https://$HOST/actuator/health"
```

#### Option A (Recommended): OpenShift S2I Binary Build (no external registry)

This option builds the Java application **inside OpenShift** using the S2I builder image and a **binary build**.

**Builder image** used in this training:

- `java:17`

##### Lab 1.2A â€” Basic Producer (Java)

```bash
APP=ebanking-producer-basic-java
ROUTE=${APP}-secure

cd lab-1.2a-producer-basic/java

# 1) Create BuildConfig (with explicit image stream)
oc new-build --image-stream="openshift/java:openjdk-17-ubi8" --binary=true --name=$APP

# 2) Start binary build from local folder
oc start-build $APP --from-dir=. --follow

# 3) Create app + configure env vars
oc new-app $APP
oc set env deployment/$APP \
  SERVER_PORT=8080 \
  KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092 \
  KAFKA_TOPIC=banking.transactions

# 4) Expose via edge route
oc create route edge $ROUTE --service=$APP --port=8080-tcp

# 5) Wait + test
oc wait --for=condition=available deployment/$APP --timeout=300s
HOST=$(oc get route $ROUTE -o jsonpath='{.spec.host}')
curl -k "https://$HOST/actuator/health"
```

##### Lab 1.2B â€” Keyed Producer (Java)

Same process, different `APP` name and folder:

```bash
APP=ebanking-producer-keyed-java
ROUTE=${APP}-secure

cd lab-1.2b-producer-keyed/java
oc new-build --image-stream="openshift/java:openjdk-17-ubi8" --binary=true --name=$APP
oc start-build $APP --from-dir=. --follow
oc new-app $APP
oc set env deployment/$APP SERVER_PORT=8080 KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092 KAFKA_TOPIC=banking.transactions
oc create route edge $ROUTE --service=$APP --port=8080-tcp
```

##### Lab 1.2C â€” Resilient Producer (Java)

```bash
APP=ebanking-producer-resilient-java
ROUTE=${APP}-secure

cd lab-1.2c-producer-error-handling/java
oc new-build --image-stream="openshift/java:openjdk-17-ubi8" --binary=true --name=$APP
oc start-build $APP --from-dir=. --follow
oc new-app $APP
oc set env deployment/$APP \
  SERVER_PORT=8080 \
  KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092 \
  KAFKA_TOPIC=banking.transactions \
  KAFKA_DLQ_TOPIC=banking.transactions.dlq \
  MAX_RETRIES=3 \
  RETRY_BACKOFF_MS=1000 \
  CIRCUIT_BREAKER_THRESHOLD=5 \
  CIRCUIT_BREAKER_OPEN_MS=60000
oc create route edge $ROUTE --service=$APP --port=8080-tcp
```

> **âš ï¸ Important Note** : All Java applications require the Spring Boot Maven plugin to be configured with the `repackage` goal to create an executable JAR. See individual lab READMEs for the exact `pom.xml` configuration.

#### Automated scripts (Option A)

The repository includes scripts that automate the steps above:

- Bash:
  - `day-01-foundations/scripts/bash/deploy-and-test-1.2a-java.sh`
  - `day-01-foundations/scripts/bash/deploy-and-test-1.2b-java.sh`
  - `day-01-foundations/scripts/bash/deploy-and-test-1.2c-java.sh`
- PowerShell:
  - `day-01-foundations/scripts/powershell/deploy-and-test-1.2a-java.ps1`
  - `day-01-foundations/scripts/powershell/deploy-and-test-1.2b-java.ps1`
  - `day-01-foundations/scripts/powershell/deploy-and-test-1.2c-java.ps1`

---

## Ordre de RÃ©alisation

Suivez les labs dans l'ordre :

1. **LAB 1.2A** â†’ Bases du Producer
2. **LAB 1.2B** â†’ Partitionnement par clÃ©
3. **LAB 1.2C** â†’ Gestion d'erreurs production-ready

Chaque lab contient :
- âœ… Un README dÃ©taillÃ© avec instructions pas Ã  pas
- âœ… Le code complet commentÃ©
- âœ… Les fichiers de configuration
- âœ… Les commandes de test et validation
- âœ… Des exercices pratiques

---

## ğŸ¯ Concepts ThÃ©oriques ClÃ©s

### Anatomie d'un Message Kafka

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MESSAGE KAFKA                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Key (optional)    : byte[]              â”‚  â†’ DÃ©termine la partition
â”‚ Value             : byte[]              â”‚  â†’ Contenu du message
â”‚ Headers (optional): Map<string, byte[]> â”‚  â†’ MÃ©tadonnÃ©es
â”‚ Timestamp         : long                â”‚  â†’ Horodatage
â”‚ Partition         : int                 â”‚  â†’ CalculÃ© par Kafka
â”‚ Offset            : long                â”‚  â†’ AssignÃ© aprÃ¨s Ã©criture
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Partitionnement

**Sans clÃ©** : Round-robin (sticky partitioner depuis Kafka 2.4+)
```csharp
await producer.ProduceAsync("orders", new Message<Null, string>
{
    Value = "{...}"  // Partition choisie automatiquement
});
```

**Avec clÃ©** : Hash-based (dÃ©terministe)
```csharp
await producer.ProduceAsync("orders", new Message<string, string>
{
    Key = "customer-123",  // Ira TOUJOURS sur la mÃªme partition
    Value = "{...}"
});
```

**Formule** :
```
partition = murmur2_hash(key) % nombre_partitions
```

### Configuration Producer : Trade-offs

| ParamÃ¨tre | Latence faible | Throughput Ã©levÃ© |
|-----------|----------------|------------------|
| `LingerMs` | 0 | 10-100 |
| `BatchSize` | 16384 (16 KB) | 100000 (100 KB) |
| `CompressionType` | None | Lz4 |
| `Acks` | Leader (1) | All (-1) |

### Garanties de Livraison

| Acks | Garantie | Performance | Cas d'usage |
|------|----------|-------------|-------------|
| `None (0)` | Aucune | TrÃ¨s rapide | MÃ©triques, logs non-critiques |
| `Leader (1)` | Leader uniquement | Rapide | Logs applicatifs |
| `All (-1)` | Tous les ISR | Plus lent | Transactions, commandes |

---

## ğŸ› ï¸ Commandes Utiles

### CrÃ©er un topic pour les labs
```bash
# Docker
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create --if-not-exists \
  --topic banking.transactions \
  --partitions 6 \
  --replication-factor 1

# OKD/K3s
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-topics.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --create --if-not-exists --topic banking.transactions --partitions 6 --replication-factor 3

# OpenShift Sandbox (via pod existant)
oc exec kafka-0 -- /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --create \
  --if-not-exists \
  --topic banking.transactions \
  --partitions 6 \
  --replication-factor 3
```

### Lister les messages produits
```bash
# Docker
docker exec kafka /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --from-beginning \
  --max-messages 10

# OKD/K3s
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-console-consumer.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --topic banking.transactions --from-beginning --max-messages 10

# OpenShift Sandbox (via pod existant)
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server localhost:9092 \
  --topic banking.transactions \
  --from-beginning \
  --max-messages 10
```

### Voir les dÃ©tails d'un topic
```bash
# Docker
docker exec kafka /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --describe \
  --topic banking.transactions

# OKD/K3s
kubectl run kafka-cli -it --rm --image=quay.io/strimzi/kafka:latest-kafka-4.0.0 \
  --restart=Never -n kafka -- \
  bin/kafka-topics.sh --bootstrap-server bhf-kafka-kafka-bootstrap:9092 \
  --describe --topic banking.transactions

# OpenShift Sandbox (via pod existant)
oc exec kafka-0 -- /opt/kafka/bin/kafka-topics.sh \
  --bootstrap-server localhost:9092 \
  --describe \
  --topic banking.transactions
```

---

## ğŸ’¡ Tips & Best Practices

### TIP #1 : Singleton Producer en ASP.NET Core
```csharp
// âœ… BON : Singleton (rÃ©utilisÃ©)
builder.Services.AddSingleton<IProducer<string, string>>(sp => ...);

// âŒ MAUVAIS : Scoped ou Transient (nouvelle connexion TCP)
builder.Services.AddScoped<IProducer<string, string>>(sp => ...);
```

### TIP #2 : Flush() avant fermeture
```csharp
// TOUJOURS flush avant Dispose
producer.Flush(TimeSpan.FromSeconds(10));
producer.Dispose();
```

### TIP #3 : Headers pour correlation IDs
```csharp
Headers = new Headers
{
    { "correlation-id", Encoding.UTF8.GetBytes(Guid.NewGuid().ToString()) },
    { "trace-id", Encoding.UTF8.GetBytes(Activity.Current?.TraceId.ToString() ?? "") }
}
```

### TIP #4 : Utilisez toujours une clÃ© si besoin d'ordre
Si vous avez besoin que les Ã©vÃ©nements d'une mÃªme entitÃ© (client, commande, compte) arrivent dans l'ordre, utilisez une clÃ© :
```csharp
Key = customerId  // Tous les events du mÃªme client sur la mÃªme partition
```

---

## ğŸ¯ Validation du Module

Ã€ la fin de ce module, vous devez Ãªtre capable de :

- [ ] CrÃ©er un Producer .NET avec configuration minimale
- [ ] Envoyer des messages avec et sans clÃ©
- [ ] Comprendre comment les messages sont partitionnÃ©s
- [ ] GÃ©rer les erreurs de production (retriable vs permanent)
- [ ] ImplÃ©menter un pattern Dead Letter Queue
- [ ] Configurer le Producer pour latence vs throughput
- [ ] Utiliser les headers pour mÃ©tadonnÃ©es
- [ ] Monitorer les confirmations de livraison

---

## ğŸ“– Ressources ComplÃ©mentaires

- [Documentation Confluent.Kafka](https://docs.confluent.io/kafka-clients/dotnet/current/overview.html)
- [Kafka Producer Configuration](https://kafka.apache.org/documentation/#producerconfigs)
- [Best Practices for Kafka Producers](https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html)

---

## ğŸš€ Commencer le Module

Rendez-vous dans le premier lab :

ğŸ‘‰ **[LAB 1.2A : Producer Synchrone Basique](./lab-1.2a-producer-basic/README.md)**
