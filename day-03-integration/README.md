# üìÖ Day 03 ‚Äî Int√©gration, Tests & Observabilit√©

> **Jeudi 12 f√©vrier 2026** | 6h (9h‚Äì12h / 13h30‚Äì16h30) | **Niveau** : Avanc√© ‚Üí Production

---

## üéØ Objectifs p√©dagogiques

√Ä la fin de cette journ√©e, vous serez capable de :

| # | Objectif | Bloc |
| --- | -------- | ---- |
| 1 | Construire un **traitement temps r√©el** avec Kafka Streams (KStream, KTable, agr√©gations) | 3.1 |
| 2 | D√©ployer des **connecteurs Source/Sink** et les g√©rer via REST API | 3.2 |
| 3 | √âcrire des **tests unitaires** avec MockProducer / MockConsumer | 3.3 |
| 4 | Impl√©menter des **tests d'int√©gration** avec EmbeddedKafka | 3.3 |
| 5 | Collecter les **m√©triques JMX** des brokers Kafka | 3.4 |
| 6 | Surveiller le **consumer lag** et la sant√© du cluster via REST | 3.4 |
| 7 | Exposer des **m√©triques Prometheus** depuis Spring Boot / ASP.NET | 3.4 |

> **Ratio th√©orie/pratique** : 30% / 70% ‚Äî Chaque bloc commence par 15-20 min de th√©orie puis encha√Æne sur un lab hands-on.

---

## üìã Pr√©requis

- ‚úÖ **Day 01 & Day 02 compl√©t√©s** (Labs 1.2a‚Äì2.3a)
- ‚úÖ Infrastructure Kafka fonctionnelle (Docker ou OpenShift Sandbox)
- ‚úÖ Topic `banking.transactions` existant (6 partitions)
- ‚úÖ **.NET 8 SDK + Confluent.Kafka 2.3.0+** (piste .NET)
- ‚úÖ **Java 17 + Spring Boot 3.2+** (piste Java)

---

## üèóÔ∏è Dual Track : .NET vs Java

Day 03 propose **deux pistes parall√®les** pour couvrir les deux √©cosyst√®mes principaux de Kafka :

| Piste | Technologie | Public Cible | Avantages |
| ----- | ----------- | ------------ | --------- |
| **.NET** | C# + Confluent.Kafka | √âquipes Microsoft | Performance native, int√©gration √©cosyst√®me .NET |
| **Java** | Spring Boot + Spring Kafka | √âquipes Java/Spring | √âcosyst√®me mature, Kafka Streams natif |

> **üìã Choix de piste** : Les deux pistes couvrent les m√™mes concepts. Choisissez selon votre expertise ou explorez les deux pour comparer !

---

## üóìÔ∏è Planning de la journ√©e

| Cr√©neau | Bloc | Dur√©e | Contenu |
| ------- | ---- | ----- | ------- |
| 09h00‚Äì09h30 | Recap | 30 min | Quiz Day 02 + correction, questions ouvertes |
| 09h30‚Äì11h00 | **3.1** | 1h30 | Kafka Streams : KStream, KTable, agr√©gations, fen√™trage |
| 11h00‚Äì11h15 | | 15 min | ‚òï Pause |
| 11h15‚Äì12h00 | **3.2** | 45 min | Kafka Connect : Source/Sink, REST API, d√©mo |
| 12h00‚Äì13h30 | | 1h30 | üçΩÔ∏è D√©jeuner |
| 13h30‚Äì14h30 | **3.3** | 1h | Tests Kafka : MockProducer/Consumer, EmbeddedKafka |
| 14h30‚Äì14h45 | | 15 min | ‚òï Pause |
| 14h45‚Äì16h00 | **3.4** | 1h15 | Observabilit√© : JMX, Prometheus, Grafana, Consumer Lag |
| 16h00‚Äì16h30 | Recap | 30 min | Bilan formation 3 jours, Q&A, prochaines √©tapes |

---

## üìö Bloc 3.1 ‚Äî Kafka Streams & ksqlDB (1h30)

> **Th√©orie** : 20 min | **Lab** : 1h10

### Architecture Globale

```mermaid
flowchart TB
    subgraph Sources["üì• Sources de Transactions"]
        Web["üåê Web Banking"]
        Mobile["üì± Mobile App"]
        ATM["üèß ATM"]
        API["üöÄ REST API"]
    end

    subgraph KafkaCluster["ÔøΩ Kafka Cluster"]
        T["üìã transactions"]
        VT["üìã verified_transactions"]
        FA["üìã fraud_alerts"]
        ABS["üìã account_balances"]
        HS["üìã hourly_stats"]
        SE["üìã sales-events"]
    end

    subgraph Processing["‚öôÔ∏è Stream Processing"]
        subgraph Java["‚òï Kafka Streams (Java)"]
            KS["Kafka Streams API"]
            SS["State Stores"]
            TO["Topology"]
        end
        
        subgraph DotNet["üöÄ .NET Streams API"]
            SP["SalesProcessor"]
            TP["TransactionProcessor"]
            BS["BalanceService"]
        end
        
        subgraph KsqlDB["‚öôÔ∏è ksqlDB Engine"]
            S1["STREAM transactions"]
            S2["STREAM verified_transactions"]
            T1["TABLE account_balances"]
            T2["TABLE hourly_stats"]
        end
    end

    subgraph Outputs["üìä Downstream Services"]
        Fraud["üîç Anti-Fraude"]
        Balance["üí∞ Balance Service"]
        Alert["‚ö†Ô∏è Alert Service"]
        Dashboard["üìà Real-time Dashboard"]
        Audit["ÔøΩ Audit Service"]
    end

    Web --> API
    Mobile --> API
    ATM --> API
    API --> T
    API --> SE
    
    T --> S1
    S1 --> S2
    S1 --> T1
    S2 --> FA
    S1 --> T2
    
    SE --> KS
    KS --> SS
    KS --> TO
    
    S2 --> Fraud
    FA --> Alert
    T1 --> Balance
    T2 --> Dashboard
    SS --> Audit

    style Sources fill:#e3f2fd,stroke:#1976d2
    style KafkaCluster fill:#fff3e0,stroke:#f57c00
    style Processing fill:#f3e5f5,stroke:#7b1fa2
    style Outputs fill:#e8f5e8,stroke:#388e3c
```

### Cycle de Vie d'une Transaction Stream Processing

```mermaid
sequenceDiagram
    actor Client as üßë‚Äçüíº Client Bancaire
    participant App as üì± App Mobile/Web
    participant API as üöÄ E-Banking API
    participant Kafka as üî• Kafka Broker
    participant Streams as ‚öôÔ∏è Stream Processing
    participant Dashboard as üìä Dashboard
    participant Alert as ‚ö†Ô∏è Alert Service

    Client->>App: Initier virement 1500‚Ç¨
    App->>API: POST /api/transactions
    API->>API: Valider IBAN, montant, devise
    API->>Kafka: Publier transaction (cl√©: customerId)
    Kafka-->>API: ACK (partition 2, offset 1234)
    API-->>App: 201 Created + m√©tadonn√©es Kafka
    App-->>Client: "Virement en cours de traitement"

    Note over Kafka,Dashboard: Stream Processing temps r√©el
    
    Kafka->>Streams: Consommer transaction
    Streams->>Streams: Appliquer r√®gles de fraude
    Streams->>Streams: Mettre √† jour solde compte
    Streams->>Kafka: √âmettre verified_transaction
    Streams->>Kafka: √âmettre fraud_alert (si n√©cessaire)
    
    par Fraude d√©tect√©e
        Kafka->>Alert: Consommer alerte fraude
        Alert->>Dashboard: Afficher alerte en temps r√©el
        Alert-->>Client: üìß "Transaction suspecte d√©tect√©e"
    and Transaction valide
        Kafka->>Dashboard: Consommer solde mis √† jour
        Dashboard-->>Client: üìß "Nouveau solde: 8500‚Ç¨"
    end
```

### Concepts Cl√©s Expliqu√©s

#### 1. Kafka Streams (Java)

```java
// Topologie de traitement temps r√©el
StreamsBuilder builder = new StreamsBuilder();

KStream<String, Transaction> transactions = builder.stream("transactions");

// Filtrer les transactions > 10000‚Ç¨ (alerte fraude)
KStream<String, Transaction> highValue = transactions
    .filter((key, tx) -> tx.getAmount().compareTo(new BigDecimal("10000")) > 0);

// Agr√©ger par produit (ventes)
KTable<String, BigDecimal> productSales = builder.stream("sales-events")
    .groupBy((key, sale) -> sale.getProductId(), Grouped.with(Serdes.String(), saleSerde))
    .aggregate(
        () -> BigDecimal.ZERO,
        (key, sale, agg) -> agg.add(sale.getUnitPrice().multiply(new BigDecimal(sale.getQuantity()))),
        Materialized.as("sales-by-product")
    );

// Fen√™trage par minute
KStream<Windowed<String>, BigDecimal> minuteStats = transactions
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofMinutes(1)))
    .aggregate(
        () -> BigDecimal.ZERO,
        (key, tx, agg) -> agg.add(tx.getAmount()),
        Materialized.as("minute-stats")
    );
```

#### 2. .NET Streams API

```csharp
// Service de traitement des ventes
public class SalesStreamProcessorService : BackgroundService
{
    private readonly IProducer<string, string> _producer;
    private readonly ILogger<SalesStreamProcessorService> _logger;

    protected override async Task ExecuteAsync(CancellationToken stoppingToken)
    {
        // Consommer les √©v√©nements de vente
        var config = new ConsumerConfig
        {
            BootstrapServers = "kafka-svc:9092",
            GroupId = "sales-processor",
            AutoOffsetReset = AutoOffsetReset.Earliest
        };

        using var consumer = new ConsumerBuilder<string, string>(config).Build();
        consumer.Subscribe(new[] { "sales-events" });

        while (!stoppingToken.IsCancellationRequested)
        {
            var result = consumer.Consume(stoppingToken);
            var sale = JsonSerializer.Deserialize<SaleEvent>(result.Message.Value);
            
            // Traiter et agr√©ger
            await ProcessSaleAsync(sale);
            
            consumer.Commit(result);
        }
    }

    private async Task ProcessSaleAsync(SaleEvent sale)
    {
        // Calculer les statistiques
        var stats = new SalesStats
        {
            ProductId = sale.ProductId,
            TotalAmount = sale.UnitPrice * sale.Quantity,
            Quantity = sale.Quantity,
            Timestamp = DateTime.UtcNow
        };

        // Publier les stats agr√©g√©es
        var message = new Message<string, string>
        {
            Key = sale.ProductId,
            Value = JsonSerializer.Serialize(stats)
        };

        await _producer.ProduceAsync("sales-stats", message);
    }
}
```

#### 3. ksqlDB Stream Processing

```sql
-- Cr√©er les streams
CREATE STREAM transactions (
    transaction_id VARCHAR PRIMARY KEY,
    from_account VARCHAR,
    to_account VARCHAR,
    amount DECIMAL(10,2),
    currency VARCHAR,
    transaction_type VARCHAR,
    customer_id VARCHAR,
    timestamp VARCHAR
) WITH (
    KAFKA_TOPIC = 'transactions',
    VALUE_FORMAT = 'JSON'
);

-- Stream des transactions v√©rifi√©es
CREATE STREAM verified_transactions WITH (
    KAFKA_TOPIC = 'verified_transactions',
    VALUE_FORMAT = 'JSON'
) AS SELECT 
    *
FROM transactions
WHERE amount < 10000  -- Filtrer les montants raisonnables
  AND LENGTH(from_account) = 27  -- Valider format IBAN
  AND LENGTH(to_account) = 27;

-- Table mat√©rialis√©e pour les soldes
CREATE TABLE account_balances (
    account_id VARCHAR PRIMARY KEY,
    balance DECIMAL(12,2),
    last_updated TIMESTAMP
) WITH (
    KAFKA_TOPIC = 'account_balances',
    VALUE_FORMAT = 'JSON',
    KEY = 'account_id'
);

-- Mettre √† jour les soldes en continu
INSERT INTO account_balances
SELECT 
    to_account AS account_id,
    SUM(amount) AS balance,
    LATEST_BY_OFFSET(timestamp) AS last_updated
FROM transactions
GROUP BY to_account;

-- Alerts fraude
CREATE STREAM fraud_alerts WITH (
    KAFKA_TOPIC = 'fraud_alerts',
    VALUE_FORMAT = 'JSON'
) AS SELECT 
    transaction_id,
    from_account,
    to_account,
    amount,
    'HIGH_AMOUNT' AS alert_type,
    timestamp
FROM transactions
WHERE amount > 10000;
```

| Concept | Description | Exemple |
| ------- | ----------- | ------- |
| **KStream** | Flux continu d'√©v√©nements | Transactions bancaires |
| **KTable** | Vue mat√©rialis√©e (changelog) | Soldes par compte |
| **Aggregation** | Regroupement et calcul | Total ventes par produit |
| **Windowing** | Fen√™trage temporel | Statistiques par minute |
| **Join** | Enrichissement de donn√©es | Transaction + d√©tails produit |
| **State Store** | Stockage local queryable | Requ√™tes REST sur l'√©tat |

### Lab 3.1a ‚Äî Kafka Streams Processing

#### üìÇ Piste .NET
> **[lab-3.1a ‚Äî Kafka Streams (.NET)](./module-05-kafka-streams-ksqldb/dotnet/)**

**Objectifs du lab** :

1. Construire une topologie de traitement temps r√©el
2. Impl√©menter des agr√©gations par produit
3. Configurer le fen√™trage temporel (par minute)
4. Exposer les r√©sultats via REST API

#### üìÇ Piste Java
> **[lab-3.1a ‚Äî Kafka Streams (Java)](./module-05-kafka-streams-ksqldb/java/README.md)**

**Objectifs du lab** :

1. Construire une `SalesTopology` avec KStream et KTable
2. Impl√©menter des agr√©gations par produit avec state store
3. Configurer le fen√™trage temporel (par minute)
4. Exposer les state stores via REST API (Interactive Queries)

**Concepts Java** :

```java
// Aggregate sales by product
salesStream
    .groupByKey()
    .aggregate(
        SaleAggregate::new,
        (key, value, aggregate) -> aggregate.add(sale),
        Materialized.as("sales-by-product-store")
    );

// Windowed aggregation per minute
salesStream
    .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMinutes(1)))
    .aggregate(/* ... */);
```

---

## üîå Bloc 3.2 ‚Äî Kafka Connect (45 min)

> **Th√©orie** : 30 min | **D√©mo** : 15 min

### Concepts cl√©s

```mermaid
flowchart LR
    subgraph Sources["üì• Sources"]
        DB[("üóÑÔ∏è SQL Server")]
        FILE["üìÑ CSV/JSON"]
    end

    subgraph Connect["üîå Kafka Connect"]
        SC["Source Connector"]
        SK["Sink Connector"]
    end

    subgraph Kafka["üì¶ Kafka"]
        T["Topics"]
    end

    subgraph Sinks["üì§ Destinations"]
        ES[("üîç Elasticsearch")]
        S3["‚òÅÔ∏è Blob Storage"]
    end

    DB --> SC --> T
    FILE --> SC
    T --> SK --> ES
    T --> SK --> S3
```

| Concept | Description |
| ------- | ----------- |
| **Source Connector** | Lit des donn√©es externes ‚Üí Kafka topics |
| **Sink Connector** | Lit Kafka topics ‚Üí √©crit vers syst√®mes externes |
| **Worker** | Process JVM qui ex√©cute les connecteurs |
| **Task** | Unit√© de parall√©lisme au sein d'un connecteur |
| **Converter** | Transforme les donn√©es (JsonConverter, AvroConverter) |

> üîó **Lab complet Kafka Connect** : voir **[Module 06](./module-06-kafka-connect/README.md)**

---

## üß™ Bloc 3.3 ‚Äî Tests Kafka (1h)

> **Th√©orie** : 15 min | **Lab** : 45 min

### Concepts cl√©s

```mermaid
flowchart TB
    subgraph Pyramid["üî∫ Pyramide de Tests Kafka"]
        E2E["üîù E2E Tests<br/>(Testcontainers + Real Kafka)<br/>10%"]
        INT["üì¶ Tests d'int√©gration<br/>(EmbeddedKafka)<br/>30%"]
        UNIT["‚ö° Tests unitaires<br/>(MockProducer/Consumer)<br/>60%"]
    end

    UNIT --> INT --> E2E

    style E2E fill:#ffcccc
    style INT fill:#ffffcc
    style UNIT fill:#ccffcc
```

| Niveau | Outil | Vitesse | Fid√©lit√© | Isolation |
| ------ | ----- | ------- | -------- | --------- |
| **Unit** | MockProducer/Consumer | ‚ö°‚ö°‚ö° | ‚≠ê | ‚úÖ Totale |
| **Integration** | EmbeddedKafka | ‚ö°‚ö° | ‚≠ê‚≠ê | ‚úÖ Process |
| **E2E** | Testcontainers | ‚ö° | ‚≠ê‚≠ê‚≠ê | ‚úÖ Container |

### Lab 3.3a ‚Äî Tests unitaires & int√©gration

#### üìÇ Piste .NET
> **[lab-3.3a ‚Äî Tests Kafka (.NET)](./module-07-testing/dotnet/)**

**Objectifs du lab** :

1. √âcrire des tests unitaires avec Moq pour le Producer
2. Tester le Consumer avec des mocks
3. Impl√©menter des tests d'int√©gration avec Testcontainers
4. Valider la s√©rialisation/d√©s√©rialisation JSON

#### üìÇ Piste Java
> **[lab-3.3a ‚Äî Tests Kafka (Java)](./module-07-testing/java/README.md)**

**Objectifs du lab** :

1. √âcrire des tests unitaires avec `MockProducer` (5 tests)
2. Tester le Consumer avec `MockConsumer` (4 tests)
3. Valider le routage par cl√©, la s√©rialisation JSON, la gestion d'erreurs
4. (Bonus) Tests d'int√©gration avec EmbeddedKafka

**Concepts Java** :

```java
// MockProducer - test sans broker Kafka
MockProducer<String, String> mockProducer =
    new MockProducer<>(true, new StringSerializer(), new StringSerializer());

service.send(transaction);

assertEquals(1, mockProducer.history().size());
assertEquals("CUST-001", mockProducer.history().get(0).key());

// MockConsumer - test sans broker Kafka
MockConsumer<String, String> mockConsumer =
    new MockConsumer<>(OffsetResetStrategy.EARLIEST);
mockConsumer.addRecord(new ConsumerRecord<>(TOPIC, 0, 0L, key, json));
```

---

## üìä Bloc 3.4 ‚Äî Observabilit√© (1h15)

> **Th√©orie** : 20 min | **Lab** : 55 min

### Concepts cl√©s ‚Äî Les 3 piliers

```mermaid
flowchart TB
    subgraph Observability["üìä Observabilit√© Kafka"]
        subgraph Metrics["üìà M√©triques"]
            JMX["JMX Exporter"]
            PROM["Prometheus"]
            GRAF["Grafana"]
        end

        subgraph Logs["üìù Logs"]
            KL["Kafka Logs"]
            AL["App Logs"]
            ELK["ELK Stack"]
        end

        subgraph Traces["üîó Traces"]
            OT["OpenTelemetry"]
            JAEG["Jaeger"]
            CORR["Correlation IDs"]
        end
    end

    JMX --> PROM --> GRAF
    KL --> ELK
    AL --> ELK
    OT --> JAEG
```

| M√©trique | Description | Seuil d'alerte |
| -------- | ----------- | -------------- |
| **consumer_lag** | Messages non consomm√©s | > 1000 |
| **request_latency_avg** | Latence moyenne | > 100ms |
| **bytes_in_per_sec** | D√©bit entrant | Selon capacit√© |
| **under_replicated_partitions** | Partitions sous-r√©pliqu√©es | > 0 |
| **active_controller_count** | Contr√¥leurs actifs | ‚â† 1 |

### Lab 3.4a ‚Äî Tableau de bord M√©triques

#### üìÇ Piste Java
> **[lab-3.4a ‚Äî Metrics Dashboard (Java)](./module-08-observability/java/README.md)**

**Objectifs du lab** :

1. Interroger la sant√© du cluster Kafka via `AdminClient`
2. Surveiller le **consumer lag** par groupe
3. Lister les topics avec m√©tadonn√©es (partitions, r√©plication)
4. Exposer des m√©triques **Prometheus** via Micrometer

**Concepts Java** :

```java
// AdminClient pour la sant√© du cluster
DescribeClusterResult cluster = adminClient.describeCluster();
Collection<Node> nodes = cluster.nodes().get();
Node controller = cluster.controller().get();

// Consumer lag
Map<TopicPartition, OffsetAndMetadata> offsets =
    adminClient.listConsumerGroupOffsets(groupId)
        .partitionsToOffsetAndMetadata().get();
```

---

## üèóÔ∏è Architecture Day 03

```mermaid
flowchart TB
    subgraph OpenShift["‚òÅÔ∏è OpenShift Sandbox (msellamitn-dev)"]
        subgraph Infra["Infrastructure"]
            K["üì¶ Kafka<br/>kafka-svc:9092"]
        end

        subgraph Bloc31["Bloc 3.1 - Kafka Streams"]
            STREAMS["üî∑ ebanking-streams-java<br/>:8080"]
        end

        subgraph Bloc34["Bloc 3.4 - Observabilit√©"]
            METRICS["üî∑ ebanking-metrics-java<br/>:8080"]
        end
    end

    subgraph Local["üñ•Ô∏è D√©veloppement Local"]
        subgraph Bloc32["Bloc 3.2 - Kafka Connect"]
            KC["üîå Kafka Connect<br/>:8083"]
        end

        subgraph Bloc33["Bloc 3.3 - Tests"]
            TESTS["üß™ mvn test / dotnet test"]
        end

        subgraph Docker["üê≥ Docker"]
            PROM["üìà Prometheus<br/>:9090"]
            GRAF["üìâ Grafana<br/>:3000"]
        end
    end

    STREAMS -->|"read/write topics"| K
    METRICS -->|"AdminClient"| K
    KC -->|"source/sink"| K
```

---

## üì¶ Modules & Labs

| Bloc | Module | Lab | Dur√©e | Description |
| ---- | ------ | --- | ----- | ----------- |
| 3.1 | [Kafka Streams](./module-05-kafka-streams-ksqldb/README.md) | Lab 3.1a | 1h10 | KStream, KTable, agr√©gations, fen√™trage |
| 3.2 | [Kafka Connect](./module-06-kafka-connect/README.md) | (d√©mo) | 15 min | Source/Sink connectors, REST API |
| 3.3 | [Tests Kafka](./module-07-testing/README.md) | Lab 3.3a | 45 min | MockProducer/Consumer, EmbeddedKafka |
| 3.4 | [Observabilit√©](./module-08-observability/README.md) | Lab 3.4a | 55 min | AdminClient, Prometheus, Consumer Lag |

---

## üöÄ Quick Start

### D√©marrer l'infrastructure

<details>
<summary>üê≥ Docker</summary>

```bash
# Depuis la racine du projet
cd day-01-foundations/module-01-cluster
./scripts/up.sh

# V√©rifier que Kafka est healthy
docker ps | grep kafka
```

</details>

<details>
<summary>‚òÅÔ∏è OpenShift Sandbox</summary>

```bash
oc login --token=<TOKEN> --server=<SERVER>
oc get pods -l app=kafka
```

</details>

### D√©ployer les labs Java sur OpenShift

<details>
<summary>üñ•Ô∏è PowerShell</summary>

```powershell
cd day-03-integration\scripts\powershell
.\deploy-all-labs.ps1 -Token "sha256~XXX" -Server "https://api.rm3.7wse.p1.openshiftapps.com:6443"
```

</details>

<details>
<summary>üêß Bash</summary>

```bash
cd day-03-integration/scripts/bash
./deploy-all-labs.sh --token "sha256~XXX" --server "https://api.rm3.7wse.p1.openshiftapps.com:6443"
```

</details>

### Tester toutes les APIs

<details>
<summary>üñ•Ô∏è PowerShell</summary>

```powershell
.\test-all-apis.ps1 -Token "sha256~XXX" -Server "https://api.rm3.7wse.p1.openshiftapps.com:6443"
```

</details>

<details>
<summary>üêß Bash</summary>

```bash
./test-all-apis.sh --token "sha256~XXX" --server "https://api.rm3.7wse.p1.openshiftapps.com:6443"
```

</details>

### Lancer les tests locaux (Lab 3.3a)

```bash
# Piste Java
cd day-03-integration/module-07-testing/java
mvn test

# Piste .NET
cd day-03-integration/module-07-testing/dotnet

# Lab 3.1b .NET ‚Äî ksqlDB Lab (d√©ploie ksqlDB + app)
./scripts/bash/deploy-and-test-3.1b-dotnet.sh --token "sha256~XXX" --server "https://api..."
```

</details>

#### Option B : D√©ploiement Manuel (√âtape par √âtape)

<details>
<summary>üìã D√©ploiement Manuel Java</summary>

```bash
# 1. Builder l'application Java
cd module-05-kafka-streams-ksqldb/java
mvn clean package -DskipTests

# 2. Cr√©er le build S2I
oc new-build java:openjdk-17-ubi8 --binary=true --name=ebanking-streams-java

# 3. Lancer la build
oc start-build ebanking-streams-java --from-dir=. --follow

# 4. Cr√©er l'application
oc new-app ebanking-streams-java

# 5. Configurer les variables d'environnement
oc set env deployment/ebanking-streams-java \
  KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092 \
  SPRING_PROFILES_ACTIVE=openshift

# 6. Cr√©er la route s√©curis√©e
oc create route edge ebanking-streams-java-secure --service=ebanking-streams-java --port=8080-tcp

# 7. Attendre le d√©ploiement
oc rollout status deployment/ebanking-streams-java
```

</details>

<details>
<summary>üìã D√©ploiement Manuel .NET</summary>

```bash
# 1. Builder l'application .NET
cd module-05-kafka-streams-ksqldb/dotnet/M05StreamsApi
dotnet publish -c Release -o ./publish

# 2. Cr√©er le build S2I
oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-streams-dotnet

# 3. Lancer la build
oc start-build ebanking-streams-dotnet --from-dir=./publish --follow

# 4. Cr√©er l'application
oc new-app ebanking-streams-dotnet

# 5. Configurer les variables d'environnement
oc set env deployment/ebanking-streams-dotnet \
  KAFKA__BootstrapServers=kafka-svc:9092 \
  ASPNETCORE_ENVIRONMENT=Production

# 6. Cr√©er la route s√©curis√©e
oc create route edge ebanking-streams-dotnet-secure --service=ebanking-streams-dotnet --port=8080-tcp

# 7. Attendre le d√©ploiement
oc rollout status deployment/ebanking-streams-dotnet
```

</details>

### √âtape 4 : Valider les D√©ploiements

#### V√©rifier les pods

```bash
# V√©rifier que tous les pods sont Running
oc get pods -l app=ebanking-streams-java
oc get pods -l app=ebanking-streams-dotnet
oc get pods -l app=banking-ksqldb-lab

# Attendu : 1/1 dans la colonne READY pour chaque pod
```

#### V√©rifier les routes

```bash
# Obtenir les URLs publiques
oc get route ebanking-streams-java-secure -o jsonpath='{.spec.host}'
oc get route ebanking-streams-dotnet-secure -o jsonpath='{.spec.host}'
oc get route banking-ksqldb-lab-secure -o jsonpath='{.spec.host}'
```

#### Health Checks

```bash
# Test des endpoints de sant√©
curl -k https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/actuator/health
curl -k https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/health
curl -k https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/health
```

---

## üß™ Tests et Validation

### Sc√©nario 1 : Produire des √âv√©nements de Vente

#### Java Kafka Streams

```bash
# Produire un √©v√©nement de vente
curl -k -X POST https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/sales \
  -H "Content-Type: application/json" \
  -d '{"productId":"PROD-001","quantity":2,"unitPrice":125.00}'

**R√©ponse attendue (201 Created)**:
```json
{
  "message": "Sale event processed",
  "productId": "PROD-001",
  "quantity": 2,
  "unitPrice": 125.00,
  "totalAmount": 250.00,
  "timestamp": "2026-02-12T10:30:00Z"
}
```

# V√©rifier les statistiques
curl -k https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/stats/by-product

**R√©ponse attendue**:
```json
[
  {
    "productId": "PROD-001",
    "totalAmount": 250.00,
    "totalQuantity": 2,
    "averagePrice": 125.00
  }
]
```
```

#### .NET Streams API

```bash
# Produire un √©v√©nement de vente
curl -k -X POST https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/sales \
  -H "Content-Type: application/json" \
  -d '{"productId":"PROD-002","quantity":3,"unitPrice":99.50}'

**R√©ponse attendue (201 Created)**:
```json
{
  "message": "Sale event processed successfully",
  "productId": "PROD-002",
  "quantity": 3,
  "unitPrice": 99.50,
  "totalAmount": 298.50
}
```

# Produire une transaction bancaire
curl -k -X POST https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","amount":1500.00,"type":"TRANSFER"}'

**R√©ponse attendue (201 Created)**:
```json
{
  "message": "Transaction processed successfully",
  "customerId": "CUST-001",
  "amount": 1500.00,
  "type": "TRANSFER",
  "status": "Processed"
}
```
```

### Sc√©nario 2 : ksqlDB Stream Processing

```bash
# Initialiser les streams ksqlDB
curl -k -X POST https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/initialize

**R√©ponse attendue (200 OK)**:
```json
{
  "message": "ksqlDB streams initialized successfully",
  "streamsCreated": ["transactions", "verified_transactions"],
  "tablesCreated": ["account_balances"]
}
```

# G√©n√©rer 10 transactions de test
curl -k -X POST https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/transactions/generate/10

**R√©ponse attendue (200 OK)**:
```json
{
  "message": "Generated 10 test transactions",
  "transactions": [
    {
      "transactionId": "tx-001",
      "fromAccount": "FR7630001000111222334",
      "toAccount": "FR7630001000445566778",
      "amount": 250.00,
      "type": "TRANSFER"
    }
    // ... 9 more transactions
  ]
}
```

# Consulter le solde d'un compte (Pull Query)
curl -k https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/account/CUST-001/balance

**R√©ponse attendue (200 OK)**:
```json
{
  "accountId": "CUST-001",
  "balance": 12500.50,
  "lastUpdated": "2026-02-12T10:35:00Z",
  "transactionCount": 15
}
```

# Stream des transactions v√©rifi√©es (Push Query)
curl -k https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/verified/stream

**R√©ponse attendue (Server-Sent Events)**:
```
data: {"transactionId":"tx-002","amount":150.00,"verifiedAt":"2026-02-12T10:36:00Z"}

data: {"transactionId":"tx-003","amount":75.25,"verifiedAt":"2026-02-12T10:36:05Z"}
```
```

### Sc√©nario 3 : V√©rification dans Kafka

#### Using Kafka UI

**Docker**: <http://localhost:8080>

1. Aller dans **Topics** ‚Üí **transactions**
2. Cliquer sur **Messages**
3. V√©rifier les transactions avec format JSON valide
4. Aller dans **Topics** ‚Üí **verified_transactions**
5. V√©rifier que seules les transactions valides sont pr√©sentes

#### Using Kafka CLI

```bash
# V√©rifier les transactions originales
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server kafka-0.kafka-svc:9092 \
  --topic transactions \
  --from-beginning \
  --max-messages 3

**R√©sultat attendu**:
```json
{"transactionId":"tx-001","fromAccount":"FR7630001000111222334","toAccount":"FR7630001000445566778","amount":250.00,"type":"TRANSFER"}
{"transactionId":"tx-002","fromAccount":"FR7630001000223344556","toAccount":"FR7630001000556677889","amount":150.00,"type":"PAYMENT"}
{"transactionId":"tx-003","fromAccount":"FR7630001000334455667","toAccount":"FR7630001000667788990","amount":75.25,"type":"TRANSFER"}
```

# V√©rifier les transactions v√©rifi√©es
oc exec kafka-0 -- /opt/kafka/bin/kafka-console-consumer.sh \
  --bootstrap-server kafka-0.kafka-svc:9092 \
  --topic verified_transactions \
  --from-beginning \
  --max-messages 3

**R√©sultat attendu** (seules les transactions valides):
```json
{"transactionId":"tx-001","amount":250.00,"verifiedAt":"2026-02-12T10:36:00Z"}
{"transactionId":"tx-002","amount":150.00,"verifiedAt":"2026-02-12T10:36:05Z"}
{"transactionId":"tx-003","amount":75.25,"verifiedAt":"2026-02-12T10:36:10Z"}
```
```

---

## üö¢ D√©ploiement ‚Äî 4 Environnements

Chaque lab Day 03 peut √™tre d√©ploy√© dans **4 environnements**, comme les labs Day 01 et Day 02 :

| Environnement | Outil | Kafka Bootstrap | Acc√®s API |
| ------------- | ----- | --------------- | --------- |
| **üê≥ Docker / Local** | `mvn spring-boot:run` / `dotnet run` | `localhost:9092` | `http://localhost:8080/` |
| **‚òÅÔ∏è OpenShift Sandbox** | Scripts automatis√©s | `kafka-svc:9092` | `https://{route}/` |
| **‚ò∏Ô∏è K8s / OKD** | `docker build` + `kubectl apply` | `kafka-svc:9092` | `http://localhost:8080/` (port-forward) |
| **üñ•Ô∏è Local (IDE)** | VS Code / IntelliJ | `localhost:9092` | `http://localhost:8080/` |

### Ports locaux Day 03

| Lab | API Name | Port Local | URL |
| --- | -------- | ---------- | --- |
| 3.1a | Kafka Streams API | `:8080` | `http://localhost:8080/api/v1/sales` |
| 3.4a | Metrics Dashboard API | `:8080` | `http://localhost:8080/api/v1/metrics/cluster` |

### R√©capitulatif des noms d'applications

| Lab | Piste | App Name (oc/kubectl) | Route OpenShift |
| --- | ----- | --------------------- | --------------- |
| 3.1a | Java | `ebanking-streams-java` | `ebanking-streams-java-secure` |
| 3.1a | .NET | `ebanking-streams-dotnet` | `ebanking-streams-dotnet-secure` |
| 3.1b | .NET | `banking-ksqldb-lab` | `banking-ksqldb-lab-secure` |
| 3.4a | Java | `ebanking-metrics-java` | `ebanking-metrics-java-secure` |

### D√©ploiement sur OpenShift (Sandbox ou CRC)

```bash
# ‚îÄ‚îÄ Piste Java (S2I avec java:openjdk-17-ubi8) ‚îÄ‚îÄ
cd day-03-integration/module-05-kafka-streams-ksqldb/java
oc new-build java:openjdk-17-ubi8 --binary=true --name=ebanking-streams-java
oc start-build ebanking-streams-java --from-dir=. --follow
oc new-app ebanking-streams-java
oc set env deployment/ebanking-streams-java SERVER_PORT=8080 KAFKA_BOOTSTRAP_SERVERS=kafka-svc:9092
oc create route edge ebanking-streams-java-secure --service=ebanking-streams-java --port=8080-tcp

# ‚îÄ‚îÄ Piste .NET (S2I avec dotnet:8.0-ubi8) ‚îÄ‚îÄ
cd day-03-integration/module-05-kafka-streams-ksqldb/dotnet/M05StreamsApi
oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-streams-dotnet
oc start-build ebanking-streams-dotnet --from-dir=. --follow
oc new-app ebanking-streams-dotnet
oc set env deployment/ebanking-streams-dotnet Kafka__BootstrapServers=kafka-svc:9092 ASPNETCORE_URLS=http://0.0.0.0:8080
oc create route edge ebanking-streams-dotnet-secure --service=ebanking-streams-dotnet --port=8080-tcp
```

> **Scripts automatis√©s** : Utilisez les scripts dans `scripts/bash/` ou `scripts/powershell/` pour un d√©ploiement complet avec tests int√©gr√©s. Voir [scripts/README.md](scripts/README.md).

---

## üìã Endpoints API

### Lab 3.1a ‚Äî Kafka Streams Processing

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| GET | `/` | Informations de l'application |
| GET | `/actuator/health` | V√©rification de sant√© |
| POST | `/api/v1/sales` | Produire un √©v√©nement de vente |
| GET | `/api/v1/stats/by-product` | Statistiques agr√©g√©es par produit |
| GET | `/api/v1/stats/per-minute` | Statistiques fen√™tr√©es par minute |
| GET | `/api/v1/stores/{name}/all` | Interroger un state store |
| GET | `/api/v1/stores/{name}/{key}` | Interroger un state store par cl√© |

### Lab 3.1a (.NET) ‚Äî Streams API

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| GET | `/` | Informations de l'application |
| GET | `/swagger` | Swagger UI |
| GET | `/api/v1/health` | V√©rification de sant√© |
| POST | `/api/v1/sales` | Produire un √©v√©nement de vente |
| GET | `/api/v1/stats/by-product` | Statistiques agr√©g√©es par produit |
| POST | `/api/v1/transactions` | Produire une transaction bancaire |
| GET | `/api/v1/balances` | Soldes clients |
| GET | `/api/v1/stores/{name}/all` | Interroger un state store |

### Lab 3.1b (.NET) ‚Äî ksqlDB Lab

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| GET | `/swagger` | Swagger UI |
| GET | `/api/TransactionStream/health` | V√©rification de sant√© |
| POST | `/api/TransactionStream/initialize` | Initialiser les streams ksqlDB |
| POST | `/api/TransactionStream/transactions/generate/{n}` | G√©n√©rer N transactions de test |
| GET | `/api/TransactionStream/account/{id}/balance` | Pull query ‚Äî solde compte |
| GET | `/api/TransactionStream/verified/stream` | Push query ‚Äî transactions v√©rifi√©es |
| GET | `/api/TransactionStream/fraud/stream` | Push query ‚Äî alertes fraude |

### Lab 3.4a ‚Äî Tableau de bord M√©triques

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| GET | `/` | Informations de l'application |
| GET | `/actuator/health` | V√©rification de sant√© |
| GET | `/actuator/prometheus` | M√©triques Prometheus (Micrometer) |
| GET | `/api/v1/metrics/cluster` | Sant√© du cluster Kafka (brokers, contr√¥leur) |
| GET | `/api/v1/metrics/topics` | M√©tadonn√©es des topics (partitions, r√©plication) |
| GET | `/api/v1/metrics/consumers` | Consumer lag par groupe |

---

## üß™ Tests API ‚Äî Sc√©narios de Validation

### Lab 3.1a (Java) ‚Äî Kafka Streams Processing

```bash
# Health check
curl -k https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/actuator/health

# Produire un √©v√©nement de vente
curl -k -X POST https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/sales \
  -H "Content-Type: application/json" \
  -d '{"productId":"PROD-001","quantity":2,"unitPrice":125.00}'

# Statistiques par produit
curl -k https://ebanking-streams-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/stats/by-product
```

### Lab 3.1a (.NET) ‚Äî Streams API

```bash
# Health check
curl -k https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/health

# Produire un √©v√©nement de vente
curl -k -X POST https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/sales \
  -H "Content-Type: application/json" \
  -d '{"productId":"PROD-001","quantity":3,"unitPrice":99.50}'

# Produire une transaction bancaire
curl -k -X POST https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","amount":1500.00,"type":"TRANSFER"}'

# Statistiques par produit
curl -k https://ebanking-streams-dotnet-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/stats/by-product
```

### Lab 3.1b (.NET) ‚Äî ksqlDB Lab

```bash
# Health check
curl -k https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/health

# Initialiser les streams ksqlDB
curl -k -X POST https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/initialize

# G√©n√©rer 5 transactions de test
curl -k -X POST https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/transactions/generate/5

# Solde d'un compte (Pull query)
curl -k https://banking-ksqldb-lab-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/TransactionStream/account/CUST-001/balance
```

### Lab 3.4a (Java) ‚Äî Metrics Dashboard

```bash
# Health check
curl -k https://ebanking-metrics-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/actuator/health

# Sant√© du cluster Kafka
curl -k https://ebanking-metrics-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/metrics/cluster

# M√©tadonn√©es des topics
curl -k https://ebanking-metrics-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/metrics/topics

# Consumer lag par groupe
curl -k https://ebanking-metrics-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/api/v1/metrics/consumers

# M√©triques Prometheus
curl -k https://ebanking-metrics-java-secure.apps.sandbox.x8i5.p1.openshiftapps.com/actuator/prometheus
```

---

## ‚ö†Ô∏è Troubleshooting

| Erreur | Cause | Solution |
| ------ | ----- | -------- |
| `Connector not found` | Plugin non install√© | V√©rifier `/usr/share/java/` |
| `No tasks assigned` | Configuration invalide | Valider avec PUT validate |
| `Testcontainers timeout` | Docker lent | Augmenter timeout startup |
| `Prometheus scrape failed` | JMX non expos√© | V√©rifier KAFKA_JMX_OPTS |
| `Streams not ready (503)` | Kafka Streams en d√©marrage | Attendre state = RUNNING |
| `AdminClient timeout` | Broker Kafka inaccessible | V√©rifier KAFKA_BOOTSTRAP_SERVERS |
| `MockProducer history empty` | Mock non inject√© | V√©rifier l'injection dans le service |
| `dotnet build failed` | .NET 8 SDK manquant | Installer .NET 8 ou utiliser `dotnet:8.0-ubi8` |
| `ksqlDB initialize failed` | ksqlDB non d√©ploy√© | D√©ployer ksqlDB d'abord via le script 3.1b |

---

## ‚úÖ Validation Day 03

### Piste Java

- [ ] Lab 3.1a : Topologie Kafka Streams fonctionnelle, agr√©gations par produit, fen√™trage par minute
- [ ] Lab 3.1a : State stores accessibles via REST API
- [ ] Lab 3.3a : 9 tests unitaires passent (5 producer + 4 consumer) avec MockProducer/Consumer
- [ ] Lab 3.4a : Sant√© du cluster visible via `/api/v1/metrics/cluster`
- [ ] Lab 3.4a : Consumer lag calcul√© via `/api/v1/metrics/consumers`
- [ ] Lab 3.4a : M√©triques Prometheus expos√©es via `/actuator/prometheus`

### Piste .NET

- [ ] Lab 3.1a : Streams API d√©ploy√©e, POST /api/v1/sales accept√©, stats par produit accessibles
- [ ] Lab 3.1a : Transactions bancaires et soldes clients fonctionnels
- [ ] Lab 3.1a : Swagger UI accessible
- [ ] Lab 3.1b : ksqlDB initialis√©, streams cr√©√©s
- [ ] Lab 3.1b : Push/Pull queries fonctionnelles (soldes, transactions v√©rifi√©es, alertes fraude)

### Commun

- [ ] Lab 3.2 : Comprendre Source/Sink connectors et la REST API de Kafka Connect
- [ ] Comprendre les 3 piliers de l'observabilit√© (m√©triques, logs, traces)

---

## ‚û°Ô∏è Navigation

‚¨ÖÔ∏è **[Day 02 ‚Äî Patterns de Production & S√©rialisation](../day-02-development/README.md)** | üè† **[Overview](../README.md)**
