# LAB 3.1A (.NET) : E-Banking Streams API - Stream Processing

## ‚è±Ô∏è Estimated Duration: 60-90 minutes

## üè¶ E-Banking Context

This lab implements a **stream processing API** using .NET and Confluent.Kafka that mirrors the Java Kafka Streams lab. It provides:

- **Sales stream processing** ‚Äî Filter, aggregate, and window sales events
- **Banking transactions** ‚Äî Process and track customer balances
- **State stores** ‚Äî In-memory queryable state for aggregations
- **REST API** ‚Äî Expose stream results via Swagger UI

---

## üèóÔ∏è Project Structure

```
M05StreamsApi/
‚îú‚îÄ‚îÄ Controllers/
‚îÇ   ‚îú‚îÄ‚îÄ BankingController.cs        # POST /api/v1/transactions, GET /api/v1/balances
‚îÇ   ‚îú‚îÄ‚îÄ SalesController.cs          # POST /api/v1/sales, GET /api/v1/stats/*
‚îÇ   ‚îî‚îÄ‚îÄ StoresController.cs         # GET /api/v1/stores/{name}/all|{key}
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ Sale.cs                     # Sale event model
‚îÇ   ‚îú‚îÄ‚îÄ SaleAggregate.cs            # Aggregation model
‚îÇ   ‚îú‚îÄ‚îÄ Transaction.cs              # Banking transaction model
‚îÇ   ‚îú‚îÄ‚îÄ CustomerBalance.cs          # Balance model
‚îÇ   ‚îî‚îÄ‚îÄ Product.cs                  # Product reference model
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ BankingStreamProcessorService.cs   # Banking consumer background service
‚îÇ   ‚îú‚îÄ‚îÄ SalesStreamProcessorService.cs     # Sales consumer + stream processing
‚îÇ   ‚îú‚îÄ‚îÄ BankingOptions.cs                  # Banking Kafka config
‚îÇ   ‚îú‚îÄ‚îÄ BankingState.cs                    # Banking in-memory state
‚îÇ   ‚îú‚îÄ‚îÄ StreamsOptions.cs                  # Streams Kafka config
‚îÇ   ‚îî‚îÄ‚îÄ StreamsState.cs                    # Sales in-memory state stores
‚îú‚îÄ‚îÄ Program.cs                      # App setup, DI, root endpoint
‚îú‚îÄ‚îÄ appsettings.json                # Configuration
‚îú‚îÄ‚îÄ Dockerfile                      # Multi-stage Docker build
‚îî‚îÄ‚îÄ M05StreamsApi.csproj            # .NET 8 project
```

---

## üöÄ Quick Start

### Prerequisites

- .NET 8 SDK
- Kafka cluster running (Docker or OpenShift)

### Local Development

```bash
# Run locally
dotnet run

# Swagger UI
open http://localhost:5000/swagger

# Post a sale
curl -X POST http://localhost:5000/api/v1/sales \
  -H "Content-Type: application/json" \
  -d '{"productId":"PROD-001","quantity":2,"unitPrice":125.00}'

# Check stats
curl http://localhost:5000/api/v1/stats/by-product

# Post a transaction
curl -X POST http://localhost:5000/api/v1/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","amount":1500.00,"type":"TRANSFER"}'

# Check balances
curl http://localhost:5000/api/v1/balances
```

### OpenShift Deployment

```bash
# Using scripts
cd ../../scripts
./bash/deploy-and-test-3.1a-dotnet.sh --token "sha256~XXX" --server "https://api..."

# Or PowerShell
./powershell/deploy-and-test-3.1a-dotnet.ps1 -Token "sha256~XXX" -Server "https://api..."
```

---

## üìã API Endpoints

| Method | Endpoint | Description |
| ------ | -------- | ----------- |
| GET | `/` | Application info with all endpoints |
| GET | `/swagger` | Swagger UI |
| POST | `/api/v1/sales` | Produce a sale event |
| GET | `/api/v1/stats/by-product` | Aggregated stats by product |
| GET | `/api/v1/stats/per-minute` | Windowed stats per minute |
| GET | `/api/v1/health` | Health check |
| POST | `/api/v1/transactions` | Produce a banking transaction |
| GET | `/api/v1/balances` | All customer balances |
| GET | `/api/v1/balances/{customerId}` | Balance for a specific customer |
| GET | `/api/v1/stores/{storeName}/all` | Query a state store |
| GET | `/api/v1/stores/{storeName}/{key}` | Query state store by key |

---

## üîß Configuration

### appsettings.json

```json
{
  "Kafka": {
    "BootstrapServers": "localhost:9092",
    "ClientId": "m05-streams-api-dotnet",
    "GroupId": "m05-streams-api-dotnet",
    "InputTopic": "sales-events",
    "TransactionsTopic": "banking.transactions"
  }
}
```

### Environment Variables (OpenShift)

| Variable | Default | Description |
| -------- | ------- | ----------- |
| `Kafka__BootstrapServers` | `localhost:9092` | Kafka brokers |
| `Kafka__ClientId` | `m05-streams-api-dotnet` | Client ID |
| `Kafka__GroupId` | `m05-streams-api-dotnet` | Consumer group |
| `Kafka__InputTopic` | `sales-events` | Sales input topic |
| `Kafka__TransactionsTopic` | `banking.transactions` | Transactions topic |
| `ASPNETCORE_URLS` | `http://+:5000` | Listen URL |

---

## üìä Stream Processing Pipeline

```mermaid
flowchart LR
    subgraph Input["üì• Input Topics"]
        SE["sales-events"]
        BT["banking.transactions"]
    end

    subgraph Processing["‚öôÔ∏è .NET Background Services"]
        SSP["SalesStreamProcessor"]
        BSP["BankingStreamProcessor"]
    end

    subgraph State["üíæ In-Memory State"]
        SBP["sales-by-product"]
        SPM["sales-per-minute"]
        BAL["customer-balances"]
    end

    subgraph Output["üì§ Output Topics"]
        LS["large-sales"]
        SBP_T["sales-by-product"]
        SPM_T["sales-per-minute"]
    end

    SE --> SSP --> SBP
    SSP --> SPM
    SSP --> LS
    SSP --> SBP_T
    SSP --> SPM_T
    BT --> BSP --> BAL
```

### Sale Processing Logic

1. **Filter**: Sales > 100‚Ç¨ ‚Üí `large-sales` topic
2. **Aggregate**: Group by productId ‚Üí `sales-by-product` state store + topic
3. **Window**: Per-minute aggregation ‚Üí `sales-per-minute` topic
4. **Enrich**: Join with product reference data ‚Üí `enriched-sales` topic

---

## üêõ Troubleshooting

| Issue | Cause | Solution |
| ----- | ----- | -------- |
| Empty stats | No sales consumed yet | POST a sale first, wait 2-3 seconds |
| Connection refused | Kafka not running | Check `Kafka__BootstrapServers` |
| Build fails on OpenShift | Missing .NET 8 image | Use `dotnet:8.0-ubi8` builder |
| Swagger not loading | Wrong port | Check `ASPNETCORE_URLS` |

---

## üìö Concepts Covered

- **Confluent.Kafka** producer and consumer in .NET
- **BackgroundService** for continuous stream processing
- **In-memory state stores** with ConcurrentDictionary
- **Windowed aggregation** with time-based bucketing
- **REST API** exposing stream processing results
- **S2I binary build** for OpenShift deployment
