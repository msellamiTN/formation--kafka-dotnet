# üîÑ Bloc 2.1 ‚Äî S√©rialisation Avanc√©e

| Dur√©e | Th√©orie | Lab | Pr√©requis |
| ----- | ------- | --- | --------- |
| 1h | 20 min | 40 min | Day 01 compl√©t√©, topic `banking.transactions` existant |

---

## üè¶ Sc√©nario E-Banking (suite)

Dans le Day 01, vos producers et consumers utilisaient `string` pour s√©rialiser les transactions bancaires en JSON brut. Cela fonctionne mais pose des probl√®mes en production :

- ‚ùå **Pas de validation** : un producer peut envoyer n'importe quel JSON
- ‚ùå **√âvolution fragile** : ajouter un champ casse les consumers existants
- ‚ùå **Performance** : JSON est verbeux (~2x plus gros qu'Avro)
- ‚ùå **Pas de contrat** : aucune garantie de compatibilit√© entre producer et consumer

Dans ce lab, vous allez **typer** la s√©rialisation et d√©couvrir comment r√©soudre ces probl√®mes.

---

## üéØ Objectifs d'apprentissage

- ‚úÖ Impl√©menter un **serializer/deserializer JSON typ√©** pour `Transaction`
- ‚úÖ Ajouter une **validation de sch√©ma** c√¥t√© producer
- ‚úÖ D√©montrer le **probl√®me d'√©volution de sch√©ma** avec JSON brut
- ‚úÖ Comprendre les **strat√©gies de compatibilit√©** (BACKWARD, FORWARD, FULL)
- ‚úÖ (Bonus) Configurer **Avro avec Schema Registry**

---

## üìö Partie Th√©orique (20 min)

### 1. Pourquoi typer la s√©rialisation ?

```mermaid
flowchart LR
    subgraph Avant["‚ùå Avant (string)"]
        P1["Producer"] -->|"JSON string"| T1["Topic"]
        T1 -->|"string"| C1["Consumer"]
        C1 -->|"JsonSerializer.Deserialize&lt;T&gt;()"| X1["üí• Runtime error"]
    end

    subgraph Apres["‚úÖ Apr√®s (typed)"]
        P2["Producer"] -->|"Transaction object"| SER["Serializer"]
        SER -->|"validated bytes"| T2["Topic"]
        T2 -->|"bytes"| DES["Deserializer"]
        DES -->|"Transaction object"| C2["Consumer"]
    end
```

### 2. Custom Serializer en Confluent.Kafka

L'interface `ISerializer<T>` / `IDeserializer<T>` de Confluent.Kafka permet d'injecter votre propre logique :

```csharp
public class TransactionJsonSerializer : ISerializer<Transaction>
{
    public byte[] Serialize(Transaction data, SerializationContext context)
    {
        // Validate before serializing
        ValidateTransaction(data);
        return JsonSerializer.SerializeToUtf8Bytes(data, _jsonOptions);
    }
}

public class TransactionJsonDeserializer : IDeserializer<Transaction>
{
    public Transaction Deserialize(ReadOnlySpan<byte> data, bool isNull, SerializationContext context)
    {
        if (isNull) throw new ArgumentNullException("Null message value");
        return JsonSerializer.Deserialize<Transaction>(data, _jsonOptions)
            ?? throw new InvalidOperationException("Deserialization returned null");
    }
}
```

### 3. √âvolution de sch√©ma ‚Äî Le probl√®me

```text
Version 1 (Day 01):
{
  "transactionId": "TX-001",
  "customerId": "CUST-001",
  "amount": 1500.00,
  "currency": "EUR"
}

Version 2 (Day 02 ‚Äî nouveau champ):
{
  "transactionId": "TX-001",
  "customerId": "CUST-001",
  "amount": 1500.00,
  "currency": "EUR",
  "riskScore": 0.85          ‚Üê NOUVEAU
}

Question : un Consumer v1 peut-il lire un message v2 ?
‚Üí Avec JSON + JsonSerializerOptions.DefaultIgnoreCondition : OUI ‚úÖ
‚Üí Avec un deserializer strict qui rejette les champs inconnus : NON ‚ùå
```

### 4. Schema Registry ‚Äî La solution production

| Composant | R√¥le |
| --------- | ---- |
| **Schema Registry** | Service HTTP qui stocke et versionne les sch√©mas |
| **Avro** | Format binaire compact avec sch√©ma int√©gr√© |
| **Compatibility check** | V√©rifie BACKWARD/FORWARD/FULL avant d'accepter un nouveau sch√©ma |

```mermaid
sequenceDiagram
    participant P as üì§ Producer
    participant SR as üèõÔ∏è Schema Registry
    participant K as üì¶ Kafka
    participant C as üì• Consumer

    P->>SR: POST /subjects/banking.transactions-value/versions (schema v1)
    SR-->>P: {id: 1}
    P->>K: Send(schemaId=1 + avro bytes)
    C->>SR: GET /schemas/ids/1
    SR-->>C: {schema: "..."}
    C->>K: Consume ‚Üí deserialize with schema v1
```

---

## üõ†Ô∏è Partie Pratique ‚Äî Lab 2.1 (40 min)

### √âtape 1 : Explorer le projet

```bash
cd day-02-development/module-04-advanced-patterns/lab-2.1a-serialization/dotnet
```

Le projet est une **ASP.NET Web API** avec Swagger :

```text
EBankingSerializationAPI/
‚îú‚îÄ‚îÄ SerializationLab.csproj
‚îú‚îÄ‚îÄ Program.cs                          # ASP.NET setup + Swagger + health check
‚îú‚îÄ‚îÄ Controllers/
‚îÇ   ‚îî‚îÄ‚îÄ TransactionsController.cs        # REST endpoints (v1, v2, schema-info, consumed, metrics)
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ SerializationProducerService.cs  # Typed producer with ISerializer<Transaction>
‚îÇ   ‚îî‚îÄ‚îÄ SchemaEvolutionConsumerService.cs # Background consumer (v1 deserializer)
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îî‚îÄ‚îÄ Transaction.cs                   # Transaction v1 + TransactionV2 models
‚îú‚îÄ‚îÄ Serializers/
‚îÇ   ‚îú‚îÄ‚îÄ TransactionJsonSerializer.cs     # ISerializer<Transaction> with validation
‚îÇ   ‚îî‚îÄ‚îÄ TransactionJsonDeserializer.cs   # IDeserializer<Transaction> (BACKWARD compat)
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ appsettings.json
‚îî‚îÄ‚îÄ requests.http                       # VS Code REST Client test requests
```

### √âtape 2 : Lancer l'API

```bash
dotnet run
# Swagger UI : http://localhost:5170/swagger
```

### Endpoints

| M√©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| `POST` | `/api/transactions` | Send v1 transaction (typed `ISerializer<Transaction>` + validation) |
| `POST` | `/api/transactions/v2` | Send v2 transaction (with `riskScore`, `sourceChannel`) |
| `GET` | `/api/transactions/schema-info` | Show schema v1/v2 structure and compatibility info |
| `GET` | `/api/transactions/consumed` | List messages consumed by the background v1 consumer |
| `GET` | `/api/transactions/metrics` | Producer + consumer serialization metrics |
| `GET` | `/health` | Health check |

### √âtape 3 : Envoyer une transaction v1 (valid√©e)

```bash
# Valid transaction ‚Äî serializer validates then sends to Kafka
curl -X POST http://localhost:5170/api/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "CUST-001",
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 1500.00,
    "currency": "EUR",
    "type": 1
  }'
```

**R√©ponse attendue** :

```json
{
  "status": "Produced",
  "transactionId": "TX-a1b2c3d4",
  "partition": 3,
  "offset": 42,
  "schemaVersion": 1
}
```

```bash
# Invalid transaction (amount < 0) ‚Äî REJECTED by serializer BEFORE Kafka
curl -X POST http://localhost:5170/api/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-002","fromAccount":"FR76300","toAccount":"FR76301","amount":-50,"currency":"EUR","type":1}'
```

**R√©ponse attendue** : `400 Bad Request` with `"status": "RejectedByValidation"`

**Point cl√©** : la transaction invalide est rejet√©e **avant** l'envoi √† Kafka gr√¢ce au serializer typ√©.

### √âtape 4 : D√©montrer l'√©volution de sch√©ma (v1 ‚Üí v2)

```bash
# Send a v2 transaction with riskScore (new field)
curl -X POST http://localhost:5170/api/transactions/v2 \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "CUST-003",
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 2500.00,
    "currency": "EUR",
    "type": 1,
    "riskScore": 0.85,
    "sourceChannel": "mobile-app"
  }'
```

Maintenant, v√©rifiez que le **consumer v1** (background service) a lu le message v2 :

```bash
curl -s http://localhost:5170/api/transactions/consumed | jq .
```

**R√©sultat attendu** : le consumer v1 lit le message v2 et **ignore `riskScore`** ‚Üí BACKWARD compatible ‚úÖ

### √âtape 5 : Inspecter les sch√©mas et m√©triques

```bash
# Schema info ‚Äî shows v1/v2 field differences and compatibility rules
curl -s http://localhost:5170/api/transactions/schema-info | jq .

# Metrics ‚Äî serialization counts, errors, schema versions seen
curl -s http://localhost:5170/api/transactions/metrics | jq .
```

### √âtape 6 : Exercices

1. **Modifier le serializer** : ouvrez `TransactionJsonSerializer.cs` et ajoutez une validation rejetant les transactions > 1 000 000 EUR
2. **V√©rifier les headers** : dans Kafka UI (`http://localhost:8080`), ouvrez le topic `banking.transactions` et inspectez les headers `schema-version` et `serializer`
3. **Tester la compatibilit√© FORWARD** : envoyez un message v1, puis v√©rifiez dans `/api/transactions/consumed` que le consumer v1 l'a lu correctement
4. **Comparer avec Day 01** : envoyez un JSON brut mal form√© (champ manquant) et observez la diff√©rence de comportement vs Day 01 (pas de validation)

### √âtape 7 (Bonus) : Schema Registry avec Avro

> ‚ö†Ô∏è Ce bonus n√©cessite un Schema Registry en cours d'ex√©cution (Docker).

```bash
# Ajouter Schema Registry au docker-compose
docker compose -f docker-compose.schema-registry.yml up -d
```

```csharp
// NuGet: Confluent.SchemaRegistry.Serdes.Avro
using var producer = new ProducerBuilder<string, Transaction>(producerConfig)
    .SetValueSerializer(new AvroSerializer<Transaction>(schemaRegistry))
    .Build();
```

---

## ÔøΩ D√©ploiement Automatis√© (Scripts)

> **Recommand√© pour OpenShift Sandbox** : Utilisez les scripts de d√©ploiement automatis√©s pour un d√©ploiement rapide et test√©.

### Bash (Linux/macOS/WSL)

```bash
# D√©ploiement complet avec validation
cd day-02-development/scripts
./bash/deploy-and-test-2.1a.sh --token=<TOKEN> --server=<SERVER>

# D√©ploiement sans tests (plus rapide)
./bash/deploy-and-test-2.1a.sh --token=<TOKEN> --server=<SERVER> --skip-tests
```

### PowerShell (Windows)

```powershell
# D√©ploiement complet avec validation
cd day-02-development\scripts
.\powershell\deploy-and-test-2.1a.ps1 -Token <TOKEN> -Server <SERVER>

# D√©ploiement sans tests (plus rapide)
.\powershell\deploy-and-test-2.1a.ps1 -Token <TOKEN> -Server <SERVER> -SkipTests
```

### Ce que fait le script

1. ‚úÖ **Login OpenShift** avec votre token et serveur
2. ‚úÖ **Build S2I** : `oc new-build` + `oc start-build`
3. ‚úÖ **D√©ploiement** : `oc new-app` avec variables d'environnement
4. ‚úÖ **Route s√©curis√©e** : `oc create route edge`
5. ‚úÖ **Validation** : Tests automatiques des objectifs du lab
6. ‚úÖ **Rapport** : URLs d'acc√®s et commandes de v√©rification

> **Note** : Les scripts utilisent les m√™mes commandes manuelles que dans les sections ci-dessous, mais de mani√®re automatis√©e avec validation.

---

## ÔøΩüê≥ D√©ploiement Docker Compose

```bash
# Depuis la racine du module M04
cd day-02-development/module-04-advanced-patterns

# D√©marrer uniquement le lab 2.1a
docker compose -f docker-compose.module.yml up -d --build serialization-api

# V√©rifier
docker logs m04-serialization-api --tail 10
```

**Acc√®s** : `http://localhost:5170/swagger`

```bash
# Tester
curl -s http://localhost:5170/health
curl -s -X POST http://localhost:5170/api/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":1500.00,"currency":"EUR","type":1}' | jq .
```

```bash
# Arr√™ter
docker compose -f docker-compose.module.yml down serialization-api
```

---

## ‚òÅÔ∏è D√©ploiement sur OpenShift Sandbox

> **üéØ Objectif** : Ce d√©ploiement valide les concepts de **s√©rialisation avanc√©e** dans un environnement cloud :
> - **`ISerializer<T>`** : le serializer typ√© valide et s√©rialise les transactions avant envoi
> - **`IDeserializer<T>`** : le deserializer reconstruit un objet `Transaction` depuis les bytes Kafka
> - **Schema evolution** : un consumer v1 lit des messages v2 (BACKWARD compatible)
> - **Validation pr√©-envoi** : les transactions invalides sont rejet√©es AVANT Kafka

### 1. Pr√©parer le Build et le D√©ploiement

```bash
cd day-02-development/module-04-advanced-patterns/lab-2.1a-serialization/dotnet

# Cr√©er une build binaire pour .NET 8
oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-serialization-api

# Lancer la build en envoyant le dossier courant
oc start-build ebanking-serialization-api --from-dir=. --follow

# Cr√©er l'application
oc new-app ebanking-serialization-api
```

### 2. Configurer les variables d'environnement

```bash
oc set env deployment/ebanking-serialization-api \
  Kafka__BootstrapServers=kafka-svc:9092 \
  Kafka__Topic=banking.transactions \
  Kafka__GroupId=serialization-lab-consumer \
  ASPNETCORE_URLS=http://0.0.0.0:8080 \
  ASPNETCORE_ENVIRONMENT=Development
```

### 3. Exposer publiquement (Secure Edge Route)

> [!IMPORTANT]
> Standard routes may hang on the Sandbox. Use an **edge route** for reliable public access.

```bash
oc create route edge ebanking-serialization-api-secure --service=ebanking-serialization-api --port=8080-tcp
```

### 4. Tester l'API d√©ploy√©e

```bash
# Obtenir l'URL publique
URL=$(oc get route ebanking-serialization-api-secure -o jsonpath='{.spec.host}')
echo "https://$URL/swagger"

# Health check
curl -k -i "https://$URL/health"

# Send a v1 transaction (typed serializer + validation)
curl -k -s -X POST "https://$URL/api/transactions" \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":1500.00,"currency":"EUR","type":1}' | jq .

# Send a v2 transaction (schema evolution)
curl -k -s -X POST "https://$URL/api/transactions/v2" \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-002","fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":2500.00,"currency":"EUR","type":1,"riskScore":0.85,"sourceChannel":"mobile-app"}' | jq .

# Verify consumer v1 reads v2 messages (BACKWARD compatible)
curl -k -s "https://$URL/api/transactions/consumed" | jq .

# Check schema info
curl -k -s "https://$URL/api/transactions/schema-info" | jq .

# Metrics
curl -k -s "https://$URL/api/transactions/metrics" | jq .
```

### 5. ‚úÖ Success Criteria ‚Äî Deployment

```bash
# Pod running?
oc get pod -l deployment=ebanking-serialization-api
# Expected: STATUS=Running, READY=1/1

# API reachable?
curl -k -s "https://$(oc get route ebanking-serialization-api-secure -o jsonpath='{.spec.host}')/health" | jq .
# Expected: Healthy

# Serializer validates? (invalid amount ‚Üí 400)
curl -k -s -X POST "https://$(oc get route ebanking-serialization-api-secure -o jsonpath='{.spec.host}')/api/transactions" \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-X","fromAccount":"FR76","toAccount":"FR76","amount":-50,"currency":"EUR","type":1}'
# Expected: 400 Bad Request, status=RejectedByValidation
```

---

## üñ•Ô∏è D√©ploiement Local OpenShift (CRC / OpenShift Local)

Si vous disposez d'un cluster **OpenShift Local** (anciennement CRC ‚Äî CodeReady Containers), vous pouvez d√©ployer l'API directement depuis votre machine.

### 1. Pr√©requis

```bash
# V√©rifier que le cluster est d√©marr√©
crc status

# Se connecter au cluster
oc login -u developer https://api.crc.testing:6443
oc project ebanking-labs
```

### 2. Build et D√©ploiement (Binary Build)

```bash
cd day-02-development/module-04-advanced-patterns/lab-2.1a-serialization/dotnet

oc new-build dotnet:8.0-ubi8 --binary=true --name=ebanking-serialization-api
oc start-build ebanking-serialization-api --from-dir=. --follow
oc new-app ebanking-serialization-api
```

### 3. Configurer les variables d'environnement

```bash
oc set env deployment/ebanking-serialization-api \
  Kafka__BootstrapServers=kafka-svc:9092 \
  Kafka__Topic=banking.transactions \
  Kafka__GroupId=serialization-lab-consumer \
  ASPNETCORE_URLS=http://0.0.0.0:8080 \
  ASPNETCORE_ENVIRONMENT=Development
```

### 4. Exposer et tester

```bash
# Cr√©er une route edge
oc create route edge ebanking-serialization-api-secure --service=ebanking-serialization-api --port=8080-tcp

# Obtenir l'URL
URL=$(oc get route ebanking-serialization-api-secure -o jsonpath='{.spec.host}')
echo "https://$URL/swagger"

# Tester
curl -k -i "https://$URL/health"
```

### 5. Alternative : D√©ploiement par manifeste YAML

```bash
sed "s/\${NAMESPACE}/ebanking-labs/g" deployment/openshift-deployment.yaml | oc apply -f -
```

---

## ‚ò∏Ô∏è D√©ploiement Kubernetes / OKD (K3s, K8s, OKD)

Pour un cluster **Kubernetes standard** (K3s, K8s, Minikube) ou **OKD**, utilisez les manifestes YAML fournis dans le dossier `deployment/`.

### 1. Construire l'image Docker

```bash
cd day-02-development/module-04-advanced-patterns/lab-2.1a-serialization/dotnet

# Build de l'image
docker build -t ebanking-serialization-api:latest .

# Pour un registry distant (adapter l'URL du registry)
docker tag ebanking-serialization-api:latest <registry>/ebanking-serialization-api:latest
docker push <registry>/ebanking-serialization-api:latest
```

> **K3s / Minikube** : Si vous utilisez un cluster local, l'image locale suffit avec `imagePullPolicy: IfNotPresent`.

### 2. D√©ployer les manifestes

```bash
# Appliquer le Deployment + Service + Ingress
kubectl apply -f deployment/k8s-deployment.yaml

# V√©rifier le d√©ploiement
kubectl get pods -l app=ebanking-serialization-api
kubectl get svc ebanking-serialization-api
```

### 3. Configurer le Kafka Bootstrap (si diff√©rent)

```bash
kubectl set env deployment/ebanking-serialization-api \
  Kafka__BootstrapServers=<kafka-bootstrap>:9092
```

### 4. Acc√©der √† l'API

```bash
# Port-forward pour acc√®s local
kubectl port-forward svc/ebanking-serialization-api 8080:8080

# Tester
curl http://localhost:8080/health
curl http://localhost:8080/swagger/index.html
```

> **Ingress** : Si vous avez un Ingress Controller (nginx, traefik), ajoutez `ebanking-serialization-api.local` √† votre fichier `/etc/hosts` pointant vers l'IP du cluster.

### 5. üß™ Validation des concepts (K8s)

```bash
# Send v1 transaction (port-forward actif sur 8080)
curl -s -X POST "http://localhost:8080/api/transactions" \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-001","fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":1500.00,"currency":"EUR","type":1}' | jq .

# Send v2 transaction (schema evolution)
curl -s -X POST "http://localhost:8080/api/transactions/v2" \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-002","fromAccount":"FR7630001000123456789","toAccount":"FR7630001000987654321","amount":2500.00,"currency":"EUR","type":1,"riskScore":0.85,"sourceChannel":"mobile-app"}' | jq .

# Verify BACKWARD compatibility ‚Äî consumer v1 reads v2
curl -s "http://localhost:8080/api/transactions/consumed" | jq .

# Schema info
curl -s "http://localhost:8080/api/transactions/schema-info" | jq .

# Metrics
curl -s "http://localhost:8080/api/transactions/metrics" | jq .
```

> **Docker Compose** : Si Kafka tourne via Docker Compose, utilisez `docker exec kafka ...` au lieu de `kubectl exec kafka-0 ...`.

### 6. OKD : Utiliser les manifestes OpenShift

```bash
sed "s/\${NAMESPACE}/$(oc project -q)/g" deployment/openshift-deployment.yaml | oc apply -f -
```

---

## üîß Troubleshooting

| Symptom | Probable Cause | Solution |
| ------- | -------------- | -------- |
| Pod CrashLoopBackOff | Missing env vars or Kafka DNS error | Check: `oc set env deployment/ebanking-serialization-api --list` |
| `400 Bad Request` on valid tx | Serializer validation too strict | Check `TransactionJsonSerializer.cs` validation rules |
| Consumer shows 0 messages | Consumer not started or wrong offset | Verify `AutoOffsetReset = Earliest` in consumer config |
| Swagger not accessible | Wrong `ASPNETCORE_URLS` | Set: `ASPNETCORE_URLS=http://0.0.0.0:8080` |
| Route returns 503/504 | Pod not ready or wrong port | Check: `oc get pods`, verify route targets port `8080-tcp` |

---

## ‚úÖ Checkpoint de validation

- [ ] Le serializer typ√© valide les transactions avant envoi
- [ ] Le deserializer typ√© reconstruit un objet `Transaction` √† partir des bytes
- [ ] L'ajout d'un champ optionnel (v2) est compatible BACKWARD avec un consumer v1
- [ ] Vous comprenez la diff√©rence entre JSON string et serializer typ√©
- [ ] (Bonus) Schema Registry accepte votre sch√©ma Avro

---

## üìñ Points √† retenir

| Concept | D√©tail |
| ------- | ------ |
| **`ISerializer<T>`** | Interface Confluent.Kafka pour s√©rialisation custom |
| **`IDeserializer<T>`** | Interface Confluent.Kafka pour d√©s√©rialisation custom |
| **Validation c√¥t√© producer** | Rejeter les messages invalides AVANT envoi √† Kafka |
| **BACKWARD compatible** | Nouveau consumer lit ancien format (ajouter champ optionnel) |
| **FORWARD compatible** | Ancien consumer lit nouveau format (ignorer champs inconnus) |
| **Schema Registry** | Service centralis√© de gestion des sch√©mas (production) |

---

## ‚û°Ô∏è Suite

üëâ **[Bloc 2.2 ‚Äî Producer Patterns Avanc√©s](../lab-2.2-producer-advanced/README.md)**
