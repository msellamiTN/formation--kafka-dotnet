# ğŸ”„ Bloc 2.1 â€” SÃ©rialisation AvancÃ©e

| DurÃ©e | ThÃ©orie | Lab | PrÃ©requis |
| ----- | ------- | --- | --------- |
| 1h | 20 min | 40 min | Day 01 complÃ©tÃ©, topic `banking.transactions` existant |

---

## ğŸ¦ ScÃ©nario E-Banking (suite)

Dans le Day 01, vos producers et consumers utilisaient `string` pour sÃ©rialiser les transactions bancaires en JSON brut. Cela fonctionne mais pose des problÃ¨mes en production :

- âŒ **Pas de validation** : un producer peut envoyer n'importe quel JSON
- âŒ **Ã‰volution fragile** : ajouter un champ casse les consumers existants
- âŒ **Performance** : JSON est verbeux (~2x plus gros qu'Avro)
- âŒ **Pas de contrat** : aucune garantie de compatibilitÃ© entre producer et consumer

Dans ce lab, vous allez **typer** la sÃ©rialisation et dÃ©couvrir comment rÃ©soudre ces problÃ¨mes.

---

## ğŸ¯ Objectifs d'apprentissage

- âœ… ImplÃ©menter un **serializer/deserializer JSON typÃ©** pour `Transaction`
- âœ… Ajouter une **validation de schÃ©ma** cÃ´tÃ© producer
- âœ… DÃ©montrer le **problÃ¨me d'Ã©volution de schÃ©ma** avec JSON brut
- âœ… Comprendre les **stratÃ©gies de compatibilitÃ©** (BACKWARD, FORWARD, FULL)
- âœ… (Bonus) Configurer **Avro avec Schema Registry**

---

## ğŸ“š Partie ThÃ©orique (20 min)

### 1. Pourquoi typer la sÃ©rialisation ?

```mermaid
flowchart LR
    subgraph Avant["âŒ Avant (string)"]
        P1["Producer"] -->|"JSON string"| T1["Topic"]
        T1 -->|"string"| C1["Consumer"]
        C1 -->|"JsonSerializer.Deserialize&lt;T&gt;()"| X1["ğŸ’¥ Runtime error"]
    end

    subgraph Apres["âœ… AprÃ¨s (typed)"]
        P2["Producer"] -->|"Transaction object"| SER["Serializer"]
        SER -->|"validated bytes"| T2["Topic"]
        T2 -->|"bytes"| DES["Deserializer"]
        DES -->|"Transaction object"| C2["Consumer"]
    end
```

### 2. Custom Serializer en Confluent.Kafka

L'interface `ISerializer<T>` / `IDeserializer<T>` de Confluent.Kafka permet d'injecter votre propre logique :

```csharp
public class TransactionJsonSerializer : ISerializer<Transaction>
{
    public byte[] Serialize(Transaction data, SerializationContext context)
    {
        // Validate before serializing
        ValidateTransaction(data);
        return JsonSerializer.SerializeToUtf8Bytes(data, _jsonOptions);
    }
}

public class TransactionJsonDeserializer : IDeserializer<Transaction>
{
    public Transaction Deserialize(ReadOnlySpan<byte> data, bool isNull, SerializationContext context)
    {
        if (isNull) throw new ArgumentNullException("Null message value");
        return JsonSerializer.Deserialize<Transaction>(data, _jsonOptions)
            ?? throw new InvalidOperationException("Deserialization returned null");
    }
}
```

### 3. Ã‰volution de schÃ©ma â€” Le problÃ¨me

```text
Version 1 (Day 01):
{
  "transactionId": "TX-001",
  "customerId": "CUST-001",
  "amount": 1500.00,
  "currency": "EUR"
}

Version 2 (Day 02 â€” nouveau champ):
{
  "transactionId": "TX-001",
  "customerId": "CUST-001",
  "amount": 1500.00,
  "currency": "EUR",
  "riskScore": 0.85          â† NOUVEAU
}

Question : un Consumer v1 peut-il lire un message v2 ?
â†’ Avec JSON + JsonSerializerOptions.DefaultIgnoreCondition : OUI âœ…
â†’ Avec un deserializer strict qui rejette les champs inconnus : NON âŒ
```

### 4. Schema Registry â€” La solution production

| Composant | RÃ´le |
| --------- | ---- |
| **Schema Registry** | Service HTTP qui stocke et versionne les schÃ©mas |
| **Avro** | Format binaire compact avec schÃ©ma intÃ©grÃ© |
| **Compatibility check** | VÃ©rifie BACKWARD/FORWARD/FULL avant d'accepter un nouveau schÃ©ma |

```mermaid
sequenceDiagram
    participant P as ğŸ“¤ Producer
    participant SR as ğŸ›ï¸ Schema Registry
    participant K as ğŸ“¦ Kafka
    participant C as ğŸ“¥ Consumer

    P->>SR: POST /subjects/banking.transactions-value/versions (schema v1)
    SR-->>P: {id: 1}
    P->>K: Send(schemaId=1 + avro bytes)
    C->>SR: GET /schemas/ids/1
    SR-->>C: {schema: "..."}
    C->>K: Consume â†’ deserialize with schema v1
```

---

## ğŸ› ï¸ Partie Pratique â€” Lab 2.1 (40 min)

### Ã‰tape 1 : Explorer le projet

```bash
cd day-02-development/module-04-advanced-patterns/lab-2.1a-serialization/dotnet
```

Le projet est une **ASP.NET Web API** avec Swagger :

```text
EBankingSerializationAPI/
â”œâ”€â”€ SerializationLab.csproj
â”œâ”€â”€ Program.cs                          # ASP.NET setup + Swagger + health check
â”œâ”€â”€ Controllers/
â”‚   â””â”€â”€ TransactionsController.cs        # REST endpoints (v1, v2, schema-info, consumed, metrics)
â”œâ”€â”€ Services/
â”‚   â”œâ”€â”€ SerializationProducerService.cs  # Typed producer with ISerializer<Transaction>
â”‚   â””â”€â”€ SchemaEvolutionConsumerService.cs # Background consumer (v1 deserializer)
â”œâ”€â”€ Models/
â”‚   â””â”€â”€ Transaction.cs                   # Transaction v1 + TransactionV2 models
â”œâ”€â”€ Serializers/
â”‚   â”œâ”€â”€ TransactionJsonSerializer.cs     # ISerializer<Transaction> with validation
â”‚   â””â”€â”€ TransactionJsonDeserializer.cs   # IDeserializer<Transaction> (BACKWARD compat)
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ appsettings.json
â””â”€â”€ requests.http                       # VS Code REST Client test requests
```

### Ã‰tape 2 : Lancer l'API

```bash
dotnet run
# Swagger UI : http://localhost:5170/swagger
```

### Endpoints

| MÃ©thode | Endpoint | Description |
| ------- | -------- | ----------- |
| `POST` | `/api/transactions` | Send v1 transaction (typed `ISerializer<Transaction>` + validation) |
| `POST` | `/api/transactions/v2` | Send v2 transaction (with `riskScore`, `sourceChannel`) |
| `GET` | `/api/transactions/schema-info` | Show schema v1/v2 structure and compatibility info |
| `GET` | `/api/transactions/consumed` | List messages consumed by the background v1 consumer |
| `GET` | `/api/transactions/metrics` | Producer + consumer serialization metrics |
| `GET` | `/health` | Health check |

### Ã‰tape 3 : Envoyer une transaction v1 (validÃ©e)

```bash
# Valid transaction â€” serializer validates then sends to Kafka
curl -X POST http://localhost:5170/api/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "CUST-001",
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 1500.00,
    "currency": "EUR",
    "type": 1
  }'
```

**RÃ©ponse attendue** :

```json
{
  "status": "Produced",
  "transactionId": "TX-a1b2c3d4",
  "partition": 3,
  "offset": 42,
  "schemaVersion": 1
}
```

```bash
# Invalid transaction (amount < 0) â€” REJECTED by serializer BEFORE Kafka
curl -X POST http://localhost:5170/api/transactions \
  -H "Content-Type: application/json" \
  -d '{"customerId":"CUST-002","fromAccount":"FR76300","toAccount":"FR76301","amount":-50,"currency":"EUR","type":1}'
```

**RÃ©ponse attendue** : `400 Bad Request` with `"status": "RejectedByValidation"`

**Point clÃ©** : la transaction invalide est rejetÃ©e **avant** l'envoi Ã  Kafka grÃ¢ce au serializer typÃ©.

### Ã‰tape 4 : DÃ©montrer l'Ã©volution de schÃ©ma (v1 â†’ v2)

```bash
# Send a v2 transaction with riskScore (new field)
curl -X POST http://localhost:5170/api/transactions/v2 \
  -H "Content-Type: application/json" \
  -d '{
    "customerId": "CUST-003",
    "fromAccount": "FR7630001000123456789",
    "toAccount": "FR7630001000987654321",
    "amount": 2500.00,
    "currency": "EUR",
    "type": 1,
    "riskScore": 0.85,
    "sourceChannel": "mobile-app"
  }'
```

Maintenant, vÃ©rifiez que le **consumer v1** (background service) a lu le message v2 :

```bash
curl -s http://localhost:5170/api/transactions/consumed | jq .
```

**RÃ©sultat attendu** : le consumer v1 lit le message v2 et **ignore `riskScore`** â†’ BACKWARD compatible âœ…

### Ã‰tape 5 : Inspecter les schÃ©mas et mÃ©triques

```bash
# Schema info â€” shows v1/v2 field differences and compatibility rules
curl -s http://localhost:5170/api/transactions/schema-info | jq .

# Metrics â€” serialization counts, errors, schema versions seen
curl -s http://localhost:5170/api/transactions/metrics | jq .
```

### Ã‰tape 6 : Exercices

1. **Modifier le serializer** : ouvrez `TransactionJsonSerializer.cs` et ajoutez une validation rejetant les transactions > 1 000 000 EUR
2. **VÃ©rifier les headers** : dans Kafka UI (`http://localhost:8080`), ouvrez le topic `banking.transactions` et inspectez les headers `schema-version` et `serializer`
3. **Tester la compatibilitÃ© FORWARD** : envoyez un message v1, puis vÃ©rifiez dans `/api/transactions/consumed` que le consumer v1 l'a lu correctement
4. **Comparer avec Day 01** : envoyez un JSON brut mal formÃ© (champ manquant) et observez la diffÃ©rence de comportement vs Day 01 (pas de validation)

### Ã‰tape 7 (Bonus) : Schema Registry avec Avro

> âš ï¸ Ce bonus nÃ©cessite un Schema Registry en cours d'exÃ©cution (Docker).

```bash
# Ajouter Schema Registry au docker-compose
docker compose -f docker-compose.schema-registry.yml up -d
```

```csharp
// NuGet: Confluent.SchemaRegistry.Serdes.Avro
using var producer = new ProducerBuilder<string, Transaction>(producerConfig)
    .SetValueSerializer(new AvroSerializer<Transaction>(schemaRegistry))
    .Build();
```

---

## âœ… Checkpoint de validation

- [ ] Le serializer typÃ© valide les transactions avant envoi
- [ ] Le deserializer typÃ© reconstruit un objet `Transaction` Ã  partir des bytes
- [ ] L'ajout d'un champ optionnel (v2) est compatible BACKWARD avec un consumer v1
- [ ] Vous comprenez la diffÃ©rence entre JSON string et serializer typÃ©
- [ ] (Bonus) Schema Registry accepte votre schÃ©ma Avro

---

## ğŸ“– Points Ã  retenir

| Concept | DÃ©tail |
| ------- | ------ |
| **`ISerializer<T>`** | Interface Confluent.Kafka pour sÃ©rialisation custom |
| **`IDeserializer<T>`** | Interface Confluent.Kafka pour dÃ©sÃ©rialisation custom |
| **Validation cÃ´tÃ© producer** | Rejeter les messages invalides AVANT envoi Ã  Kafka |
| **BACKWARD compatible** | Nouveau consumer lit ancien format (ajouter champ optionnel) |
| **FORWARD compatible** | Ancien consumer lit nouveau format (ignorer champs inconnus) |
| **Schema Registry** | Service centralisÃ© de gestion des schÃ©mas (production) |

---

## â¡ï¸ Suite

ğŸ‘‰ **[Bloc 2.2 â€” Producer Patterns AvancÃ©s](../lab-2.2-producer-advanced/README.md)**
